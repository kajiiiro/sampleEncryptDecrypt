<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<title>Iaasクラウド</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<style>
body { font:80% Verdana,Tahoma,Arial,sans-serif; }
h1, h2, h3, h4 {  font-family: "Trebuchet MS",Georgia,"Times New Roman",serif; }
ul.toc { padding: 4px; margin-left: 0; }
ul.toc li { list-style-type:none; }
ul.toc li.heading2 { margin-left: 1em; }
ul.toc li.heading3 { margin-left: 2em; }
a.wiki-anchor { display: none; margin-left: 6px; text-decoration: none; }
a.wiki-anchor:hover { color: #aaa !important; text-decoration: none; }
h1:hover a.wiki-anchor, h2:hover a.wiki-anchor, h3:hover a.wiki-anchor { display: inline; color: #ddd; }
</style>
</head>
<body>

<strong>索引(名前順)</strong>
<ul>
    <li><a href="#Ansible">Ansible</a></li>
    <li><a href="#Ansible設計">Ansible設計</a></li>
    <li><a href="#Bind">Bind</a></li>
    <li><a href="#CA設計">CA設計</a></li>
    <li><a href="#Centos">Centos</a></li>
    <li><a href="#Clocon技術調査">Clocon技術調査</a></li>
    <li><a href="#Clocon設計">Clocon設計</a></li>
    <li><a href="#DB設計">DB設計</a></li>
    <li><a href="#Dns設計">Dns設計</a></li>
    <li><a href="#Drbd">Drbd</a></li>
    <li><a href="#Eqltune">Eqltune</a></li>
    <li><a href="#Fio">Fio</a></li>
    <li><a href="#Firewall">Firewall</a></li>
    <li><a href="#Hadoop設計">Hadoop設計</a></li>
    <li><a href="#HTTPProxy設計">HTTPProxy設計</a></li>
    <li><a href="#Internet_Firewall詳細設計">Internet Firewall詳細設計</a></li>
    <li><a href="#Internet_Gateway詳細設計">Internet Gateway詳細設計</a></li>
    <li><a href="#ISCSIストレージボリュームの拡張〜Linux上での認識まで〜">ISCSIストレージボリュームの拡張〜Linux上での認識まで〜</a></li>
    <li><a href="#ISCSIボリュームの認識〜接続まで">ISCSIボリュームの認識〜接続まで</a></li>
    <li><a href="#ISCSI設計">ISCSI設計</a></li>
    <li><a href="#Keepalivedとlvs">Keepalivedとlvs</a></li>
    <li><a href="#Kickstart">Kickstart</a></li>
    <li><a href="#Kickstart設計">Kickstart設計</a></li>
    <li><a href="#KVMクラスタ設計">KVMクラスタ設計</a></li>
    <li><a href="#KVMホスト運用手順">KVMホスト運用手順</a></li>
    <li><a href="#KVM仮想マシンテンプレートからのリストア">KVM仮想マシンテンプレートからのリストア</a></li>
    <li><a href="#KVM設計">KVM設計</a></li>
    <li><a href="#L2_Switch詳細設計">L2 Switch詳細設計</a></li>
    <li><a href="#L3_Switch詳細設計">L3 Switch詳細設計</a></li>
    <li><a href="#Libvirt">Libvirt</a></li>
    <li><a href="#Linux_ha_pacemaker">Linux ha pacemaker</a></li>
    <li><a href="#Load_Balancer詳細設計">Load Balancer詳細設計</a></li>
    <li><a href="#Mail設計">Mail設計</a></li>
    <li><a href="#Mrepo">Mrepo</a></li>
    <li><a href="#NAS設計">NAS設計</a></li>
    <li><a href="#Nginx">Nginx</a></li>
    <li><a href="#Nodejs">Nodejs</a></li>
    <li><a href="#Ntpd">Ntpd</a></li>
    <li><a href="#NTP設計">NTP設計</a></li>
    <li><a href="#NW機器サポート問い合わせ先">NW機器サポート問い合わせ先</a></li>
    <li><a href="#NW設計">NW設計</a></li>
    <li><a href="#Openvpn">Openvpn</a></li>
    <li><a href="#Postfix">Postfix</a></li>
    <li><a href="#Serverspec">Serverspec</a></li>
    <li><a href="#Serverspec設計">Serverspec設計</a></li>
    <li><a href="#Squid">Squid</a></li>
    <li><a href="#TKVMについて">TKVMについて</a></li>
    <li><a href="#TMMC用試験環境">TMMC用試験環境</a></li>
    <li><a href="#Vmware形式の仮想マシンをkvmへ移行">Vmware形式の仮想マシンをkvmへ移行</a></li>
    <li><a href="#VyOs">VyOs</a></li>
    <li><a href="#Vyosインストール">Vyosインストール</a></li>
    <li><a href="#Wiki">Wiki</a></li>
    <li><a href="#Zabbix">Zabbix</a></li>
    <li><a href="#クラコン構築メモ">クラコン構築メモ</a></li>
    <li><a href="#クラスタ設計">クラスタ設計</a></li>
    <li><a href="#ノウハウまとめ">ノウハウまとめ</a></li>
    <li><a href="#バックアップ方法まとめ">バックアップ方法まとめ</a></li>
    <li><a href="#マルチパス設定">マルチパス設定</a></li>
    <li><a href="#ログサーバ設計">ログサーバ設計</a></li>
    <li><a href="#仮想インスタンス運用手順">仮想インスタンス運用手順</a></li>
    <li><a href="#仮想ルータ設計">仮想ルータ設計</a></li>
    <li><a href="#共通監視設計">共通監視設計</a></li>
    <li><a href="#共通設計">共通設計</a></li>
    <li><a href="#監視プロキシ設計">監視プロキシ設計</a></li>
    <li><a href="#監視設計">監視設計</a></li>
    <li><a href="#笑い男ロゴ">笑い男ロゴ</a></li>
    <li><a href="#議事録">議事録</a></li>
</ul>

<hr />
<a name="Ansible" />
<a name="Ansible_Ansible"></a>
<h1 >Ansible<a href="#Ansible_Ansible" class="wiki-anchor">&para;</a></h1>


	<a name="Ansible_検証用環境Install手順"></a>
<h4 >検証用環境Install手順<a href="#Ansible_検証用環境Install手順" class="wiki-anchor">&para;</a></h4>


	<p><strong>■Installする環境について</strong><br /><strong><em>共通設定でInstallされたサーバに対しansibleをInstallする<br />ssh接続に関しては鍵認証にて接続できるように設定しておく<br />~/.ssh/configにservermanagerユーザで鍵認証にて接続できるように設定をしておく</em></strong></p>


	<p><strong>■Installするバージョン</strong></p>


<blockquote>

	<p>[root@ANSIBLESRV ansible]# ansible --version<br />[WARNING]: The version of gmp you have installed has a known issue regarding<br />timing vulnerabilities when used with pycrypto. If possible, you should update<br />it (i.e. yum update gmp).</p>


	<p>ansible 1.8.2<br />configured module search path = None</p>


</blockquote>

	<p><strong>■まずはパッケージのダウンロードだけする</strong></p>


	<p>rpm -ivh <a class="external" href="http://ftp.riken.jp/Linux/fedora/epel/6/i386/epel-release-6-8.noarch.rpm">http://ftp.riken.jp/Linux/fedora/epel/6/i386/epel-release-6-8.noarch.rpm</a><br />yum install --downloadonly --downloaddir=/home/package/ansible ansible</p>


<blockquote>

[root@ANSIBLESRV ansible]# rpm -ivh <a class="external" href="http://ftp.riken.jp/Linux/fedora/epel/6/i386/epel-release-6-8.noarch.rpm">http://ftp.riken.jp/Linux/fedora/epel/6/i386/epel-release-6-8.noarch.rpm</a><br /><a class="external" href="http://ftp.riken.jp/Linux/fedora/epel/6/i386/epel-release-6-8.noarch.rpm">http://ftp.riken.jp/Linux/fedora/epel/6/i386/epel-release-6-8.noarch.rpm</a> を取得中<br />警告: /var/tmp/rpm-tmp.a9Yomr: ヘッダ V3 RSA/SHA256 Signature, key ID 0608b895: NOKEY<br />準備中...                ########################################### [100%]<br />1:epel-release           ########################################### [100%]<br />[root@ANSIBLESRV ansible]# yum install --downloadonly --downloaddir=/home/package/ansible ansible<br />読み込んだプラグイン:downloadonly, fastestmirror, security<br />インストール処理の設定をしています<br />Loading mirror speeds from cached hostfile<br />epel/metalink                                                                 | 4.6 kB     00:00
	<ul>
	<li>base: ftp.riken.jp</li>
		<li>epel: ftp.kddilabs.jp</li>
		<li>extras: ftp.riken.jp</li>
		<li>updates: ftp.riken.jp<br />epel                                                                          | 4.4 kB     00:00<br />epel/primary_db                                                               | 6.4 MB     00:00<br />依存性の解決をしています<br />--> トランザクションの確認を実行しています。<br />---> Package ansible.noarch 0:1.8.2-3.el6 will be インストール<br />--> 依存性の処理をしています: python-simplejson のパッケージ: ansible-1.8.2-3.el6.noarch<br />--> 依存性の処理をしています: python-setuptools のパッケージ: ansible-1.8.2-3.el6.noarch<br />--> 依存性の処理をしています: python-paramiko のパッケージ: ansible-1.8.2-3.el6.noarch<br />--> 依存性の処理をしています: python-keyczar のパッケージ: ansible-1.8.2-3.el6.noarch<br />--> 依存性の処理をしています: python-jinja2 のパッケージ: ansible-1.8.2-3.el6.noarch<br />--> 依存性の処理をしています: python-httplib2 のパッケージ: ansible-1.8.2-3.el6.noarch<br />--> 依存性の処理をしています: python-crypto2.6 のパッケージ: ansible-1.8.2-3.el6.noarch<br />--> 依存性の処理をしています: PyYAML のパッケージ: ansible-1.8.2-3.el6.noarch<br />--> トランザクションの確認を実行しています。<br />---> Package PyYAML.x86_64 0:3.10-3.1.el6 will be インストール<br />--> 依存性の処理をしています: libyaml-0.so.2()(64bit) のパッケージ: PyYAML-3.10-3.1.el6.x86_64<br />---> Package python-crypto2.6.x86_64 0:2.6.1-1.el6 will be インストール<br />---> Package python-httplib2.noarch 0:0.7.7-1.el6 will be インストール<br />---> Package python-jinja2.x86_64 0:2.2.1-2.el6_5 will be インストール<br />--> 依存性の処理をしています: python-babel >= 0.8 のパッケージ: python-jinja2-2.2.1-2.el6_5.x86_64<br />---> Package python-keyczar.noarch 0:0.71c-1.el6 will be インストール<br />--> 依存性の処理をしています: python-pyasn1 のパッケージ: python-keyczar-0.71c-1.el6.noarch<br />--> 依存性の処理をしています: python-crypto のパッケージ: python-keyczar-0.71c-1.el6.noarch<br />---> Package python-paramiko.noarch 0:1.7.5-2.1.el6 will be インストール<br />---> Package python-setuptools.noarch 0:0.6.10-3.el6 will be インストール<br />---> Package python-simplejson.x86_64 0:2.0.9-3.1.el6 will be インストール<br />--> トランザクションの確認を実行しています。<br />---> Package libyaml.x86_64 0:0.1.3-4.el6_6 will be インストール<br />---> Package python-babel.noarch 0:0.9.4-5.1.el6 will be インストール<br />---> Package python-crypto.x86_64 0:2.0.1-22.el6 will be インストール<br />---> Package python-pyasn1.noarch 0:0.0.12a-1.el6 will be インストール<br />--> 依存性解決を終了しました。</li>
	</ul>


	<p>依存性を解決しました</p>


	<p>=====================================================================================================<br />パッケージ                   アーキテクチャ    バージョン                  リポジトリー        容量
=====================================================================================================<br />インストールしています:<br />ansible                      noarch            1.8.2-3.el6                 epel               1.5 M<br />依存性関連でのインストールをします。:<br />PyYAML                       x86_64            3.10-3.1.el6                updates            157 k<br />libyaml                      x86_64            0.1.3-4.el6_6               updates             52 k<br />python-babel                 noarch            0.9.4-5.1.el6               base               1.4 M<br />python-crypto                x86_64            2.0.1-22.el6                base               159 k<br />python-crypto2.6             x86_64            2.6.1-1.el6                 epel               530 k<br />python-httplib2              noarch            0.7.7-1.el6                 epel                70 k<br />python-jinja2                x86_64            2.2.1-2.el6_5               base               466 k<br />python-keyczar               noarch            0.71c-1.el6                 epel               219 k<br />python-paramiko              noarch            1.7.5-2.1.el6               base               728 k<br />python-pyasn1                noarch            0.0.12a-1.el6               base                70 k<br />python-setuptools            noarch            0.6.10-3.el6                base               336 k<br />python-simplejson            x86_64            2.0.9-3.1.el6               base               126 k</p>


	<p>トランザクションの要約
=====================================================================================================<br />インストール        13 パッケージ</p>


	<p>総ダウンロード容量: 5.8 M<br />インストール済み容量: 28 M<br />これでいいですか? [y/N]y<br />パッケージをダウンロードしています:<br />(1/13): PyYAML-3.10-3.1.el6.x86_64.rpm                                        | 157 kB     00:00<br />(2/13): ansible-1.8.2-3.el6.noarch.rpm                                        | 1.5 MB     00:00<br />(3/13): libyaml-0.1.3-4.el6_6.x86_64.rpm                                      |  52 kB     00:00<br />(4/13): python-babel-0.9.4-5.1.el6.noarch.rpm                                 | 1.4 MB     00:00<br />(5/13): python-crypto-2.0.1-22.el6.x86_64.rpm                                 | 159 kB     00:00<br />(6/13): python-crypto2.6-2.6.1-1.el6.x86_64.rpm                               | 530 kB     00:00<br />(7/13): python-httplib2-0.7.7-1.el6.noarch.rpm                                |  70 kB     00:00<br />(8/13): python-jinja2-2.2.1-2.el6_5.x86_64.rpm                                | 466 kB     00:00<br />(9/13): python-keyczar-0.71c-1.el6.noarch.rpm                                 | 219 kB     00:00<br />(10/13): python-paramiko-1.7.5-2.1.el6.noarch.rpm                             | 728 kB     00:00<br />(11/13): python-pyasn1-0.0.12a-1.el6.noarch.rpm                               |  70 kB     00:00<br />(12/13): python-setuptools-0.6.10-3.el6.noarch.rpm                            | 336 kB     00:00<br />(13/13): python-simplejson-2.0.9-3.1.el6.x86_64.rpm                           | 126 kB     00:00<br />-----------------------------------------------------------------------------------------------------<br />合計                                                                 3.6 MB/s | 5.8 MB     00:01</p>


	<p>exiting because --downloadonly specified<br />[root@ANSIBLESRV ansible]# ls<br />PyYAML-3.10-3.1.el6.x86_64.rpm           python-jinja2-2.2.1-2.el6_5.x86_64.rpm<br />ansible-1.8.2-3.el6.noarch.rpm           python-keyczar-0.71c-1.el6.noarch.rpm<br />libyaml-0.1.3-4.el6_6.x86_64.rpm         python-paramiko-1.7.5-2.1.el6.noarch.rpm<br />python-babel-0.9.4-5.1.el6.noarch.rpm    python-pyasn1-0.0.12a-1.el6.noarch.rpm<br />python-crypto-2.0.1-22.el6.x86_64.rpm    python-setuptools-0.6.10-3.el6.noarch.rpm<br />python-crypto2.6-2.6.1-1.el6.x86_64.rpm  python-simplejson-2.0.9-3.1.el6.x86_64.rpm<br />python-httplib2-0.7.7-1.el6.noarch.rpm</p>


</blockquote>

	<p><strong>■ダウンロードしたパッケージをInstallする</strong></p>


<blockquote>

	<p>[root@ANSIBLESRV ansible]# cd /home/package/ansible/<br />[root@ANSIBLESRV ansible]# ls<br />PyYAML-3.10-3.1.el6.x86_64.rpm           python-jinja2-2.2.1-2.el6_5.x86_64.rpm<br />ansible-1.8.2-3.el6.noarch.rpm           python-keyczar-0.71c-1.el6.noarch.rpm<br />libyaml-0.1.3-4.el6_6.x86_64.rpm         python-paramiko-1.7.5-2.1.el6.noarch.rpm<br />python-babel-0.9.4-5.1.el6.noarch.rpm    python-pyasn1-0.0.12a-1.el6.noarch.rpm<br />python-crypto-2.0.1-22.el6.x86_64.rpm    python-setuptools-0.6.10-3.el6.noarch.rpm<br />python-crypto2.6-2.6.1-1.el6.x86_64.rpm  python-simplejson-2.0.9-3.1.el6.x86_64.rpm<br />python-httplib2-0.7.7-1.el6.noarch.rpm<br />[root@ANSIBLESRV ansible]# rpm -ivh ./*<br />警告: ./ansible-1.8.2-3.el6.noarch.rpm: ヘッダ V3 RSA/SHA256 Signature, key ID 0608b895: NOKEY<br />準備中...                ########################################### [100%]<br />1:python-crypto          ########################################### [  8%]<br />2:python-paramiko        ########################################### [ 15%]<br />3:python-simplejson      ########################################### [ 23%]<br />4:python-setuptools      ########################################### [ 31%]<br />5:python-pyasn1          ########################################### [ 38%]<br />6:python-keyczar         ########################################### [ 46%]<br />7:python-httplib2        ########################################### [ 54%]<br />8:python-crypto2.6       ########################################### [ 62%]<br />9:python-babel           ########################################### [ 69%]<br />10:python-jinja2          ########################################### [ 77%]<br />11:libyaml                ########################################### [ 85%]<br />12:PyYAML                 ########################################### [ 92%]<br />13:ansible                ########################################### [100%]</p>


</blockquote>

	<p><strong>■疎通確認</strong></p>


<blockquote>

	<p>[root@ANSIBLESRV ansible]# ansible 192.168.100.216 -m ping<br />[WARNING]: The version of gmp you have installed has a known issue regarding<br />timing vulnerabilities when used with pycrypto. If possible, you should update<br />it (i.e. yum update gmp).</p>


	<p>No hosts matched</p>


</blockquote>

	<p><strong><em>warningが出ているがAnsible事態の動作に問題はないらしい。<br />結果的にはNo hosts matchedが出力されて失敗。<br />Ansibleには独自のHostsファイルが用意されていてそれを編集しないと<br />疎通できないらしい<br />/etc/ansible/hosts<br />編集してみる。デフォルトの行はすべてコメントアウト。<br />通信対象のIPを追記</em></strong></p>


<blockquote>

[root@ANSIBLESRV ansible]# cat /etc/ansible/hosts
	<ol>
	<li>This is the default ansible 'hosts' file.
#</li>
		<li>It should live in /etc/ansible/hosts
#</li>
		<li>  - Comments begin with the '#' character</li>
		<li>  - Blank lines are ignored</li>
		<li>  - Groups of hosts are delimited by [header] elements</li>
		<li>  - You can enter hostnames or ip addresses</li>
		<li>  - A hostname/ip can be a member of multiple groups</li>
	</ol>


	<ol>
	<li>Ex 1: Ungrouped hosts, specify before any group headers.</li>
	</ol>


	<p>#green.example.com<br />#blue.example.com<br />#192.168.100.1<br />#192.168.100.10</p>


	<ol>
	<li>Ex 2: A collection of hosts belonging to the 'webservers' group</li>
	</ol>


	<p>#[webservers]<br />#alpha.example.org<br />#beta.example.org<br />#192.168.1.100<br />#192.168.1.110</p>


	<ol>
	<li>If you have multiple hosts following a pattern you can specify</li>
		<li>them like this:</li>
	</ol>


	<p>#www[001:006].example.com</p>


	<ol>
	<li>Ex 3: A collection of database servers in the 'dbservers' group</li>
	</ol>


	<p>#[dbservers]</p>


	<p>#db01.intranet.mydomain.net<br />#db02.intranet.mydomain.net<br />#10.25.1.56<br />#10.25.1.57</p>


	<ol>
	<li>Here's another example of host ranges, this time there are no</li>
		<li>leading 0s:</li>
	</ol>


	<p>#db-[99:101]-node.example.com</p>


	<p>#hosts<br />192.168.100.216</p>


</blockquote>

	<p>再度試す</p>


<blockquote>

	<p>[root@ANSIBLESRV ansible]# ansible 192.168.100.216 --user=servermanager --private-key=/home/servermanager/.ssh/id_rsa -m ping<br />[WARNING]: The version of gmp you have installed has a known issue regarding<br />timing vulnerabilities when used with pycrypto. If possible, you should update<br />it (i.e. yum update gmp).</p>


	<p>192.168.100.216 | success >> {<br />"changed": false,<br />"ping": "pong" <br />}</p>


</blockquote>

	<p><strong><em>うまくいった。コマンドを実行してみる</em></strong></p>


<blockquote>

	<p>[root@ANSIBLESRV ansible]# ansible -i hosts all --user=servermanager --private-key=/home/servermanager/.ssh/id_rsa -a 'uname -a'<br />[WARNING]: The version of gmp you have installed has a known issue regarding<br />timing vulnerabilities when used with pycrypto. If possible, you should update<br />it (i.e. yum update gmp).</p>


	<p>192.168.100.216 | success | rc=0 >><br />Linux TESTSRV1 2.6.32-504.el6.x86_64 #1 SMP Wed Oct 15 04:27:16 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux</p>


</blockquote>

	<p><strong><em>yumコマンドを実行してみる</em></strong></p>


<blockquote>

	<p>[root@ANSIBLESRV ansible]# ansible -i hosts all --user=servermanager --private-key=/home/servermanager/.ssh/id_rsa -a 'yum search httpd'<br />[WARNING]: The version of gmp you have installed has a known issue regarding<br />timing vulnerabilities when used with pycrypto. If possible, you should update<br />it (i.e. yum update gmp).</p>


192.168.100.216 | success | rc=0 >><br />Loaded plugins: fastestmirror, security<br />Loading mirror speeds from cached hostfile
	<ul>
	<li>base: <a class="external" href="http://www.ftp.ne.jp">www.ftp.ne.jp</a></li>
		<li>extras: <a class="external" href="http://www.ftp.ne.jp">www.ftp.ne.jp</a></li>
		<li>updates: <a class="external" href="http://www.ftp.ne.jp">www.ftp.ne.jp</a>
============================== N/S Matched: httpd ==============================<br />libmicrohttpd-devel.i686 : Development files for libmicrohttpd<br />libmicrohttpd-devel.x86_64 : Development files for libmicrohttpd<br />libmicrohttpd-doc.noarch : Documentation for libmicrohttpd<br />httpd.x86_64 : Apache HTTP Server<br />httpd-devel.i686 : Development interfaces for the Apache HTTP server<br />httpd-devel.x86_64 : Development interfaces for the Apache HTTP server<br />httpd-manual.noarch : Documentation for the Apache HTTP server<br />httpd-tools.x86_64 : Tools for use with the Apache HTTP Server<br />libmicrohttpd.i686 : Lightweight library for embedding a webserver in<br />: applications<br />libmicrohttpd.x86_64 : Lightweight library for embedding a webserver in<br />: applications<br />mod_auth_mellon.x86_64 : A SAML 2.0 authentication module for the Apache Httpd<br />: Server<br />mod_dav_svn.x86_64 : Apache httpd module for Subversion server<br />mod_dnssd.x86_64 : An Apache HTTPD module which adds Zeroconf support</li>
	</ul>


	<p>Name and summary matches only, use "search all" for everything.</p>


</blockquote>

	<p><strong><em>うまくいく。<br />プレイブックを試してみる</em></strong></p>


<blockquote>

	<p>[root@ANSIBLESRV playbook]# cat /var/tmp/playbook/test-playbook.yml<br />---<br />- hosts: test-servers<br />sudo: yes<br />tasks:<br />- name: be sure httpd is installed<br />yum: name=httpd state=installed</p>


	<p>- name: be sure httpd is running and enabled<br />service: name=httpd state=running enabled=yes</p>


</blockquote>

	<p><strong><em>シンタックスチェック実行</em></strong></p>


<blockquote>

	<p>[root@ANSIBLESRV playbook]# ansible-playbook test-playbook.yml --syntax-check<br />[WARNING]: The version of gmp you have installed has a known issue regarding<br />timing vulnerabilities when used with pycrypto. If possible, you should update<br />it (i.e. yum update gmp).</p>


	<p>playbook: test-playbook.yml</p>


	<p>[root@ANSIBLESRV playbook]# ansible-playbook test-playbook.yml --list-tasks<br />[WARNING]: The version of gmp you have installed has a known issue regarding<br />timing vulnerabilities when used with pycrypto. If possible, you should update<br />it (i.e. yum update gmp).</p>


	<p>playbook: test-playbook.yml</p>


	<p>play #1 (test-servers):<br />be sure httpd is installed<br />be sure httpd is running and enabled</p>


</blockquote>

	<p><strong><em>ドライラン実行</em></strong></p>


<blockquote>

	<p>[root@ANSIBLESRV playbook]# ansible-playbook --user=servermanager --private-key=/home/servermanager/.<br />ssh/id_rsa test-playbook.yml --check<br />[WARNING]: The version of gmp you have installed has a known issue regarding<br />timing vulnerabilities when used with pycrypto. If possible, you should update<br />it (i.e. yum update gmp).</p>


	<p>PLAY [test-servers] *<strong><b></strong>*</b>**<strong>**</strong>*********************************************</p>


	<p>GATHERING FACTS *<strong><b></strong>*</b>**<strong>**</strong>*************************************************<br />ok: [192.168.100.216]</p>


	<p>TASK: [be sure httpd is installed] *<strong><b></strong>*</b>**<strong>**</strong>******************************<br />changed: [192.168.100.216]</p>


	<p>TASK: [be sure httpd is running and enabled] *<strong><b></strong>*</b>**<strong>**</strong>********************<br />failed: [192.168.100.216] => {"failed": true}<br />msg: cannot find 'service' binary or init script for service,  possible typo in service name?, aborting</p>


	<p>FATAL: all hosts have already failed -- aborting</p>


	<p>PLAY RECAP *<strong><b></strong>*</b>**<strong>**</strong>******************************************************<br />to retry, use: --limit @/root/test-playbook.retry</p>


	<p>192.168.100.216            : ok=2    changed=1    unreachable=0    failed=1</p>


</blockquote>

	<p><strong><em>うまくいきそうです<br />本番実行してみる</em></strong></p>


<blockquote>

	<p>[root@ANSIBLESRV playbook]# ansible-playbook --user=servermanager --private-key=/home/servermanager/.ssh/id_rsa test-playbook.yml<br />[WARNING]: The version of gmp you have installed has a known issue regarding<br />timing vulnerabilities when used with pycrypto. If possible, you should update<br />it (i.e. yum update gmp).</p>


	<p>PLAY [test-servers] *<strong><b></strong>*</b>**<strong>**</strong>*********************************************</p>


	<p>GATHERING FACTS *<strong><b></strong>*</b>**<strong>**</strong>*************************************************<br />ok: [192.168.100.216]</p>


	<p>TASK: [be sure httpd is installed] *<strong><b></strong>*</b>**<strong>**</strong>******************************<br />changed: [192.168.100.216]</p>


	<p>TASK: [be sure httpd is running and enabled] *<strong><b></strong>*</b>**<strong>**</strong>********************<br />changed: [192.168.100.216]</p>


	<p>PLAY RECAP *<strong><b></strong>*</b>**<strong>**</strong>******************************************************<br />192.168.100.216            : ok=3    changed=2    unreachable=0    failed=0</p>


</blockquote>

	<p><strong><em>成功した。<br />Installできているか確認する。</em></strong></p>


<blockquote>

	<p>[root@TESTSRV1 ~]# rpm -qa |grep httpd<br />httpd-2.2.15-39.el6.centos.x86_64<br />httpd-tools-2.2.15-39.el6.centos.x86_64<br />[root@TESTSRV1 ~]# ps auxf | grep httpd<br />root     30872  0.0  0.0 107464   904 pts/0    S+   11:06   0:00  |       \_ grep httpd<br />root     30856  0.0  0.2 177812  3876 ?        Ss   11:05   0:00 /usr/sbin/httpd<br />apache   30858  0.0  0.1 177812  2480 ?        S    11:05   0:00  \_ /usr/sbin/httpd<br />apache   30859  0.0  0.1 177812  2480 ?        S    11:05   0:00  \_ /usr/sbin/httpd<br />apache   30860  0.0  0.1 177812  2480 ?        S    11:05   0:00  \_ /usr/sbin/httpd<br />apache   30861  0.0  0.1 177812  2480 ?        S    11:05   0:00  \_ /usr/sbin/httpd<br />apache   30862  0.0  0.1 177812  2480 ?        S    11:05   0:00  \_ /usr/sbin/httpd<br />apache   30863  0.0  0.1 177812  2480 ?        S    11:05   0:00  \_ /usr/sbin/httpd<br />apache   30865  0.0  0.1 177812  2480 ?        S    11:05   0:00  \_ /usr/sbin/httpd<br />apache   30866  0.0  0.1 177812  2480 ?        S    11:05   0:00  \_ /usr/sbin/httpd<br />[root@TESTSRV1 ~]# chkconfig --list | grep httpd<br />httpd           0:off   1:off   2:on    3:on    4:on    5:on    6:off</p>


</blockquote>

	<p><strong>■rootユーザで配布したい場合は</strong></p>


	<p>/etc/ansible/hostsに以下を追加</p>


<blockquote>

	<p>[root-servers]<br />TESTSRV1<br />[root-servers:vars]<br />ansible_ssh_user=servermanager<br />ansible_ssh_port=22<br />ansible_ssh_pass=Passwprd123<br />ansible_sudo_pass=Passwprd123</p>


</blockquote>

	<p>以下のパッケージをInstallする</p>


<blockquote>

	<p>yum install --downloadonly --downloaddir=/var/tmp/package/sshpass/ sshpass<br />cd /var/tmp/package/sshpass/<br />rpm -ivh sshpass-1.05-1.el6.x86_64.rpm</p>


</blockquote>

	<p>こんな感じで実行する</p>


<blockquote>

	<p>ansible-playbook root-playbook.yml -k -c paramiko --check</p>


</blockquote>
<hr />
<a name="Ansible設計" />
<a name="Ansible設計_Ansible設計"></a>
<h1 >Ansible設計<a href="#Ansible設計_Ansible設計" class="wiki-anchor">&para;</a></h1>


	<p>ここでは、ansibleを使用する上での運用ルールを定義する。</p>


	<a name="Ansible設計_anbileサーバについて"></a>
<h2 >anbileサーバについて<a href="#Ansible設計_anbileサーバについて" class="wiki-anchor">&para;</a></h2>


	<a name="Ansible設計_ネットワーク"></a>
<h3 >ネットワーク<a href="#Ansible設計_ネットワーク" class="wiki-anchor">&para;</a></h3>


	<p>ansibleパッケージをインストールするサーバはクラウド管理サーバとし、各通信を行うネットワークは<br />クラウド管理ネットワークを使用する。</p>


	<a name="Ansible設計_ディレクトリ構成"></a>
<h3 >ディレクトリ構成<a href="#Ansible設計_ディレクトリ構成" class="wiki-anchor">&para;</a></h3>


	<p>ansibleを構成するディレクトリ構成は以下の通りとする。</p>


<blockquote>

	<p>playbook/<br />├ roles<br />｜ ├ ミドルウェア名称ディレクトリ<br />｜ ｜ ├ files<br />｜ ｜ ├ handlers<br />｜ ｜ ｜   └ main.yml<br />｜ ｜ └ tasks<br />｜ ｜    -- └ main.yml<br />｜ ├ bind<br />｜ ├ common<br />｜ ├ ntpd<br />｜ └ postgresql<br />├ production<br />├ production_initialize<br />└ (サーバ種別名).yml</p>


</blockquote>

	<ul>
	<li>playbook<br />プレイブックを保存する基本ディレクトリ<br />(サーバ種別名).ymlを配置する</li>
	</ul>


	<ul>
	<li>(サーバ種別名).yml<br />大元となるプレイブック。ansible‐playbookを実行する際にはこのファイルを指定する。<br />サーバ種別名をファイル名に付けサーバ種別ごとに作成する。</li>
	</ul>


	<ul>
	<li>roles<br />(サーバ種別名).ymlから呼び出すミドルウェアごとに細分化されたプレイブックを配置する。<br />(サーバ種別名).ymlでは「roles:」でこのディレクトリを指定する。</li>
	</ul>


	<ul>
	<li>ミドルウェア名称ディレクトリ<br />ミドルウェアごとに細分化されたプレイブックを配置する。<br />名称にミドルウェアの名称を付与しミドルウェアごとに存在する。<br />(サーバ種別名).ymlでは「roles: ミドルウェア名称ディレクトリ」でこのディレクトリを指定する。</li>
	</ul>


	<ul>
	<li>tasks<br />プレイブックの「tasks:」に該当するディレクトリ。<br />main.ymlを配置することによりプレイブック内に「tasks:」を指定しなくてもtasksとして読み込まれる。</li>
	</ul>


	<ul>
	<li>handlers<br />「notify:」により呼び出されるディレクトリ。<br />main.ymlを配置し「name:」で定義した値で「notify:」から呼び出す。</li>
	</ul>


	<ul>
	<li>files<br />配布用ファイルを配置する。</li>
	</ul>


	<ul>
	<li>main.yml<br />各役割に該当する内容を記述する。<br />ディレクトリに配置することにより暗黙的に読み込まれる。</li>
	</ul>


	<ul>
	<li>production<br />ホストをグルーピングするためのファイル。<br />設定対象ホストをホスト名称で記載する。</li>
	</ul>


	<ul>
	<li>production_initialize<br />初期配布用のグルーピングファイル。</li>
	</ul>


	<ul>
	<li><del>common</del><br /><del>サーバの共通編で構成されるミドルウェアのインストール設定ファイルを配置する。ファイル名は<ミドルウェア名.yml>とする。</del></li>
		<li><del>middle</del><br /><del>各サーバで個別に必要なミドルウェアのインストール用ansibleファイルを配置する。ファイル名は&lt;hostname.yml&gt;とする。</del></li>
		<li><del>host list</del><br /><del>ansibleで管理対象となるサーバを構成するファイルを配置する。各サーバの設定ファイルには構築に必要なミドルウェアインストール用の<br />ansibleファイルを呼び出す設定が書かれており、それらインストールを組み合わせてサーバを構築する。ファイル名は<ミドルウェア.yml>とする。</del></li>
	</ul>


	<a name="Ansible設計_プレイブックについて"></a>
<h2 >プレイブックについて<a href="#Ansible設計_プレイブックについて" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>001_initialize_playbook.yml<br />初期設定を行うプレイブック。以下のrolesを呼び出しする。</li>
	</ul>


<blockquote>

	<p>roles:<br />- selinux<br />- users<br />- sudoers<br />- sshd</p>


</blockquote>

	<ul>
	<li>002_commonsetting_playbook.yml<br />共通設定を行うプレイブック。以下のrolesを呼び出しする。</li>
	</ul>


<blockquote>

	<p>roles:<br />- common</p>


</blockquote>

	<ul>
	<li>003_サーバ種別名_playbook.yml<br />ミドルウェアの設定を行うプレイブック。以下のrolesを呼び出しする。</li>
	</ul>


	<a name="Ansible設計_ansibleモジュールについて"></a>
<h2 >ansibleモジュールについて<a href="#Ansible設計_ansibleモジュールについて" class="wiki-anchor">&para;</a></h2>


	<p><a class="external" href="http://docs.ansible.com/list_of_all_modules.html">http://docs.ansible.com/list_of_all_modules.html</a></p>


	<a name="Ansible設計_rolesについて"></a>
<h2 >rolesについて<a href="#Ansible設計_rolesについて" class="wiki-anchor">&para;</a></h2>


<strong>注意点</strong>
	<ol>
	<li>filesなどを利用する場合は相対パスで指定すること</li>
	</ol>


	<ul>
	<li>selinux<br />tasks:</li>
	</ul>


<blockquote>

	<p>- name: install python-selinux<br />yum: pkg=libselinux-python state=latest   ←このモジュールを入れないとAnsibleからSELinuxをコントロールできない</p>


	<p>- name: selinux disabled<br />selinux: policy=targeted state=disabled</p>


</blockquote>

	<ul>
	<li>users<br />tasks:</li>
	</ul>


<blockquote>

	<p>- name: make isb group<br />group: name=isb gid=1001</p>


	<p>- name: make servermanager group<br />group: name=servermanager gid=1002</p>


	<p>- name: add a ta1 user<br />user: name=ta1 password=$1$wiJrevNJ$nEgYx5zM1XrptfSDu2Ehl1 state=present shell=/bin/bash group=isb uid=1001 home=/home/ta1</p>


	<p>- name: add a suga user<br />user: name=suga password=$1$qYQ7aKcq$3FrFWq.CfLn5bTWOEImNN/ state=present shell=/bin/bash group=isb uid=1002 home=/home/suga</p>


	<p>- name: add a kajiro user<br />user: name=kajiro password=$1$1iig82jZ$SEe6C0.hkLY2rgf6DHfKZ. state=present shell=/bin/bash group=isb uid=1003 home=/home/kajiro</p>


	<p>- name: add a akiba user<br />user: name=akiba password=$1$YuwPJKND$bmIAXDgEvEiX2dbm8fT2m/ state=present shell=/bin/bash group=isb uid=1004 home=/home/akiba</p>


	<p>- name: add a servermanager user<br />user: name=servermanager password=$1$Wg0Z66H5$xt27xv.3KrnMhCX.dwk2x1 state=present shell=/bin/bash group=servermanager uid=1101 home=/home/servermanager</p>


</blockquote>

	<ul>
	<li>sudoers<br />tasks:</li>
	</ul>


<blockquote>

	<p>- name: add a sudo user<br />lineinfile: "dest=/etc/sudoers backup=yes state=present regexp='^%isb' line='%isb ALL=(ALL) ALL'"</p>


	<p>- name: add a sudo user<br />lineinfile: "dest=/etc/sudoers backup=yes state=present regexp='^servermanager' line='servermanager ALL=(ALL) NOPASSWD: ALL'"</p>


</blockquote>

	<ul>
	<li>sshd<br />tasks:</li>
	</ul>


<blockquote>

	<p>- name: sshd file copy<br />copy: src=../files/sshd_config dest=/etc/ssh/sshd_config owner=root group=root mode=0600<br />notify: restart sshd</p>


	<p>- name: authorized_keys file copy<br />copy: src=../files/authorized_keys dest=/home/servermanager/.ssh/authorized_keys owner=servermanager group=servermanager mode=0600</p>


	<p>- name: /home/servermanager/.ssh setting<br />file: path=/home/servermanager/.ssh owner=servermanager group=servermanager mode=0700</p>


</blockquote>

	<p>handlers:</p>


<blockquote>

	<p>- name: "restart sshd" <br />service: name=sshd state=restarted</p>


</blockquote>

	<p>files:</p>


<blockquote>

	<p>[root@ANSIBLESRV files]# ll<br />合計 8<br /><del>rw------</del>. 1 servermanager servermanager  406  2月 17 21:13 2015 authorized_keys<br /><del>rw-r--r-</del>. 1 root          root          3908  2月 24 15:07 2015 sshd_config</p>


</blockquote>

	<ul>
	<li>common<br />tasks:</li>
	</ul>


<blockquote>

	<p>- name: file copy<br />copy: src=../files/{{ item.srcpath }} dest={{ item.destpath }} owner=root group=root mode=0644<br />with_items:<br />- { srcpath: 'hosts', destpath: '/etc/hosts' }<br />- { srcpath: 'resolv.conf', destpath: '/etc/resolv.conf' }<br />- { srcpath: 'disable-ipv6.conf', destpath: '/etc/modprobe.d/disable-ipv6.conf' }<br />- { srcpath: 'CentOS-Base.repo', destpath: '/etc/yum.repos.d/CentOS-Base.repo' }<br />- { srcpath: 'control-alt-delete.override', destpath: '/etc/init/control-alt-delete.override' }<br />- { srcpath: 'ntp.conf', destpath: '/etc/ntp.conf' }<br />- { srcpath: 'sysstat', destpath: '/etc/sysconfig/sysstat' }</p>


	<p>- name: service stop<br />service: name={{ item }} state=stopped enabled=no<br />with_items:<br />- auditd<br />- haldaemon<br />- ip6tables<br />- iptables<br />- mdmonitor<br />- messagebus<br />- postfix</p>


</blockquote>

	<p>files:</p>


<blockquote>

	<p><del>rw-r--r-</del>. 1 root root  876  2月 23 13:22 2015 CentOS-Base.repo<br /><del>rw-r--r-</del>. 1 root root   42  2月 23 14:23 2015 control-alt-delete.override<br /><del>rw-r--r-</del>. 1 root root   23  2月 20 14:18 2015 disable-ipv6.conf<br /><del>rw-r--r-</del>. 1 root root 1739  2月 24 14:11 2015 hosts<br /><del>rw-r--r-</del>. 1 root root 1909  2月 24 15:11 2015 ntp.conf<br /><del>rw-r--r-</del>. 1 root root   46  2月 20 14:15 2015 resolv.conf<br /><del>rw-r--r-</del>. 1 root root  473  2月 24 15:00 2015 sysstat</p>


</blockquote>

	<a name="Ansible設計_配布の仕方"></a>
<h2 >配布の仕方<a href="#Ansible設計_配布の仕方" class="wiki-anchor">&para;</a></h2>


	<p>rootで配る場合<br />ansible-playbook -i production_initialize 001_initialize_playbook.yml -k -c paramiko</p>


	<p>servermanagerユーザで配る場合</p>


	<p>ansible-playbook -i production 002_commonsetting_playbook.yml</p>
<hr />
<a name="Bind" />
<a name="Bind_Bind"></a>
<h1 >Bind<a href="#Bind_Bind" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#Bind_Bind">Bind</a></li><li><a href="#Bind_SDBI-Simplified-Database-Interfaceの導入方法DBPostgreSQL">SDBI (Simplified Database Interface）の導入方法(DB:PostgreSQL)</a><ul><li><ul><li><ul><li><a href="#Bind_PostgreSQLの設定">PostgreSQLの設定</a></li><li><a href="#Bind_データベース設定">データベース設定</a></li><li><a href="#Bind_BINDSDBIのインストール">BIND・SDBIのインストール</a></li><li><a href="#Bind_BINDの設定">BINDの設定</a></li><li><a href="#Bind_正引き逆引きの確認">正引き・逆引きの確認</a></li></ul>
</li></ul>
</li></ul>
</li><li><a href="#Bind_DNSへの攻撃手法">DNSへの攻撃手法</a><ul><li><ul><li><ul><li><a href="#Bind_DNSキャッシュポイズニングカミンスキー攻撃">DNSキャッシュポイズニング(カミンスキー攻撃)</a></li><li><a href="#Bind_DNSアンプDNSリフレクター">DNSアンプ(DNSリフレクター)</a></li><li><a href="#Bind_DNSスプーフィング">DNSスプーフィング</a></li><li><a href="#Bind_DNSハイジャック">DNSハイジャック</a></li></ul></li></ul></li></ul></li></ul>


<hr />


	<a name="Bind_SDBI-Simplified-Database-Interfaceの導入方法DBPostgreSQL"></a>
<h1 >SDBI (Simplified Database Interface）の導入方法(DB:PostgreSQL)<a href="#Bind_SDBI-Simplified-Database-Interfaceの導入方法DBPostgreSQL" class="wiki-anchor">&para;</a></h1>


	<table>
		<tr>
			<td>PostgreSQLサーバ</td>
			<td>localhost</td>
			<td>　<br /></td>
			<td>データベース名</td>
			<td>bind_psql</td>
		</tr>
		<tr>
			<td>テーブル名(正引き用)</td>
			<td>isb_zone</td>
		</tr>
		<tr>
			<td>テーブル名(逆引き用)</td>
			<td>isb_rev</td>
		</tr>
		<tr>
			<td>データベース接続ユーザ名</td>
			<td>named</td>
		</tr>
		<tr>
			<td>データベース接続パスワード</td>
			<td>pass</td>
		</tr>
	</table>




	<a name="Bind_PostgreSQLの設定"></a>
<h4 >PostgreSQLの設定<a href="#Bind_PostgreSQLの設定" class="wiki-anchor">&para;</a></h4>


	<p>編集ファイル:/var/lib/pgsql/9.3/data/pg_hba.conf<br /><pre>
#変更
local   all             all                                     trust
host    all             all             127.0.0.1/32            trust
host    all             all             ::1/128                 trust
</pre></p>


	<p>PostgreSQL設定反映<br /><pre>
[root@localhost ~]# /etc/init.d/postgresql-9.3 restart
</pre></p>


	<a name="Bind_データベース設定"></a>
<h4 >データベース設定<a href="#Bind_データベース設定" class="wiki-anchor">&para;</a></h4>


	<p>データベース・データベースユーザ作成・データベースに接続</p>


<pre>
[root@localhost ~]# su - postgres

-bash-4.1$ createuser -d -U postgres -P named　　#データベースユーザ作成
Enter password for new role: pass　　　　　　　　#パスワード入力
Enter it again: pass　　　　　　　　　　　　　　 #パスワード再入力
-bash-4.1$ createdb -O named bind_psql　　　　　 #データベース作成
-bash-4.1$ psql -d bind_psql -U named　　　　　　#データベースに接続
</pre>

	<p>テーブル(正引き)作成<br /><pre>
bind_psql=&gt; create table isb_zone (name varchar(255) default NULL,ttl integer default NULL,rdtype varchar(255) default NULL,rdata varchar(255) default NULL);
bind_psql=&gt; grant all on isb_zone to named;
bind_psql=&gt; insert INTO isb_zone values ('isb.local','86400','SOA','isb.local. root.isb.local. 2014120101 3H 2M 1W 1D');
bind_psql=&gt; insert INTO isb_zone values ('isb.local','86400','NS','isb.local.');
bind_psql=&gt; insert INTO isb_zone values ('isb.local','86400','A','192.168.12.25');
bind_psql=&gt; select * from isb_zone;
   name    |  ttl  | rdtype |                       rdata
-----------+-------+--------+-------------------------------------------------------------------
 isb.local | 86400 | SOA    | isb.local. root.isb.local. 2014120101 3H 2M 1W 1D
 isb.local | 86400 | NS     | isb.local.
 isb.local | 86400 | A      | 192.168.12.25
</pre></p>


	<p>テーブル(逆引き)作成<br /><pre>
bind_psql=&gt; create table isb_rev (name varchar(255) default NULL,ttl integer default NULL,rdtype varchar(255) default NULL,rdata varchar(255) default NULL);
bind_psql=&gt; grant all on isb_rev to named;
bind_psql=&gt; insert INTO isb_rev values ('168.192.in-addr.arpa','86400','SOA','isb.local. root.isb.local. 2014120102 3H 2M 1W 1D');
bind_psql=&gt; insert INTO isb_rev values ('168.192.in-addr.arpa','86400','NS','isb.local.');
bind_psql=&gt; insert INTO isb_rev values ('25.12.168.192.in-addr.arpa','86400','PTR','isb.local.');
bind_psql=&gt; select * from isb_rev;
            name            |  ttl  | rdtype |                       rdata
----------------------------+-------+--------+---------------------------------------------------
 168.192.in-addr.arpa       | 86400 | SOA    | isb.local. root.isb.local. 2014120102 3H 2M 1W 1D
 168.192.in-addr.arpa       | 86400 | NS     | isb.local.
 25.12.168.192.in-addr.arpa | 86400 | PTR    | isb.local.
</pre></p>


	<a name="Bind_BINDSDBIのインストール"></a>
<h4 >BIND・SDBIのインストール<a href="#Bind_BINDSDBIのインストール" class="wiki-anchor">&para;</a></h4>


<pre>
[root@localhost ~]# yum -y install bind bind-sdb
[root@localhost ~]# chkconfig named on
</pre>

	<a name="Bind_BINDの設定"></a>
<h4 >BINDの設定<a href="#Bind_BINDの設定" class="wiki-anchor">&para;</a></h4>


	<p>編集ファイル:/etc/named.conf<br /><pre>
#追記
zone "isb.local" {
        type master;
        #書式 database "pgsql データベース名　テーブル名　ホスト名　データベース接続ユーザ名　ユーザパスワード" 
        database "pgsql bind_psql isb_zone localhost named pass";
};
zone "168.192.in-addr.arpa" {
        type master;
        database "pgsql bind_psql isb_rev localhost named pass";
};
　
　
#コメントアウト
#include "/etc/named.rfc1912.zones";　
</pre></p>


	<p>BIND設定反映<br /><pre>
[root@localhost ~]# /etc/init.d/named restart
</pre></p>


	<p>編集ファイル:/etc/resolv.conf<br /><pre>
#追記
nameserver 127.0.0.1   
</pre></p>


	<a name="Bind_正引き逆引きの確認"></a>
<h4 >正引き・逆引きの確認<a href="#Bind_正引き逆引きの確認" class="wiki-anchor">&para;</a></h4>


<pre>
[root@localhost ~]# nslookup isb.local
Server:         127.0.0.1
Address:        127.0.0.1#53

Name:   isb.local
Address: 192.168.12.25

[root@localhost ~]# nslookup 192.168.12.25
Server:         127.0.0.1
Address:        127.0.0.1#53

25.12.168.192.in-addr.arpa      name = isb.local.
</pre>

	<p>　</p>


<hr />


	<a name="Bind_DNSへの攻撃手法"></a>
<h1 >DNSへの攻撃手法<a href="#Bind_DNSへの攻撃手法" class="wiki-anchor">&para;</a></h1>


	<a name="Bind_DNSキャッシュポイズニングカミンスキー攻撃"></a>
<h4 >DNSキャッシュポイズニング(カミンスキー攻撃)<a href="#Bind_DNSキャッシュポイズニングカミンスキー攻撃" class="wiki-anchor">&para;</a></h4>


	<p>あるドメインについて偽の情報を発信し、インターネット上のDNSサーバに伝播させることにより、一般の利用者がそのドメイン内のサーバに到達できないようにしたり、ドメイン所有者の意図しない別のサーバにアクセスを誘導する手法。<br />・対策<br />キャッシュサーバに問い合わせ可能なクライアントを限定すること、ソースアドレスの偽装されたパケットを遮断することなどで、リスクを軽減することができる。<a href="https://www.nic.ad.jp/ja/newsletter/No40/0800.html" class="external">引用</a><br />DNSキャッシュサーバーの問い合わせUDPポートを固定せずランダム化すること(ソースポートランダマイゼーション)。カミンスキー型攻撃と呼ばれるDNSキャッシュポイズニングの攻撃手法に対する有効な対策法の一つとされる。<br />・主な事例<br />2008年7月29日、アメリカAT&#38;TのISPが運営するDNSキャッシュサーバの一部キャッシュが書き換えられ、このISPを使っている会社のネットワークからiGoogleにアクセスすると、インフレームで広告をロードさせるWebページに置き換えられるという事件が発生しました。表示されるWebページには不正プログラムは仕掛けられておらず、広告を繰り返し表示することで広告料収入増加を狙ったものと推測されています。この会社のDNSサーバはちゃんとパッチを適用していましたが、上位のISPのDNSサーバの情報が書き換えられてしまうと、そのISPを使っている会社や個人にも広く影響が及んでしまうことがわかります。<a href="http://www.webdbm.jp/column2009/column2009-03/2324/" class="external">引用</a></p>


	<a name="Bind_DNSアンプDNSリフレクター"></a>
<h4 >DNSアンプ(DNSリフレクター)<a href="#Bind_DNSアンプDNSリフレクター" class="wiki-anchor">&para;</a></h4>


	<p>DDoS（分散DoS攻撃）の一種で、DNSキャッシュサーバーの再帰的問合せ機能を悪用して、パケットを増幅（amplify）させ、大量のDNSパケットを生成して攻撃に利用する手法。<br />DNSアンプでは、DNSサーバーにサイズの大きいTXTレコードをキャッシュさせてから、ボットネットなどを利用して攻撃対象のサーバーのIPアドレスを詐称したDNS問合せを一斉送信し、キャッシュサーバーの応答を詐称された攻撃対象のサーバーに一斉に送信させることで、攻撃を仕掛ける。<br />・対策<br />DNSのオープンリレーな設定をやめること。オープンリレーとは，インターネットからの問い合わせをすべて受け付けて処理すること。<a href="http://itpro.nikkeibp.co.jp/article/COLUMN/20060905/247220/" class="external">引用</a><br />キャッシュサーバとコンテンツサーバに分離し，インターネット側からキャッシュサーバに問合せできないようにする。また、利用可能なホストのIPアドレスの範囲を設定するなど、DNSキャッシュサーバが不要なクエリを拒否するようにアクセス制限を施す必要があります。<a href="http://www.sc-siken.com/kakomon/24_haru/am2_14.html" class="external">引用</a><br />・主な事例<br />2013年3月18日にSpamhausのWebサイトを狙った攻撃がスタートし、Spamhausの援助要請を受けてCloudFlareが攻撃を緩和した。当時トラフィックにして10Gbps程度の攻撃だったが、3月19日には最大で90Gbpsに拡大した。この時点ですでに最大級の攻撃だったという。<br />少しの休止を経た後、22日に再開された攻撃ではピーク時に120Gpbsに。CloudFlareは分散技術を利用して攻撃に耐えた。<br />だがCloudFlareによると、攻撃者はその後戦術を変え、サイトを直接狙うのではなく、CloudFlareが帯域を利用するプロバイダーを攻撃した。Tier 1プロバイダーの中にはこの攻撃に関連したトラフィックを300Gbps以上観測したところもあったという。さらには、相互接続ポイントのインターネットエクスチェンジ(IX)にも攻撃が及んだという。ロンドン、フランクフルト、香港などのIXが標的になり、最大の影響を受けたと見られるロンドンIX(LIX)の場合トラフィック量が急速に減少したことがモニタリングからわかったという。この結果、ロンドンベースのCloudFlareの顧客から接続が断続的になるなどの問題が報告された。<a href="http://news.mynavi.jp/news/2013/03/28/142/" class="external">引用</a></p>


	<a name="Bind_DNSスプーフィング"></a>
<h4 >DNSスプーフィング<a href="#Bind_DNSスプーフィング" class="wiki-anchor">&para;</a></h4>


	<p>DNSサーバの脆弱性を悪用するなどして、ホスト名とIPアドレスの対応テーブルを書き換えたり、DNSプロトコルの脆弱性を悪用して偽のWebサイトにユーザーをアクセスさせたりする攻撃手法。<br />・対策<br />DNSサーバからの応答にデジタル署名が施されます。デジタル署名をリゾルバ側が確認することで、正規のDNSサーバからの応答であることが保証される。<a href="http://books.google.co.jp/books?id=KPweBAAAQBAJ&#38;pg=PA218&#38;lpg=PA218&#38;dq=dns+spoofing+%E5%AF%BE%E7%AD%96&#38;source=bl&#38;ots=W-3dTuZ8Dw&#38;sig=9ulgt9oJF-fNVd-lifa9OlMjbb8&#38;hl=ja&#38;sa=X&#38;ei=eHttVP27B4TGmAWAyoKgDA&#38;ved=0CEkQ6AEwBzgK#v=onepage&#38;q=dns%20spoofing%20%E5%AF%BE%E7%AD%96&#38;f=false" class="external">引用</a><br />・主な事例<br />オランダのCAであるDigiNotar社の偽SSLサーバ証明書発行事件では、6つの同社CAに不正侵入が行われ、SSLサーバ証明書発行機能を不正に使われて少なくとも531枚の偽SSL証明書が発行されてしまった。このケースでは、イラン国内のDNSサーバの情報も書き換えられていた可能性が高く、正規のサーバへのアクセスを不正発行された証明書が導入された攻撃用サーバへと誘導、以降の通信は攻撃用サーバを介して正当なサービスサイトに中継するようにしたようだ（中間者攻撃）。<a href="http://www.keyman.or.jp/at/30005894/" class="external">引用</a></p>


	<a name="Bind_DNSハイジャック"></a>
<h4 >DNSハイジャック<a href="#Bind_DNSハイジャック" class="wiki-anchor">&para;</a></h4>


	<p>コンピュータの設定を改ざんして、DNSを無効にしたり、攻撃者が制御している DNS サーバーに置き換える行為。<br />・対策<br />ゾーン（ドメイン名）のサブドメインや上位ドメインを別の利用者が作成する際には、権威DNSサーバを同一のサーバではなく、別サーバ（別IPアドレス）とするよう運用方法を変更すること。また、別の利用者が設定済みゾーンのサブドメインや上位ドメインを作成する際には何らかの制限を設けることでも、リスクを軽減できる。<a href="http://www.atmarkit.co.jp/news/201206/22/subdomain.html" class="external">引用</a><br />・主な事例<br />2014年11月には、国内のサイトが使用する「.com」ドメインの登録情報が不正に書き換えられるDNSハイジャックの被害が複数報告された。日本のドメイン名である「JPドメイン」の登録管理と、DNSの運用等を行うJPRSやJPCERTコーディネーションセンター(JPCERT/CC)は、ドメイン名登録者やドメイン名管理担当者に対し、登録情報を管理するためのIDやパスワードなどの認証情報を適切に管理するとともに、各システムにおける不正アクセスの防止・不正なコード実行の防止などの適切な脆弱性対策や情報漏えい対策を実施するよう呼びかけた。<a href="http://www.itmedia.co.jp/enterprise/articles/1411/05/news101.html" class="external">引用</a></p>
<hr />
<a name="CA設計" />
<a name="CA設計_CA設計"></a>
<h1 >CA設計<a href="#CA設計_CA設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#CA設計_CA設計">CA設計</a></li><li><a href="#CA設計_Introduction">●Introduction</a><ul><li><a href="#CA設計_目的">目的</a></li></ul>
</li><li><a href="#CA設計_構成">●構成</a><ul><li><a href="#CA設計_サーバ構成">サーバ構成</a></li><li><a href="#CA設計_導入パッケージ">導入パッケージ</a></li></ul>
</li><li><a href="#CA設計_システム設計">●システム設計</a><ul><li><a href="#CA設計_idc設備内部で利用するサーバの秘密鍵を作成する手順">idc設備内部で利用するサーバの秘密鍵を作成する手順</a></li><li><a href="#CA設計_証明書要求ファイルCSR">証明書要求ファイル(CSR)</a></li><li><a href="#CA設計_CA構築">CA構築</a><ul><li><a href="#CA設計_設定ファイル">設定ファイル</a></li></ul>
</li><li><a href="#CA設計_署名">署名</a></li><li><a href="#CA設計_サーバ証明書を作成する便利スクリプト">サーバ証明書を作成する便利スクリプト</a></li><li><a href="#CA設計_dhパラメータ鍵について">dhパラメータ鍵について</a></li></ul>
</li><li><a href="#CA設計_冗長化">●冗長化</a></li><li><a href="#CA設計_バックアップ">●バックアップ</a></li></ul>


<hr />


	<a name="CA設計_Introduction"></a>
<h1 >●Introduction<a href="#CA設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="CA設計_目的"></a>
<h2 >目的<a href="#CA設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>CAはopenvpn等で用いる鍵に署名を行う重要。</p>


<hr />


	<a name="CA設計_構成"></a>
<h1 >●構成<a href="#CA設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="CA設計_サーバ構成"></a>
<h2 >サーバ構成<a href="#CA設計_サーバ構成" class="wiki-anchor">&para;</a></h2>


	<p>zabbixサーバ上にCAを構築する。</p>


	<a name="CA設計_導入パッケージ"></a>
<h2 >導入パッケージ<a href="#CA設計_導入パッケージ" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>パッケージ名</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>postgresql94-server.x86_64</td>
			<td>RDBMSの代表的なOSSであるpostgresql server</td>
		</tr>
	</table>




<hr />


	<a name="CA設計_システム設計"></a>
<h1 >●システム設計<a href="#CA設計_システム設計" class="wiki-anchor">&para;</a></h1>


	<a name="CA設計_idc設備内部で利用するサーバの秘密鍵を作成する手順"></a>
<h2 >idc設備内部で利用するサーバの秘密鍵を作成する手順<a href="#CA設計_idc設備内部で利用するサーバの秘密鍵を作成する手順" class="wiki-anchor">&para;</a></h2>


	<p>例えばidcで提供するWebサービスのサーバ証明書が必要な場合には以下のコマンドで秘密鍵を作成すること。<br />鍵の名称は適宜読み替えること。</p>


<pre>
# openssl genrsa 2048 &gt; server.key
Generating RSA private key, 2048 bit long modulus
.....................................................................+++
............................................................+++
e is 65537 (0x10001)
</pre>

	<a name="CA設計_証明書要求ファイルCSR"></a>
<h2 >証明書要求ファイル(CSR)<a href="#CA設計_証明書要求ファイルCSR" class="wiki-anchor">&para;</a></h2>


	<p>CAの署名を得るためには、まず秘密鍵から証明書要求ファイルを作成する必要がある。<br />この作業は各PC等で行う。<br />以下に例を示す。</p>


	<p>入力している内容は適宜読み替えること。</p>


<pre>
# openssl req -new -key kajiro_yota.key 
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [XX]:JP
State or Province Name (full name) []:Tokyo
Locality Name (eg, city) [Default City]:Shinagawa
Organization Name (eg, company) [Default Company Ltd]:isb
Organizational Unit Name (eg, section) []:infra system
Common Name (eg, your name or your server's hostname) []:kajiro_yota
Email Address []:kajiro@isb.co.jp

Please enter the following 'extra' attributes
to be sent with your certificate request
A challenge password []:
An optional company name []:
-----BEGIN CERTIFICATE REQUEST-----
MIIC0TCCAbkCAQAwgY0xCzAJBgNVBAYTAkpQMQ4wDAYDVQQIDAVUb2t5bzESMBAG
A1UEBwwJU2hpbmFnYXdhMQwwCgYDVQQKDANpc2IxFTATBgNVBAsMDGluZnJhIHN5
c3RlbTEUMBIGA1UEAwwLa2FqaXJvX3lvdGExHzAdBgkqhkiG9w0BCQEWEGthamly
b0Bpc2IuY28uanAwggEgMA0GCSqGSIb3DQEBAQUAA4IBDQAwggEIAoIBAQCXXyur
ojXBoBiWvU76nXngjCTImypGxQISYYEJn8w9Cx0Y2Vjr/jETalVXfXKSEdMJehQ4
qZM9Y7MYTK9LPjBzmV9p98qOy5Z6GFXHpijWKMXpMr9hG++fvLqo4s8bSGH8xwkk
wXcseJMWF37oObQbaMUPhdzGt0AdaUoXgOYKefUHah5HwnuGtp4nOjoZ5tiUigx4
VagTYj6AmBOyuVE+yXVM8rnLW2m3zdzXAIi4yKWpS9ht3EwkBA9+/2SYsJN9tbNY
cxqkjcilDJ07da4nioi6VtWR14x6LR+Oy3UF5HcgGljJuudjEaiGOfTxKhcs+oI2
gpeb0XUeqxWFNozLAgEjoAAwDQYJKoZIhvcNAQEFBQADggEBAIQReUSavgUtNl6z
SmCwJ6tGUnaHDP9yZ67226Fm2G3c83toqvVyFQ0WlciICtMe1ghRwL14tNCqw7bs
jA7NgyM1F+oWxdlH4YyN+MRkwUaQeWOvH6H/uCgzFwkrywZC/UbyrynkZzPnDm6i
GbtJbqRhd6MBJSKU2piUtIcIdL22BWU/jiqJFJycbhOk/iEHYynPPqYcVbvN/+du
atfcpn0fAyRVBLjT74KOtyEdB0N7/TMAt28S8mdN7VjxBSjjyx8FluCd0mle4FCa
Mk8eWhFfhWiA8FOPEtsuWcNtqLWynBwN3RtwVDq4Qu5YIRs0QRqqeGdtjKKK09TM
KBrS0Rg=
-----END CERTIFICATE REQUEST-----
</pre>

	<a name="CA設計_CA構築"></a>
<h2 >CA構築<a href="#CA設計_CA構築" class="wiki-anchor">&para;</a></h2>


	<p>以下のコマンドにてCAの秘密鍵を生成する。<br />この作業はzabbixサーバ上で初回時のみ行う必要がある。<br />ここで生成する秘密鍵が変更されると今まで利用していたものが利用できなくなるため注意すること。<br />この鍵はzabbixサーバ#1,2で完全に同一となるようにする。</p>


	<p>※CA秘密鍵のパスワードは「isbadmin」とする。</p>


<pre>
# cd /etc/pki/CA
# openssl req -new -x509 -newkey rsa:2048 -out cacert.pem -keyout private/cakey.pem -days 3650
Generating a 2048 bit RSA private key
..................................+++
............................................+++
writing new private key to 'private/cakey.pem'
Enter PEM pass phrase:
Verifying - Enter PEM pass phrase:
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [JP]:
State or Province Name (full name) [Tokyo]:
Locality Name (eg, city) [Shinagawa]:
Organization Name (eg, company) [isb]:
Organizational Unit Name (eg, section) [infra system]:
Common Name (eg, your name or your server's hostname) []:idc CA
Email Address [XXX@isb.co.jp]:kajiro@isb.co.jp
### 最後に必要なファイルを生成しておく
# echo "01" &gt; /etc/pki/CA/serial
# touch /etc/pki/CA/index.txt
</pre>

	<a name="CA設計_設定ファイル"></a>
<h3 >設定ファイル<a href="#CA設計_設定ファイル" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>/etc/pki/tls/openssl.cnf</li>
	</ul>


	<p>CAが署名する際にはcommon name以外はほぼ一致していることが望ましいため、<br />入力時にデフォルト設定を追記する。<br />ただし、秘密鍵は各自で設定していることから、<br />organizationName,organizationalUnitNameは一致しなくてもよい設定を入れている。<br />また、有効期限はデフォルトで10年とする。</p>


	<p><a class="collapsible collapsed" href="#" id="collapse-2559fea9-show" onclick="$(&#x27;#collapse-2559fea9-show, #collapse-2559fea9-hide&#x27;).toggle(); $(&#x27;#collapse-2559fea9&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-2559fea9-hide" onclick="$(&#x27;#collapse-2559fea9-show, #collapse-2559fea9-hide&#x27;).toggle(); $(&#x27;#collapse-2559fea9&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-2559fea9" style="display:none;"><pre>
HOME            = .
RANDFILE        = $ENV::HOME/.rnd
oid_section        = new_oids
[ new_oids ]
tsa_policy1 = 1.2.3.4.1
tsa_policy2 = 1.2.3.4.5.6
tsa_policy3 = 1.2.3.4.5.7
[ ca ]
default_ca    = CA_default        # The default ca section
[ CA_default ]
dir        = /etc/pki/CA        # Where everything is kept
certs        = $dir/certs        # Where the issued certs are kept
crl_dir        = $dir/crl        # Where the issued crl are kept
database    = $dir/index.txt    # database index file.
                    # several ctificates with same subject.
new_certs_dir    = $dir/newcerts        # default place for new certs.
certificate    = $dir/cacert.pem     # The CA certificate
serial        = $dir/serial         # The current serial number
crlnumber    = $dir/crlnumber    # the current crl number
                    # must be commented out to leave a V1 CRL
crl        = $dir/crl.pem         # The current CRL
private_key    = $dir/private/cakey.pem# The private key
RANDFILE    = $dir/private/.rand    # private random number file
x509_extensions    = usr_cert        # The extentions to add to the cert
name_opt     = ca_default        # Subject Name options
cert_opt     = ca_default        # Certificate field options
default_days    = 3650            # how long to certify for
default_crl_days= 30            # how long before next CRL
default_md    = default        # use public key default MD
preserve    = no            # keep passed DN ordering
policy        = policy_match
[ policy_match ]
countryName        = match
stateOrProvinceName    = optional
organizationName    = optional
organizationalUnitName    = optional
commonName        = supplied
emailAddress        = optional
[ policy_anything ]
countryName        = optional
stateOrProvinceName    = optional
localityName        = optional
organizationName    = optional
organizationalUnitName    = optional
commonName        = supplied
emailAddress        = optional
[ req ]
default_bits        = 2048
default_md        = sha1
default_keyfile     = privkey.pem
distinguished_name    = req_distinguished_name
attributes        = req_attributes
x509_extensions    = v3_ca    # The extentions to add to the self signed cert
string_mask = utf8only
[ req_distinguished_name ]
countryName            = Country Name (2 letter code)
countryName_default        = JP
countryName_min            = 2
countryName_max            = 2
stateOrProvinceName        = State or Province Name (full name)
stateOrProvinceName_default    = Tokyo
localityName            = Locality Name (eg, city)
localityName_default        = Shinagawa
0.organizationName        = Organization Name (eg, company)
0.organizationName_default    = isb
organizationalUnitName        = Organizational Unit Name (eg, section)
organizationalUnitName_default    = infra system
commonName            = Common Name (eg, your name or your server\'s hostname)
commonName_max            = 64
emailAddress            = Email Address
emailAddress_default        = XXX@isb.co.jp
emailAddress_max        = 64
[ req_attributes ]
challengePassword        = A challenge password
challengePassword_min        = 4
challengePassword_max        = 20
unstructuredName        = An optional company name
[ usr_cert ]
basicConstraints=CA:FALSE
nsComment            = "OpenSSL Generated Certificate" 
subjectKeyIdentifier=hash
authorityKeyIdentifier=keyid,issuer
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
[ v3_ca ]
subjectKeyIdentifier=hash
authorityKeyIdentifier=keyid:always,issuer
basicConstraints = CA:true
[ crl_ext ]
authorityKeyIdentifier=keyid:always
[ proxy_cert_ext ]
basicConstraints=CA:FALSE
nsComment            = "OpenSSL Generated Certificate" 
subjectKeyIdentifier=hash
authorityKeyIdentifier=keyid,issuer
proxyCertInfo=critical,language:id-ppl-anyLanguage,pathlen:3,policy:foo
[ tsa ]
default_tsa = tsa_config1    # the default TSA section
[ tsa_config1 ]
dir        = ./demoCA        # TSA root directory
serial        = $dir/tsaserial    # The current serial number (mandatory)
crypto_device    = builtin        # OpenSSL engine to use for signing
signer_cert    = $dir/tsacert.pem     # The TSA signing certificate
                    # (optional)
certs        = $dir/cacert.pem    # Certificate chain to include in reply
                    # (optional)
signer_key    = $dir/private/tsakey.pem # The TSA private key (optional)
default_policy    = tsa_policy1        # Policy if request did not specify it
                    # (optional)
other_policies    = tsa_policy2, tsa_policy3    # acceptable policies (optional)
digests        = md5, sha1        # Acceptable message digests (mandatory)
accuracy    = secs:1, millisecs:500, microsecs:100    # (optional)
clock_precision_digits  = 0    # number of digits after dot. (optional)
ordering        = yes    # Is ordering defined for timestamps?
                # (optional, default: no)
tsa_name        = yes    # Must the TSA name be included in the reply?
                # (optional, default: no)
ess_cert_id_chain    = no    # Must the ESS cert id chain be included?
                # (optional, default: no)
</pre></div></p>


	<a name="CA設計_署名"></a>
<h2 >署名<a href="#CA設計_署名" class="wiki-anchor">&para;</a></h2>


	<p>以下のコマンドをzabbixサーバ上で発行し証明書要求ファイルからサーバ証明書(CRT)を作成する。<br />証明書要求、サーバ証明書は適宜読み替えること。</p>


<pre>
# openssl ca -config /etc/pki/tls/openssl.cnf -keyfile /etc/pki/CA/private/cakey.pem -cert /etc/pki/CA/cacert.pem -in uchida.csr -out uchida.crt
Using configuration from /etc/pki/tls/openssl.cnf
Enter pass phrase for /etc/pki/CA/private/cakey.pem:
Check that the request matches the signature
Signature ok
Certificate Details:
        Serial Number: 6 (0x6)
        Validity
            Not Before: Apr 23 06:33:17 2015 GMT
            Not After : Apr 20 06:33:17 2025 GMT
        Subject:
            countryName               = JP
            stateOrProvinceName       = Tokyo
            organizationName          = isb
            organizationalUnitName    = infra system
            commonName                = uchida
            emailAddress              = a.uchida@isb.co.jp
        X509v3 extensions:
            X509v3 Basic Constraints: 
                CA:FALSE
            Netscape Comment: 
                OpenSSL Generated Certificate
            X509v3 Subject Key Identifier: 
                F9:72:B7:CA:4D:0E:35:7A:8F:EC:BC:32:31:55:9B:A6:6A:6A:88:28
            X509v3 Authority Key Identifier: 
                keyid:DD:0B:4E:D6:73:F4:7E:F4:CE:78:E5:82:B3:B4:1D:8D:A0:F0:97:FF

Certificate is to be certified until Apr 20 06:33:17 2025 GMT (3650 days)
Sign the certificate? [y/n]:y

1 out of 1 certificate requests certified, commit? [y/n]y
Write out database with 1 new entries
Data Base Updated
</pre>

	<a name="CA設計_サーバ証明書を作成する便利スクリプト"></a>
<h2 >サーバ証明書を作成する便利スクリプト<a href="#CA設計_サーバ証明書を作成する便利スクリプト" class="wiki-anchor">&para;</a></h2>


	<p>idcサービスで利用するサーバ証明書に関しては以下のスクリプトをzabbixサーバ(/etc/pki/tls/createServerCert.sh)に用意する。</p>


<pre>
# cat /etc/pki/tls/createServerCert.sh 
name=server_$1

if [ "${name}" = "" ];then
  echo "input server name(common name)" 
  echo "usage) $0 common_name" 
  echo "       $0 openvpn" 
  exit 1
fi

### directory
mkdir ${name}
cd ${name}

### private key
openssl genrsa 2048 &gt; ${name}.key

### csr
openssl req -new -key ${name}.key &gt; ${name}.csr

### sign(crt)
### 有効期間の開始日は昨日からとする
openssl ca -config /etc/pki/tls/openssl.cnf -keyfile /etc/pki/CA/private/cakey.pem -cert /etc/pki/CA/cacert.pem -in ${name}.csr -out ${name}.crt -startdate `date +%y%m%d000000 -d yesterday`

cd ../
tar czf ${name}.tar.gz ${name}
rm -rf ${name}
</pre>

	<a name="CA設計_dhパラメータ鍵について"></a>
<h2 >dhパラメータ鍵について<a href="#CA設計_dhパラメータ鍵について" class="wiki-anchor">&para;</a></h2>


	<p>openvpn等ではdhパラメータ鍵が必要となる。<br />その場合は以下のコマンドで生成すること。<br />ファイル名は適宜読み替えること。</p>


<pre>
# openssl dhparam -out dbparam.pem 1024
Generating DH parameters, 1024 bit long safe prime, generator 2
This is going to take a long time
..........+...+................+.........................+........................................+..........+........+............................................+.................+............................................+........................................................+.....+.....................................+.....................................................................................................+...........................................................+........................+.....................................................................................+.................+...+.....+........+..................................................................................+.....................+........................+.......................+...................+........+........+...........................................+......................+..........................+.....+.........+...+.....................+.+..............................................................................+..................................................+...............+.......+...........................................+...+..............................+..........................................................................................+..........................+............+...............................................................................................+......+..............+........................................................+.........................................+........................................+..........................+......+..............+..+............................................................................+.............................+..................................................+.......+..........................+..............................................+......................................+.+........................................................................................................................................................................................................................+.................................................+.............+............................................................+........+...............................................+......................................+.............+...........+..............+..........+.....................+..................+......................+.............................+.....................................................................+...........................................................................................................+...............................................................+......................................+................................+..............................+...+...........+.............................................................................................................+...........+....................++*++*++*
</pre>

<hr />


	<a name="CA設計_冗長化"></a>
<h1 >●冗長化<a href="#CA設計_冗長化" class="wiki-anchor">&para;</a></h1>


	<p>CAはzabbixサーバ#1,2に作成し、同一の鍵を用いる。</p>


<hr />


	<a name="CA設計_バックアップ"></a>
<h1 >●バックアップ<a href="#CA設計_バックアップ" class="wiki-anchor">&para;</a></h1>


	<p>/etc/pkiディレクトリ配下全てをバックアップする。</p>
<hr />
<a name="Centos" />
<a name="Centos_Centos"></a>
<h1 >Centos<a href="#Centos_Centos" class="wiki-anchor">&para;</a></h1>


	<a name="Centos_インストール時の条件"></a>
<h2 >インストール時の条件<a href="#Centos_インストール時の条件" class="wiki-anchor">&para;</a></h2>


	<p>パッケージ選択画面にて以下をインストールすること。</p>


	<ul>
	<li>ベースシステムのベース</li>
		<li>開発の開発ツール</li>
		<li>日本語サポート</li>
	</ul>
<hr />
<a name="Clocon技術調査" />
<a name="Clocon技術調査_Clocon技術調査"></a>
<h1 >Clocon技術調査<a href="#Clocon技術調査_Clocon技術調査" class="wiki-anchor">&para;</a></h1>


<hr />


	<a name="Clocon技術調査_機能一覧"></a>
<h2 >機能一覧<a href="#Clocon技術調査_機能一覧" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>KVMのホスト管理</li>
		<li>企業毎にセキュアなNW構成の構築
	<ul>
	<li>仮想SWとvlanを用いる、nwが別れるので企業毎のdhcp等必要か？</li>
	</ul>
	</li>
		<li>インスタンスの生成、起動、停止、破棄</li>
		<li>フリーvlanの設定</li>
		<li>新規参入企業等の管理</li>
		<li>グローバルアドレスの付与</li>
	</ul>


<hr />


	<a name="Clocon技術調査_ユーザの利用シーンと内部処理の流れ"></a>
<h2 >ユーザの利用シーンと内部処理の流れ<a href="#Clocon技術調査_ユーザの利用シーンと内部処理の流れ" class="wiki-anchor">&para;</a></h2>


	<p>以下、クラウド利用シーン毎の処理の流れを記載する</p>


<hr />


	<a name="Clocon技術調査_インスタンスの生成"></a>
<h3 >インスタンスの生成<a href="#Clocon技術調査_インスタンスの生成" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>企業idやインスタンス名等の情報とスペックを入力しインスタンスの新規作成命令の発行
	<ol>
	<li>事前作業：スペック毎に予め仮想マシンを作成しそれが保存されたパーティションを用意しておく(ifcfg-ethXにmacが記載されている場合は書き換える必要がある、/etc/ssh/*の鍵を削除しておく)</li>
		<li>ネットワーク：物理ホストのインターフェイスにvlanを割り当てておき、それに繋がるブリッジを作成する(dhcpの場合はvirbrXで定義した方が良いかも)</li>
		<li>ストレージ：新たなlvm領域を作成しddコマンド等で新たなLVMに移行、もしくは差分のみを保存する場合はそれも不要</li>
		<li>kvm：kvmのホストを管理しておき、どのホストへ展開するかを選択する</li>
		<li>libvirt：libvirtのxmlにあるuuidとmac、lvm領域、vncポートを変更してインスタンスを定義する(virt-cloneを使用する場合はuuidとmacの変更は不要、ただしvirt-cloneは同一ホストのみなので複製後にマイグレーションが必要)</li>
	</ol></li>
	</ol>


<hr />


	<a name="Clocon技術調査_インスタンスの起動"></a>
<h3 >インスタンスの起動<a href="#Clocon技術調査_インスタンスの起動" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>インスタンスを特定できる情報を入力し起動命令の発行
	<ol>
	<li>libvirt：インスタンスを起動する</li>
	</ol></li>
	</ol>


<hr />


	<a name="Clocon技術調査_インスタンスの停止"></a>
<h3 >インスタンスの停止<a href="#Clocon技術調査_インスタンスの停止" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>企業idやインスタンス名を入力しインスタンスを停止命令の発行
	<ol>
	<li>libvirt：入力情報からドメインを特定する</li>
		<li>kvm：ドメインが起動しているホストを特定する</li>
		<li>libvirt：インスタンスを停止する</li>
	</ol></li>
	</ol>


<hr />


	<a name="Clocon技術調査_インスタンスの破棄"></a>
<h3 >インスタンスの破棄<a href="#Clocon技術調査_インスタンスの破棄" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>企業idやインスタンス名を入力しインスタンスを破棄命令の発行
	<ol>
	<li>libvirt：入力情報からドメインを特定する</li>
		<li>kvm：ドメインが起動しているホストを特定する</li>
		<li>libvirt：インスタンスを破棄する</li>
		<li>ストレージ：インスタンスに割り当てられたlvm領域を解放する</li>
		<li>ネットワーク：インスタンスに紐付いたブリッジを削除する(複数のインスタンスが関連付いている場合はスキップ)</li>
	</ol></li>
	</ol>


<hr />


	<a name="Clocon技術調査_フリーvlanの設定"></a>
<h3 >フリーvlanの設定<a href="#Clocon技術調査_フリーvlanの設定" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>企業idや設定するvlan idを入力し新規ネットワーク作成命令の発行
	<ol>
	<li>ネットワーク：入力情報からブリッジを作成する(既に使用済のvlanの場合は割り当てない)</li>
		<li>libvirt：ブリッジをインスタンスにアタッチする(virbrXのnatのインターフェイスをアタッチするとデフォルトゲートウェイが変更する？)</li>
	</ol></li>
	</ol>


<hr />


	<a name="Clocon技術調査_新規参入企業"></a>
<h3 >新規参入企業<a href="#Clocon技術調査_新規参入企業" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>新たに参入する企業の情報の追加命令を発行
	<ol>
	<li>誰かがエクセルに情報を記載しvlanやnw帯を決定する<br />→管理webページを検討</li>
	</ol></li>
	</ol>


<hr />


	<a name="Clocon技術調査_懸念事項"></a>
<h2 >懸念事項<a href="#Clocon技術調査_懸念事項" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>ライブマイグレーションのサポート　 <ins><strong>解決</strong></ins>
	<ul>
	<li>cmanを利用してれば簡単、ただしクラスタグループを管理する必要がある</li>
	</ul>
	</li>
		<li>スナップショットのサポート(lvmのスナップショット機能)　 <ins><strong>解決</strong></ins></li>
		<li>dhcpの利用　 <ins><strong>解決</strong></ins>
	<ul>
	<li>仮想ルータにdhcpの機能を持たせる vyosでは全ての設定を/config/config.bootに記載しているためdhcp機能を付与できる</li>
	</ul>
	</li>
		<li>virt-cloneが現状の開発環境だと2時間程かかる(ストレージNWの通信帯域をギガビットイーサに変更してどうなるか)
	<ul>
	<li>もし速度改善が見込めない場合は新規でインストールさせた方が早い</li>
	</ul>
	</li>
		<li>インスタンス作成後のdisk追加等の要望のサポート
	<ul>
	<li>disk追加ぐらいはサポートしたい</li>
	</ul>
	</li>
		<li>virt-v2v(p2v)なる機能がありそれがあると使い勝手が向上しそう(初期バージョンからこの機能をサポートしたくないが)</li>
		<li>kvmホストの管理をどのように行うか(cloconはkvmホストのipを知っている必要がある)</li>
		<li>企業idや設定しているvlan idはどのように管理するか(エクセル？db？)</li>
		<li>物理マシンの障害はどこまでサポートするか  <ins><strong>解決</strong></ins>
	<ul>
	<li>libvirtdが生きている限りはライブマイグレーションで対応</li>
	</ul>
	</li>
		<li>DBと整合性がとれなくなるような情報以外はなるべく持ちたい</li>
		<li>仮想ルータの設定変更が自動でできない
	<ul>
	<li>shell api等を利用すればコンフィグのロードまではできるが、saveができない</li>
		<li>apiを利用してロード後、設定をscpで送信すれば同期されたことになる？</li>
	</ul></li>
	</ul>


<hr />


	<a name="Clocon技術調査_検討技術"></a>
<h2 >検討技術<a href="#Clocon技術調査_検討技術" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>ユニケージ開発
	<ul>
	<li>シェル等の既存のものを組み合わせ開発する方法（開発コストが抑えられる）</li>
	</ul>
	</li>
		<li>プログラムの開発が必要であればwebのguiを意識してnodeのlibvirtライブラリを用いた開発(pythonでも良いけど…)
	<ul>
	<li>やるとしたらapi化して開発したい</li>
		<li>kvmホストの管理や企業idとの関連を管理するにはシェルだと面倒かもしれない...</li>
	</ul>
	</li>
		<li>libguestfs-toolsに含まれるvirt-XXX系のコマンドツール
	<ul>
	<li>停止したゲストOSのFSにアクセスできるため、host名等の書き換えが可能</li>
	</ul>
	</li>
		<li>brctlを利用したブリッジの作成
	<ul>
	<li>openvswitchの検討</li>
	</ul>
	</li>
		<li>仮想ルータは全てのkvmに仮想マシンとして動作させ、グローバルipと1対1ナットする</li>
		<li>インスタンス生成時に仮想マシンの情報をcmanに登録する必要がある</li>
		<li>dockerを利用</li>
	</ul>


<hr />


	<a name="Clocon技術調査_プロジェクトの進め方"></a>
<h2 >プロジェクトの進め方<a href="#Clocon技術調査_プロジェクトの進め方" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>年内にサーバ購入</li>
		<li>年内に上記の内容が完成</li>
		<li>人は内田さんと柴田さん</li>
	</ul>
<hr />
<a name="Clocon設計" />
<a name="Clocon設計_Clocon設計"></a>
<h1 >■Clocon設計<a href="#Clocon設計_Clocon設計" class="wiki-anchor">&para;</a></h1>


	<p>VLANを超えた場合やKVMを束ねるグループを分ける必要がある<br />クラコンはローカルDBに変更<br />クラコンにntp,dns,proxyを同居する</p>


	<ul class="toc"><li><a href="#Clocon設計_Clocon設計">■Clocon設計</a></li><li><a href="#Clocon設計_Introduction">●Introduction</a><ul><li><a href="#Clocon設計_目的">目的</a></li><li><a href="#Clocon設計_定義範囲">定義範囲</a></li></ul>
</li><li><a href="#Clocon設計_構成">●構成</a><ul><li><a href="#Clocon設計_物理機器仕様">物理機器仕様</a></li><li><a href="#Clocon設計_パーティション構成">パーティション構成</a></li><li><a href="#Clocon設計_自動起動サービス一覧">自動起動サービス一覧</a></li><li><a href="#Clocon設計_ミドルウェア一覧">ミドルウェア一覧</a></li><li><a href="#Clocon設計_ネットワーク">ネットワーク</a></li></ul>
</li><li><a href="#Clocon設計_機能">●機能</a><ul><li><a href="#Clocon設計_機能一覧">機能一覧</a></li><li><a href="#Clocon設計_機能と状態遷移の関係">機能と状態遷移の関係</a></li></ul>
</li><li><a href="#Clocon設計_データ定義">●データ定義</a><ul><li><a href="#Clocon設計_設定ファイル">設定ファイル</a></li><li><a href="#Clocon設計_DBスキーマ">DBスキーマ</a></li><li><a href="#Clocon設計_lvmの名称について">lvmの名称について</a></li><li><a href="#Clocon設計_テンプレート">テンプレート</a></li><li><a href="#Clocon設計_リターンコード">リターンコード</a></li><li><a href="#Clocon設計_ネットワークインターフェイスの命名規則">ネットワークインターフェイスの命名規則</a></li></ul>
</li><li><a href="#Clocon設計_シーケンス">●シーケンス</a><ul><li><ul><li><a href="#Clocon設計_kvmホスト定義">kvmホスト定義</a></li><li><a href="#Clocon設計_kvmホスト破棄">kvmホスト破棄</a></li><li><a href="#Clocon設計_企業id定義">企業id定義</a></li><li><a href="#Clocon設計_企業id削除">企業id削除</a></li><li><a href="#Clocon設計_インスタンス定義">インスタンス定義</a></li><li><a href="#Clocon設計_インスタンス破棄">インスタンス破棄</a></li><li><a href="#Clocon設計_インスタンス起動">インスタンス起動</a></li><li><a href="#Clocon設計_インスタンス停止">インスタンス停止</a></li><li><a href="#Clocon設計_グローバルIP定義">グローバルIP定義</a></li><li><a href="#Clocon設計_グローバルIP破棄">グローバルIP破棄</a></li><li><a href="#Clocon設計_グローバルIPアタッチ">グローバルIPアタッチ</a></li><li><a href="#Clocon設計_グローバルIPデタッチ">グローバルIPデタッチ</a></li><li><a href="#Clocon設計_vlan定義">vlan定義</a></li><li><a href="#Clocon設計_vlan破棄">vlan破棄</a></li><li><a href="#Clocon設計_vlanアタッチ">vlanアタッチ</a></li><li><a href="#Clocon設計_vlanデタッチ">vlanデタッチ</a></li><li><a href="#Clocon設計_disk定義">disk定義</a></li><li><a href="#Clocon設計_disk破棄">disk破棄</a></li><li><a href="#Clocon設計_diskアタッチ">diskアタッチ</a></li><li><a href="#Clocon設計_diskデタッチ">diskデタッチ</a></li><li><a href="#Clocon設計_ライブマイグレーション">ライブマイグレーション</a></li><li><a href="#Clocon設計_企業id初期化">企業id初期化</a></li><li><a href="#Clocon設計_ファイアウォール定義">ファイアウォール定義</a></li><li><a href="#Clocon設計_ファイアウォール破棄">ファイアウォール破棄</a></li><li><a href="#Clocon設計_ポートフォワード定義">ポートフォワード定義</a></li><li><a href="#Clocon設計_ポートフォワード破棄">ポートフォワード破棄</a></li></ul>
</li></ul>
</li><li><a href="#Clocon設計_業務フロー">●業務フロー</a></li><li><a href="#Clocon設計_可用性">●可用性</a><ul><li><a href="#Clocon設計_cloconの冗長性">cloconの冗長性</a></li><li><a href="#Clocon設計_DBの冗長性">DBの冗長性</a></li></ul>
</li><li><a href="#Clocon設計_性能拡張性">●性能・拡張性</a><ul><li><a href="#Clocon設計_cloconのスケールアウト">cloconのスケールアウト</a></li><li><a href="#Clocon設計_kvmのスケールアウト">kvmのスケールアウト</a></li></ul>
</li><li><a href="#Clocon設計_セキュリティ">●セキュリティ</a></li><li><a href="#Clocon設計_運用保守性">●運用・保守性</a><ul><li><a href="#Clocon設計_ログ">ログ</a></li><li><a href="#Clocon設計_ログメッセージ一覧">ログメッセージ一覧</a></li><li><a href="#Clocon設計_監視">監視</a></li><li><a href="#Clocon設計_障害">障害</a></li><li><a href="#Clocon設計_clocon技術調査">clocon技術調査</a></li><li><a href="#Clocon設計_現状設定のメモ">現状設定のメモ</a></li></ul></li></ul>


<hr />


	<a name="Clocon設計_Introduction"></a>
<h1 >●Introduction<a href="#Clocon設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="Clocon設計_目的"></a>
<h2 >目的<a href="#Clocon設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>近年IaaS型のクラウドサービス(以下、クラウドと呼ぶ)が普及し、誰もが手軽にサーバを持つことができるようになっている。<br />ビジネスにおいてもクラウドの重要性は大きく、AWSに見られる即座にサービスが始められる点、<br />必要に応じてスケーラブルに拡張できる点は大きなアドバンテージである。<br />この流れを背景に当社ではクラウドシステム(以下、本システムと呼ぶ)を構築する。<br />本書はクラウドの管理を行うクラウドコントローラー(以下、cloconと呼ぶ)を定義する。<br />本システムでは同一環境に複数の顧客システムを稼働できるようにするため、<br />大前提としてセキュアであることが求められる。<br />本書はLinuxやネットワーク、仮想化技術等の有識者を対象読者としているため、<br />本書に記述のあるそれらの一般的な知識については割愛する。</p>


	<a name="Clocon設計_定義範囲"></a>
<h2 >定義範囲<a href="#Clocon設計_定義範囲" class="wiki-anchor">&para;</a></h2>


	<p>本書は以下の範囲を定義していく。</p>


	<p><img src="/attachments/download/806/define_range1.png" alt="" /></p>


	<ul>
	<li>clocon group
	<ul>
	<li>物理マシン2台の構成とする</li>
		<li>利用時はPCからcloconへsshで接続しコマンドを発行する</li>
	</ul>
	</li>
		<li>kvm group
	<ul>
	<li>本クラウドはkvm上のゲストOSを提供する</li>
		<li>cloconからlibvirt経由でのゲストOSの操作依頼を受ける</li>
		<li>cloconからlvm領域の操作依頼を受ける</li>
	</ul>
	</li>
		<li>storage group
	<ul>
	<li>iscsiによるclvmを提供する</li>
		<li>ゲストOSはlvmを利用する</li>
		<li>cloconからはkvmを経由してlvm領域作成依頼を受ける</li>
	</ul></li>
	</ul>


<hr />


	<a name="Clocon設計_構成"></a>
<h1 >●構成<a href="#Clocon設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="Clocon設計_物理機器仕様"></a>
<h2 >物理機器仕様<a href="#Clocon設計_物理機器仕様" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr>
			<th style="background:#d3eaf3;">製品名</th>
			<td>HP</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;">モデル名</th>
			<td></td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;">製品番号</th>
			<td></td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;">プロセッサ</th>
			<td></td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;">CPU</th>
			<td></td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;">チップセット</th>
			<td></td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;">メモリ</th>
			<td></td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;">ディスク</th>
			<td></td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;">ネットワーク</th>
			<td></td>
		</tr>
	</table>




	<a name="Clocon設計_パーティション構成"></a>
<h2 >パーティション構成<a href="#Clocon設計_パーティション構成" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>デバイス</th>
			<th>マウントポイント</th>
			<th>FSタイプ</th>
			<th>サイズ(MB)</th>
			<th>用途</th>
		</tr>
		<tr>
			<td>sshd</td>
			<td></td>
			<td></td>
			<td>virshでの利用や、lvm操作の指示もsshを利用して行う。</td>
		</tr>
	</table>




	<a name="Clocon設計_自動起動サービス一覧"></a>
<h2 >自動起動サービス一覧<a href="#Clocon設計_自動起動サービス一覧" class="wiki-anchor">&para;</a></h2>


	<a name="Clocon設計_ミドルウェア一覧"></a>
<h2 >ミドルウェア一覧<a href="#Clocon設計_ミドルウェア一覧" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>名称</th>
			<th>パッケージ名</th>
			<td>コマンド名</td>
			<th>内容</th>
		</tr>
		<tr>
			<td>sshd</td>
			<td></td>
			<td></td>
			<td>virshでの利用や、lvm操作の指示もsshを利用して行う。</td>
		</tr>
		<tr>
			<td>libvirtd</td>
			<td></td>
			<td></td>
			<td>インスタンスを管理するlibvirtのデーモン。libvirtを利用するコマンドはこのデーモンへアクセスする。</td>
		</tr>
		<tr>
			<td>postgresql</td>
			<td></td>
			<td></td>
			<td>DBサーバ。kvmホストの情報等を保持する。2台構成としレプリケーションを利用する。</td>
		</tr>
		<tr>
			<td>lvm関連コマンド</td>
			<td></td>
			<td></td>
			<td>lvcreate等で新規lvm領域を作成することが可能である。</td>
			<td> <- kvmホスト側に必要<br /></td>
			<td>brctl</td>
			<td></td>
			<td></td>
			<td></td>
			<td> <- kvmホスト側に必要<br /></td>
			<td><del>docker</del></td>
			<td>-</td>
			<td>-</td>
			<td><del>コンテナ型の仮想化ソフトウェア。DB等のミドルウェアをコンテナ上で動作させる。</del></td>
		</tr>
	</table>




	<p>!!!!各ミドルウェアの使用方法や設定内容を記載する</p>


<pre>
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!！！！！！！！！！！！！！！！！！１１
各ネットワーク機器やサーバの構成が知りたい
予め⚪︎⚪︎を定義しておくなど

kvmと周辺機器の物理＆論理構成が知りたい
どこがどのようにプログラムから制御されるのか？など
</pre>

	<a name="Clocon設計_ネットワーク"></a>
<h2 >ネットワーク<a href="#Clocon設計_ネットワーク" class="wiki-anchor">&para;</a></h2>


	<p>仮想ルータの役割を以下に記載する。</p>


	<ul>
	<li>ファイアウォール(L4レベルでのパケットフィルタ)機能</li>
		<li>ポートフォワード(送信元NAPT,宛先NAPT)機能</li>
		<li>dhcp(macアドレスによるip割り当てにより実質固定ipとなる)</li>
	</ul>


	<p>また、上記の設定は動的に変更する必要がある。<br />vyOsにはコンフィグモード、オペレーションモードが存在し、設定変更時はコンフィグモードに変更する必要がある。<br />コンフィグモードは対話型であるため、動的な設定の変更を行うためにはvyOsに用意されたapi(perl/shell apiが有る)を利用する。<br />具体的にはvyOsに設定変更を行うapiのスクリプトを作成しておき、ssh等を利用してスクリプトの実行を行う必要がある。</p>


<hr />


	<a name="Clocon設計_機能"></a>
<h1 >●機能<a href="#Clocon設計_機能" class="wiki-anchor">&para;</a></h1>


	<a name="Clocon設計_機能一覧"></a>
<h2 >機能一覧<a href="#Clocon設計_機能一覧" class="wiki-anchor">&para;</a></h2>


	<p>cloconが持つ機能の一覧を以下に示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>機能名</th>
			<th>コマンド名称</th>
			<th>内容</th>
		</tr>
		<tr>
			<td>hypervisor定義</td>
			<td>clocon.define.hypervisor.sh</td>
			<td>kvmホストの追加を行う。</td>
		</tr>
		<tr>
			<td>hypervisor破棄</td>
			<td>clocon.undefine.hypervisor.sh</td>
			<td>kvmホストの削除を行う。</td>
		</tr>
		<tr>
			<td>企業id定義</td>
			<td>clocon.define.company.sh</td>
			<td>企業idの追加を行う。</td>
		</tr>
		<tr>
			<td>企業id破棄</td>
			<td>clocon.undefine.company.sh</td>
			<td>企業idの削除を行う。</td>
		</tr>
		<tr>
			<td>企業id初期化</td>
			<td>clocon.initialize.company.sh</td>
			<td>企業idにグローバルip,企業idに紐づくvlan,仮想ルータ,監視プロキシ(zabbix proxy),マスターキー(※1)を作成し、ポートを変更したsshの通信を設定する。</td>
		</tr>
		<tr>
			<td>インスタンス定義</td>
			<td>clocon.define.instance.sh</td>
			<td>新規lvm領域を確保しインスタンス定義(起動はしない)を行う。virshコマンドのdefineに相当。</td>
		</tr>
		<tr>
			<td>インスタンス破棄</td>
			<td>clocon.undefine.instance.sh</td>
			<td>定義済のインスタンスを破棄する。virshコマンドのundefineに相当。</td>
		</tr>
		<tr>
			<td>インスタンス起動</td>
			<td>clocon.start.instance.sh</td>
			<td>定義済のインスタンスを起動する。virshコマンドのstartに相当。</td>
		</tr>
		<tr>
			<td>インスタンス停止</td>
			<td>clocon.shutdown.instance.sh</td>
			<td>起動済みのインスタンスを停止する。virshコマンドのshutdownに相当。</td>
		</tr>
		<tr>
			<td>ip定義</td>
			<td>clocon.define.wan.sh</td>
			<td>ipと企業idをマッピングする。</td>
		</tr>
		<tr>
			<td>ip破棄</td>
			<td>clocon.undefine.wan.sh</td>
			<td>ipと企業idをマッピングを解除する。</td>
		</tr>
		<tr>
			<td>ipアタッチ</td>
			<td>clocon.attach.wan.sh</td>
			<td>ipを仮想インスタンスへアタッチする。</td>
		</tr>
		<tr>
			<td>ipデタッチ</td>
			<td>clocon.detach.wan.sh</td>
			<td>ipを仮想インスタンスからデタッチする。</td>
		</tr>
		<tr>
			<td>vlan定義</td>
			<td>clocon.define.vlan.sh</td>
			<td>フリーvlanの範囲から特定のvlanと企業idをマッピングする。</td>
		</tr>
		<tr>
			<td>vlan破棄</td>
			<td>clocon.undefine.vlan.sh</td>
			<td>フリーvlanの範囲から特定のvlanと企業idのマッピングを解除する。</td>
		</tr>
		<tr>
			<td>vlanアタッチ</td>
			<td>clocon.attach.vlan.sh</td>
			<td>フリーvlanを仮想インスタンスへアタッチする。</td>
		</tr>
		<tr>
			<td>vlanデタッチ</td>
			<td>clocon.detach.vlan.sh</td>
			<td>フリーvlanを仮想インスタンスからデタッチする。</td>
		</tr>
		<tr>
			<td>disk定義</td>
			<td>clocon.define.disk.sh</td>
			<td>lvm領域を確保し企業idとマッピングする。</td>
		</tr>
		<tr>
			<td>disk破棄</td>
			<td>clocon.undefine.disk.sh</td>
			<td>lvm領域を企業idのマッピングを解除する。</td>
		</tr>
		<tr>
			<td>diskアタッチ</td>
			<td>clocon.attach.disk.sh</td>
			<td>lvm領域を仮想インスタンスへアタッチする。</td>
		</tr>
		<tr>
			<td>diskデタッチ</td>
			<td>clocon.detach.disk.sh</td>
			<td>lvm領域を仮想インスタンスからデタッチする。</td>
		</tr>
		<tr>
			<td>ライブマイグレーション</td>
			<td>clocon.migrate.instance.sh</td>
			<td>インスタンスを別のkvmホストへ移行する。virshコマンドのmigrate --liveに相当。</td>
		</tr>
		<tr>
			<td>ファイアウォール定義</td>
			<td>clocon.define.firewall.sh</td>
			<td>インターネットから企業LANへの通信において送信元ipと宛先port、送信元portによる通信許可制御を行う。デフォルトは全ての通信を遮断する設定とする。</td>
		</tr>
		<tr>
			<td>ファイアウォール破棄</td>
			<td>clocon.undefine.firewall.sh</td>
			<td>定義済の設定を破棄する(通信を遮断する)。</td>
		</tr>
		<tr>
			<td>ポートフォワード定義</td>
			<td>clocon.define.portforward.sh</td>
			<td>ファイアウォールを通過した通信に対して送信元portから仮想インスタンスと宛先portのNAPT変換を行う。</td>
		</tr>
		<tr>
			<td>ポートフォワード破棄</td>
			<td>clocon.undefine.portforward.sh</td>
			<td>定義済の設定を破棄する(通信を転送しない)。</td>
		</tr>
	</table>




	<ul>
	<li>※1 マスターキーは各仮想インスタンスにデフォルトで設定する鍵
	<ul>
	<li>新規作成した仮想インスタンスのsshサービスは公開鍵認証のみを許可する。企業id初期化によってランダムに選択されたポート番号でのssh通信を許可する設定も入るため、そのポート向けにマスターキーを用いた公開鍵認証を行う。</li>
	</ul></li>
	</ul>


	<a name="Clocon設計_機能と状態遷移の関係"></a>
<h2 >機能と状態遷移の関係<a href="#Clocon設計_機能と状態遷移の関係" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Clocon設計_データ定義"></a>
<h1 >●データ定義<a href="#Clocon設計_データ定義" class="wiki-anchor">&para;</a></h1>


	<a name="Clocon設計_設定ファイル"></a>
<h2 >設定ファイル<a href="#Clocon設計_設定ファイル" class="wiki-anchor">&para;</a></h2>


	<p>cloconで利用する設定ファイルに記載する情報を以下に示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>項目名=値</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>clocon_db_uris="postgresql://user:pass@localhost:port/dbname?connect_timeout=10&#38;application_name=myapp"</td>
			<td><a href="http://www.postgresql.jp/document/9.3/html/libpq-connect.html#LIBPQ-CONNSTRING" class="external">postgres公式HP</a> を参考にURI表記によってDB情報を記載する。複数DB情報が存在する場合は半角空白区切りで記述すること。</td>
		</tr>
	</table>




	<a name="Clocon設計_DBスキーマ"></a>
<h2 >DBスキーマ<a href="#Clocon設計_DBスキーマ" class="wiki-anchor">&para;</a></h2>


	<p>DBのER図を以下に記載する。</p>


	<p><img src="/attachments/download/824/db_schema1.png" alt="" /></p>


	<ul>
	<li>company resources…企業に属するリソースや情報。企業毎の利用状況等を把握できる。</li>
		<li>instance resources…企業の属しているリソースの仮想インスタンスへの割り当て状況を把握できる。デバイス名等を保存しておき、障害時にはこれにより仮想インスタンスの再現を行う。</li>
		<li>management…上記以外で本システムが管理すべき情報。</li>
	</ul>


	<p>ips,vlansに関しては予めリソースを登録しておく。<br />これは本システムが利用できるWAN側ip等を理解しておくのではなく、<br />DBを見ることで利用できるipを理解できるようにするためである。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>テーブル名</th>
			<th>カラム名</th>
			<th>オプション</th>
			<th>内容</th>
		</tr>
		<tr>
			<td>hypervisors</td>
			<td>hypervisor_id</td>
			<td>pk</td>
			<td>255文字列。kvmホストのホスト名。本名称は名前解決できる必要がある。</td>
		</tr>
		<tr>
			<td style="vertical-align:top;" rowspan="2">companies</td>
			<td>company_id</td>
			<td>pk</td>
			<td>255までの整数(8bit)。ただし、0,1はisb用、253～255,125～128(vlan制限に該当する事業者)は使用不可とし残りを利用する。</td>
		</tr>
		<tr>
			<td>company_name</td>
			<td>not null</td>
			<td>255の文字列。企業名。必須項目とする。</td>
		</tr>
		<tr>
			<td style="vertical-align:top;" rowspan="3">ips</td>
			<td>ip</td>
			<td>pk</td>
			<td>18文字列(数字3*4=12,ドット3,プレフィックス3計18文字 ex:202.215.185.100/32)。仮想ルータに付与するWAN側のip。</td>
		</tr>
		<tr>
			<td>company_id</td>
			<td>fk</td>
			<td>企業id。nullの場合は未予約のリソースを表す。</td>
		</tr>
		<tr>
			<td>is_default</td>
			<td>not null</td>
			<td><strong>true</strong>:企業のデフォルトip <strong>false</strong>:デフォルトでないip。</td>
		</tr>
		<tr>
			<td style="vertical-align:top;" rowspan="3">vlans</td>
			<td>vlan_id</td>
			<td>pk</td>
			<td>4095までの整数(12bit)。ただし、0,4095,1006～1024(FC等の仕様による制限)は使用不可とする。使用不可のvlanはシステム管理用のcompany_idを利用して予約済であることを示す。</td>
		</tr>
		<tr>
			<td>company_id</td>
			<td>fk</td>
			<td>企業id。nullの場合は未予約のリソースを表す。</td>
		</tr>
		<tr>
			<td>is_default</td>
			<td>not null</td>
			<td><strong>true</strong>:企業のデフォルトvlan <strong>false</strong>:デフォルトでないvlan。</td>
		</tr>
		<tr>
			<td style="vertical-align:top;" rowspan="3">disks</td>
			<td>disk_id</td>
			<td>pk</td>
			<td>255文字列。lvm領域の名前を指定する。</td>
		</tr>
		<tr>
			<td>company_id</td>
			<td>fk,not null</td>
			<td>企業id。diskに関しては企業idに属さないlvm領域は回収するためnullを許容せず必須項目とする。</td>
		</tr>
		<tr>
			<td>volume</td>
			<td>not null</td>
			<td>integer。ディスク容量をGB単位で指定する。</td>
		</tr>
		<tr>
			<td style="vertical-align:top;" rowspan="4">instances</td>
			<td>instance_id</td>
			<td>pk</td>
			<td>255文字列。仮想インスタンスのドメイン名。本名称は名前解決できる必要がある。</td>
		</tr>
		<tr>
			<td>company_id</td>
			<td>fk,not null</td>
			<td>企業id。必須項目とする。</td>
		</tr>
		<tr>
			<td>hypervisor_id</td>
			<td>fk,not null</td>
			<td>インスタンスが稼働するkvmホスト。必須項目とする。</td>
		</tr>
		<tr>
			<td>is_virtual_router</td>
			<td>not null</td>
			<td><strong>true</strong>:仮想ルータ <strong>false</strong>:通常のインスタンス。</td>
		</tr>
		<tr>
			<td style="vertical-align:top;" rowspan="3">instances_ips</td>
			<td>instance_id</td>
			<td>fk,not null</td>
			<td>ipを追加するインスタンス。必須項目とする。</td>
		</tr>
		<tr>
			<td>ip</td>
			<td>fk,not null</td>
			<td>付与するip。必須項目とする。</td>
		</tr>
		<tr>
			<td>company_id</td>
			<td>fk,not null</td>
			<td>企業id。必須項目とする。</td>
		</tr>
		<tr>
			<td style="vertical-align:top;" rowspan="4">instances_vlans</td>
			<td>instance_id</td>
			<td>fk,not null</td>
			<td>vlanを設定するインスタンス。必須項目とする。</td>
		</tr>
		<tr>
			<td>vlan_id</td>
			<td>fk,not null</td>
			<td>インスタンスに設定するvlan。</td>
		</tr>
		<tr>
			<td>company_id</td>
			<td>fk,not null</td>
			<td>企業id。必須項目とする。</td>
		</tr>
		<tr>
			<td>interface_name</td>
			<td>not null</td>
			<td>255文字列。kvmホスト側から確認できるインターフェイス名。同一vlanに仮想インスタンスの複数のNICが繋がる可能性があるため、この情報により区別する。ex) vnet0,vnet1,vnet2等</td>
		</tr>
		<tr>
			<td style="vertical-align:top;" rowspan="4">instances_disks</td>
			<td>instance_id</td>
			<td>fk,not null</td>
			<td>disk追加を行うインスタンス。必須項目とする。</td>
		</tr>
		<tr>
			<td>disk_id</td>
			<td>fk,not null</td>
			<td>インスタンスに設定するdisk。</td>
		</tr>
		<tr>
			<td>company_id</td>
			<td>fk,not null</td>
			<td>企業id。必須項目とする。</td>
		</tr>
		<tr>
			<td>disk_name</td>
			<td>not null</td>
			<td>255文字列。仮想インスタンスにアタッチする際につける名称。ex) vda,vdb,vdc等。</td>
		</tr>
		<tr>
			<td style="vertical-align:top;" rowspan="8">instances_firewall_rules</td>
			<td>instance_id</td>
			<td>fk,not null</td>
			<td>仮想インスタンス。必須項目とする。ここで指定する仮想インスタンスは仮想ルータであること。</td>
		</tr>
		<tr>
			<td>company_id</td>
			<td>fk,not null</td>
			<td>企業id。必須項目とする。</td>
		</tr>
		<tr>
			<td>rule_number</td>
			<td>not null</td>
			<td>1-9999までの整数。仮想ルータにfirewall設定をする際に指定するルール番号。番号に意味はないが、重複すると設定を上書きしてしまう可能性がある。</td>
		</tr>
		<tr>
			<td>protocol</td>
			<td>not null</td>
			<td>255文字列。通信プロトコルを指定する。ex) tcp,udp,all etc</td>
		</tr>
		<tr>
			<td>source_ip</td>
			<td>not null</td>
			<td>18文字列。必須項目とする。0.0.0.0/0を指定する事でanyを意味する。</td>
		</tr>
		<tr>
			<td>source_port</td>
			<td>not null</td>
			<td>255文字列。必須項目とする。範囲指定も可能。ex)80,1001-1005</td>
		</tr>
		<tr>
			<td>destination_ip</td>
			<td>not null</td>
			<td>18文字列。必須項目とする。このipによって仮想ルータのインターフェイス名を判別する。</td>
		</tr>
		<tr>
			<td>destination_port</td>
			<td>not null</td>
			<td>255文字列。必須項目とする。範囲指定も可能。ex)80,1001-1005</td>
		</tr>
		<tr>
			<td style="vertical-align:top;" rowspan="9">instances_portforward_rules</td>
			<td>instance_id</td>
			<td>fk,not null</td>
			<td>仮想インスタンス。必須項目とする。ここで指定する仮想インスタンスは仮想ルータであること。</td>
		</tr>
		<tr>
			<td>company_id</td>
			<td>fk,not null</td>
			<td>企業id。必須項目とする。</td>
		</tr>
		<tr>
			<td>rule_number</td>
			<td>not null</td>
			<td>1-9999までの整数。仮想ルータにportforward設定をする際に指定するルール番号。番号に意味はないが、重複すると設定を上書きしてしまう可能性がある。portforwardの場合はnat_type毎に重複の有無を考慮する必要がある。</td>
		</tr>
		<tr>
			<td>protocol</td>
			<td>not null</td>
			<td>255文字列。通信プロトコルを指定する。ex) tcp,udp,all etc</td>
		</tr>
		<tr>
			<td>source_ip</td>
			<td>not null</td>
			<td>18文字列。必須項目とする。0.0.0.0/0を指定する事でanyを意味する。</td>
		</tr>
		<tr>
			<td>source_port</td>
			<td>not null</td>
			<td>255文字列。必須項目とする。範囲指定も可能。ex)80,1001-1005</td>
		</tr>
		<tr>
			<td>destination_ip</td>
			<td>not null</td>
			<td>18文字列。必須項目とする。このipによって仮想ルータのインターフェイス名を判別する。</td>
		</tr>
		<tr>
			<td>destination_port</td>
			<td>not null</td>
			<td>255文字列。必須項目とする。範囲指定も可能。ex)80,1001-1005</td>
		</tr>
		<tr>
			<td>nat_type</td>
			<td>not null</td>
			<td>255文字列。必須項目とする。sourceおよびdestinationを指定し、各々source nat,destination natを表す。</td>
		</tr>
	</table>




	<a name="Clocon設計_lvmの名称について"></a>
<h2 >lvmの名称について<a href="#Clocon設計_lvmの名称について" class="wiki-anchor">&para;</a></h2>


	<p>lvmを作成する場合に以下の命名規約に従う。</p>


	<ul>
	<li>仮想インスタンスに提供する領域　->　仮想インスタンスと同一名称とする</li>
		<li>仮想インスタンスに追加で提供する領域　->　仮想インスタンス_[device name]とする</li>
	</ul>


	<p>仮想インスタンスがtest1という名称の場合を想定した例を以下に示す(vgはISCSIVolGroup00を想定)。</p>


<pre>
[root@kvm-01 ~]# lvcreate -L 10G -n test1 ISCSIVolGroup00
  Logical volume "test1" created
[root@kvm-01 ~]# lvcreate -L 10G -n test1_vda ISCSIVolGroup00
  Logical volume "test1_vda" created
[root@kvm-01 ~]# virsh attach-disk test1 /dev/ISCSIVolGroup00/test1_vda vda
</pre>

	<p>デバイス名はvdaから始まり、vdb/vdc/vdd...という規則に従い増やしていく。</p>


	<a name="Clocon設計_テンプレート"></a>
<h2 >テンプレート<a href="#Clocon設計_テンプレート" class="wiki-anchor">&para;</a></h2>


	<p>本システムでは即座に利用できることを重視して、テンプレートを用いる。<br />テンプレートの実体は選択できるスペックの組み合わせ毎のOSインストール済パーティション(lvm領域)である。<br />テンプレート(lvmのLV Name)の命名規則を以下に示す。</p>


	<p>template_[OS種別:名称とバージョンがわかる情報]_[bit数]__[cpu:個数]_[memory:MB単位]_[disk:GB単位]</p>


	<p>本システムでは以下のテンプレートを用意しておく。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>lvm名称</th>
		</tr>
		<tr>
			<td>template_centos6.6_64_1_1024_10</td>
		</tr>
	</table>




	<a name="Clocon設計_リターンコード"></a>
<h2 >リターンコード<a href="#Clocon設計_リターンコード" class="wiki-anchor">&para;</a></h2>


	<p>各種コマンドのリターンコードを定義する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>リターンコード</th>
			<th>内容</th>
		</tr>
		<tr>
			<td>0</td>
			<td>成功</td>
		</tr>
	</table>




	<a name="Clocon設計_ネットワークインターフェイスの命名規則"></a>
<h2 >ネットワークインターフェイスの命名規則<a href="#Clocon設計_ネットワークインターフェイスの命名規則" class="wiki-anchor">&para;</a></h2>


	<p>各種インターフェイスが統一されていないとコマンドの動作は失敗する</p>


<hr />


	<a name="Clocon設計_シーケンス"></a>
<h1 >●シーケンス<a href="#Clocon設計_シーケンス" class="wiki-anchor">&para;</a></h1>


	<p>各機能のシーケンスを記載する。</p>


	<a name="Clocon設計_kvmホスト定義"></a>
<h3 >kvmホスト定義<a href="#Clocon設計_kvmホスト定義" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.define.kvm.shにハイパーバイザのホスト名を引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>hypervisorsテーブルに引数のホスト名が登録されていないことを確認する</li>
		<li>既に登録されている場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>hypervisorsテーブルに引数のホスト名を登録する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_kvmホスト破棄"></a>
<h3 >kvmホスト破棄<a href="#Clocon設計_kvmホスト破棄" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.undefine.kvm.shにハイパーバイザのホスト名を引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>hypervisorsテーブルに引数のホスト名が登録されていることを確認する</li>
		<li>登録がない場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>hypervisorsテーブルから引数のホスト名を削除する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_企業id定義"></a>
<h3 >企業id定義<a href="#Clocon設計_企業id定義" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.define.company.shに企業id、企業名を引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>companiesテーブルに引数の企業idが登録されていないこと確認する</li>
		<li>既に登録がある場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>companiesテーブルに情報を登録する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_企業id削除"></a>
<h3 >企業id削除<a href="#Clocon設計_企業id削除" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.undefine.company.shに企業idを引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>companiesテーブルに引数の企業idが登録されていること確認する</li>
		<li>登録がない場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>companiesテーブルから情報を削除する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_インスタンス定義"></a>
<h3 >インスタンス定義<a href="#Clocon設計_インスタンス定義" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.define.instance.shに企業id、ドメイン名、OS種別(ビット数を含む)、cpu、メモリ、ディスク容量を引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>hypervisorsテーブルからkvmホストを情報を取得する</li>
		<li>kvmホストが存在しない場合はハイパーバイザが存在しない旨を表示してエラー終了する</li>
		<li>引数のスペック情報からテンプレート名を決定し、そのテンプレート名のインスタンス定義ファイルが存在するか確認する</li>
		<li>ファイルが存在しない場合はスペックが一致しない旨を表示してエラー終了する</li>
		<li>instancesテーブルから引数のドメイン名が使用可能か(重複していないか)を確認する</li>
		<li>ドメイン名が(企業idが異なるドメイン名も含み)重複した場合は同一名称が存在する旨を表示してエラー終了する</li>
		<li>kvmホストが複数存在する場合は仮想インスタンスを稼働させる負荷の低いkvmホストをlibvirtを用いて(nodecpustats,nodememstats)確認する <span style="color: red;"> <strong>TODO</strong>:負荷の低いという定義の選択方法を要検討</span></li>
		<li>追加でインスタンスが稼働できない場合はkvmホストのスペック不足である旨を表示してエラー終了する</li>
		<li><span style="color: blue;">[これ以降環境変更を行うためエラー時にはロールバックの検討が必要になる。環境が変更があった直後の時点を採番する。]</span></li>
		<li>新規lvm領域の作成依頼をkvmホストへ行う。lvm名称はドメイン名と同一名称とする</li>
		<li>lvm領域の作成に失敗した場合はディスク確保失敗の旨を表示してエラー終了する　 <span style="color: blue;">①lvm領域の追加：以降のエラーではlvremoveを行う必要がある</span></li>
		<li>テンプレートからの複製を新規lvm領域へ行う</li>
		<li>複製が失敗した場合はテンプレートの展開失敗の旨を表示してエラー終了する　 <span style="color: blue;">②仮想インスタンスの追加：以降のエラーではvirsh undefineを行う必要がある</span></li>
		<li>仮想インスタンスのインターフェイスのMACアドレスとドメイン名で固定ipとなるように仮想ルータのdhcp機能に設定変更を追加する</li>
		<li>仮想ルータの設定反映に失敗した場合はその旨を表示してエラー終了する</li>
		<li>ドメイン名と割り当てるipアドレスをdnsに登録する</li>
		<li>dnsへの登録に失敗した場合はその旨を表示してエラー終了する</li>
		<li>vlansテーブルから企業IDを元にデフォルトvlan_idを取得する</li>
		<li>vlan_idインターフェイスに繋がるブリッジが存在しない場合はブリッジを作成する(仮想NICはまだ起動していないため不明)</li>
		<li>vlan_idがない,ブリッジ操作に失敗した場合はネットワーク設定失敗の旨を表示してエラー終了する</li>
		<li>virt-copy-in等を用いてホスト名の変更、xmlを編集して接続するブリッジの情報書き換え等の作業を行う <span style="color: red;"> <strong>TODO</strong>:xmlの変更方法を要検討</span></li>
		<li>上記作業の変更作業に失敗した場合、その旨を表示しエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する <span style="color: red;"> <strong>TODO</strong>:トランザクション開始を更新直前においているがselectの結果を保証するためにもコマンド開始時に実行すべきかもしれない。ただしその場合は各エラー時にロールバックする必要があるのと仮想インスタンス生成時間が長いためテーブルのロック時間も長くなる点に注意する必要がある</span></li>
		<li>instancesテーブルに作成した仮想インスタンスを挿入する</li>
		<li>instances_vlansテーブルに所属するvlan情報を挿入する</li>
		<li>instances_disksテーブルに情報を登録する</li>
		<li>挿入に失敗した場合はローバックを行いDB書き込み失敗の旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する　 <span style="color: blue;">③DBレコードの追加：以降インスタンス削除時には削除を行う必要がある</span>  <span style="color: red;"> <strong>TODO</strong>:仮想インスタンスの起動方法の確認</span></li>
	</ol>


	<a name="Clocon設計_インスタンス破棄"></a>
<h3 >インスタンス破棄<a href="#Clocon設計_インスタンス破棄" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.undefine.instance.shに企業id、ドメイン名を引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>instancesテーブルからドメイン名の有無と稼働するハイパーバイザを確認する</li>
		<li>ドメイン名が存在しない、ハイパーバイザ上で定義されていない場合は指定した仮想インスタンスがない旨を表示しエラー終了する</li>
		<li>仮想インスタンスが稼働中で強制停止オプションも指定されていない場合は稼働中である旨を表示しエラー終了する</li>
		<li>仮想インスタンスが稼働中の場合は強制停止オプションが指定されていれば強制停止を行う</li>
		<li>仮想インスタンスの削除を行う</li>
		<li>削除に失敗した場合はその旨を表示してエラー終了する <span style="color: blue;">インスタンス定義の②に対応</span></li>
		<li>lvm領域の解放を行う</li>
		<li>lvm領域の解放に失敗した場合はその旨を表示してエラー終了する <span style="color: blue;">インスタンス定義の①に対応</span></li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>instances_vlansテーブルから該当のドメイン名のレコードを削除する</li>
		<li>instancesテーブルから該当のドメイン名のレコードを削除する</li>
		<li>削除に失敗した場合はロールバックを行いDBレコード削除失敗の旨を表示してエラー終了する</li>
		<li>削除に成功した場合はコミットを行い正常終了する <span style="color: blue;">インスタンス定義の③に対応</span></li>
	</ol>


	<a name="Clocon設計_インスタンス起動"></a>
<h3 >インスタンス起動<a href="#Clocon設計_インスタンス起動" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.start.instance.shに企業id、ドメイン名を引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>instancesテーブルからドメイン名の有無と稼働するハイパーバイザを確認する</li>
		<li>ドメイン名が存在しない、ハイパーバイザ上で定義されていない場合は指定した仮想インスタンスがない旨を表示しエラー終了する</li>
		<li>仮想インスタンスが稼働中であれば実行中の旨を表示して <strong>正常終了</strong> する</li>
		<li>仮想インスタンスを起動する</li>
		<li>起動に失敗した場合はその旨を表示してエラー終了する</li>
		<li>起動に成功した場合は正常終了する</li>
	</ol>


	<a name="Clocon設計_インスタンス停止"></a>
<h3 >インスタンス停止<a href="#Clocon設計_インスタンス停止" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.shutdown.instance.shに企業id、ドメイン名を引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>instancesテーブルからドメイン名の有無と稼働するハイパーバイザを確認する</li>
		<li>ドメイン名が存在しない、ハイパーバイザ上で定義されていない場合は指定した仮想インスタンスがない旨を表示しエラー終了する</li>
		<li>仮想インスタンスが停止していればその旨を表示して <strong>正常終了</strong> する</li>
		<li>強制停止オプションの指定がある場合は仮想インスタンスを強制停止する</li>
		<li>強制停止オプションの指定がない場合は仮想インスタンスを停止する</li>
		<li>停止に失敗した場合はその旨を表示してエラー終了する</li>
		<li>停止に成功した場合は正常終了する</li>
	</ol>


	<a name="Clocon設計_グローバルIP定義"></a>
<h3 >グローバルIP定義<a href="#Clocon設計_グローバルIP定義" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.define.net.wan.shに企業id、グローバルipを引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>global_ipsテーブルから引数で与えられたグローバルipが存在しないことを確認する</li>
		<li>存在する場合は既に利用されている旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>global_ipsテーブルに情報を登録する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_グローバルIP破棄"></a>
<h3 >グローバルIP破棄<a href="#Clocon設計_グローバルIP破棄" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.undefine.net.wan.shに企業id、グローバルipを引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>global_ipsテーブルから引数で与えられたグローバルipが存在することを確認する</li>
		<li>存在しない場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>global_ipsテーブルから情報を削除する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_グローバルIPアタッチ"></a>
<h3 >グローバルIPアタッチ<a href="#Clocon設計_グローバルIPアタッチ" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.attach.net.wan.shに企業id、ドメイン名、ipアドレスを引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>instancesテーブルからドメイン名の有無と稼働するハイパーバイザ、かつ仮想ルータであるかどうかを確認する</li>
		<li>ドメイン名が存在しない、ハイパーバイザ上で定義されていない、仮想ルータでない場合は指定した仮想インスタンスがない旨を表示しエラー終了する</li>
		<li>global_ipsテーブルに引数のipアドレスが存在することを確認する</li>
		<li>存在しない場合はその旨を表示してエラー終了する</li>
		<li>仮想インスタンスへアタッチする(virsh attach-interface test1 bridge br1)</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>instances_global_ipsテーブルへ情報を登録する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_グローバルIPデタッチ"></a>
<h3 >グローバルIPデタッチ<a href="#Clocon設計_グローバルIPデタッチ" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.detach.net.wan.shに企業id、ドメイン名、ipアドレスを引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>instances_global_ipsテーブルから引数情報でアタッチされていることを確認する</li>
		<li>アタッチされていない場合はその旨を表示してエラー終了する</li>
		<li>仮想インスタンスとデタッチする</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>instances_global_ipsテーブルから情報を削除する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_vlan定義"></a>
<h3 >vlan定義<a href="#Clocon設計_vlan定義" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.define.net.vlan.shに企業id、vlan_idを引数に設定し実行する</li>
		<li>引数のvlan_idがフリーvlanの範囲であることを確認する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>vlansテーブルから引数のvlan_idが存在しないことを確認する</li>
		<li>存在する場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>vlansテーブルに情報を登録する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_vlan破棄"></a>
<h3 >vlan破棄<a href="#Clocon設計_vlan破棄" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.undefine.net.vlan.shに企業id、vlan_idを引数に設定し実行する</li>
		<li>引数のvlan_idがフリーvlanの範囲であることを確認する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>vlansテーブルから引数のvlan_idが存在することを確認する</li>
		<li>存在しない場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>vlansテーブルから情報を削除する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_vlanアタッチ"></a>
<h3 >vlanアタッチ<a href="#Clocon設計_vlanアタッチ" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.attach.net.vlan.shに企業id、ドメイン名、vlan_idを引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>instancesテーブルからドメイン名の有無と稼働するハイパーバイザを確認する</li>
		<li>ドメイン名が存在しない、ハイパーバイザ上で定義されていない場合は指定した仮想インスタンスがない旨を表示しエラー終了する</li>
		<li>vlansテーブルに引数のvlan_idが存在することを確認する</li>
		<li>存在しない場合はその旨を表示してエラー終了する</li>
		<li>ブリッジを作成する(virsh iface-begin発行後virsh iface-bridge eth2.200 br3 --no-stp)</li>
		<li>ブリッジの作成に失敗した場合はロールバックしその旨を表示してエラー終了する</li>
		<li>仮想インスタンスへアタッチする(virsh attach-interface test1 bridge br1)</li>
		<li>失敗した場合はロールバックしその旨を表示してエラー終了する</li>
		<li>作成したブリッジをコミットする(virsh iface-commit)</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>instances_vlansテーブルへ情報を登録する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_vlanデタッチ"></a>
<h3 >vlanデタッチ<a href="#Clocon設計_vlanデタッチ" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.detach.net.vlan.shに企業id、ドメイン名、vlan_idを引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>instances_disksテーブルから引数情報でアタッチされていることを確認する</li>
		<li>アタッチされていない場合はその旨を表示してエラー終了する</li>
		<li>仮想インスタンスとデタッチする</li>
		<li>ブリッジを削除する(virsh iface-begin発行後virsh iface-unbridge br3)</li>
		<li>ブリッジの削除に失敗した場合はその旨を表示してエラー終了する</li>
		<li>ブリッジ削除をコミットする(virsh iface-commit)</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>instances_vlansテーブルから情報を削除する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_disk定義"></a>
<h3 >disk定義<a href="#Clocon設計_disk定義" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.define.disk.shに企業id、領域名、割り当て容量を設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>disksテーブルから引数の領域名が存在しないことを確認する</li>
		<li>存在する場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>disksテーブルに情報を登録する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_disk破棄"></a>
<h3 >disk破棄<a href="#Clocon設計_disk破棄" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.undefine.disk.shに企業id、領域名、割り当て容量を設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>disksテーブルから引数の領域名が存在することを確認する</li>
		<li>存在しない場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>disksテーブルに情報を削除する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_diskアタッチ"></a>
<h3 >diskアタッチ<a href="#Clocon設計_diskアタッチ" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.attach.disk.shに企業id、ドメイン名、領域名を設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>instancesテーブルからドメイン名の有無と稼働するハイパーバイザを確認する</li>
		<li>ドメイン名が存在しない、ハイパーバイザ上で定義されていない場合は指定した仮想インスタンスがない旨を表示しエラー終了する</li>
		<li>disksテーブルに引数の領域名が存在することを確認する</li>
		<li>存在しない場合はその旨を表示してエラー終了する</li>
		<li>lvm領域を確保する(lvcreate)</li>
		<li>lvm領域確保に失敗した場合はその旨を表示してエラー終了する</li>
		<li>仮想インスタンスへアタッチする(virsh attach-disk)</li>
		<li>失敗した場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>instances_disksテーブルに情報を登録する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_diskデタッチ"></a>
<h3 >diskデタッチ<a href="#Clocon設計_diskデタッチ" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.detach.disk.shに企業id、ドメイン名、領域名を設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>instances_disksテーブルに引数情報が存在することを確認する</li>
		<li>存在しない場合はその旨を表示してエラー終了する</li>
		<li>仮想インスタンスからデタッチする(virsh detach-disk)</li>
		<li>失敗した場合はその旨を表示してエラー終了する</li>
		<li>lvm領域を削除する(lvremove)</li>
		<li>失敗した場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>instances_disksテーブルに情報を登録する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_ライブマイグレーション"></a>
<h3 >ライブマイグレーション<a href="#Clocon設計_ライブマイグレーション" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li>clocon.migrate.instance.shに企業id、ドメイン名を引数に設定し実行する</li>
		<li>設定ファイルのDB情報からアクセスするDBを決定する</li>
		<li>instancesテーブルからドメイン名の有無と稼働するハイパーバイザを確認する</li>
		<li>ドメイン名が存在しない、ハイパーバイザ上で定義されていない場合は指定した仮想インスタンスがない旨を表示しエラー終了する</li>
		<li>hypervisorsテーブルからkvmホストを情報を取得する</li>
		<li>稼働しているハイパーバイザ以外で負荷の低いkvmホストを決定する</li>
		<li>追加でインスタンスが稼働できない場合はkvmホストのスペック不足である旨を表示してエラー終了する</li>
		<li>ライブマイグレーションを実行する</li>
		<li>マイグレーションに失敗した場合はその旨を表示してエラー終了する</li>
		<li>DB更新のためにトランザクションを開始する</li>
		<li>instancesテーブルのhypervisor_idを変更する</li>
		<li>失敗した場合はロールバックを行いその旨を表示してエラー終了する</li>
		<li>コミットを行い正常終了する</li>
	</ol>


	<a name="Clocon設計_企業id初期化"></a>
<h3 >企業id初期化<a href="#Clocon設計_企業id初期化" class="wiki-anchor">&para;</a></h3>


	<a name="Clocon設計_ファイアウォール定義"></a>
<h3 >ファイアウォール定義<a href="#Clocon設計_ファイアウォール定義" class="wiki-anchor">&para;</a></h3>


	<a name="Clocon設計_ファイアウォール破棄"></a>
<h3 >ファイアウォール破棄<a href="#Clocon設計_ファイアウォール破棄" class="wiki-anchor">&para;</a></h3>


	<a name="Clocon設計_ポートフォワード定義"></a>
<h3 >ポートフォワード定義<a href="#Clocon設計_ポートフォワード定義" class="wiki-anchor">&para;</a></h3>


	<a name="Clocon設計_ポートフォワード破棄"></a>
<h3 >ポートフォワード破棄<a href="#Clocon設計_ポートフォワード破棄" class="wiki-anchor">&para;</a></h3>


<hr />


	<a name="Clocon設計_業務フロー"></a>
<h1 >●業務フロー<a href="#Clocon設計_業務フロー" class="wiki-anchor">&para;</a></h1>


	<p>本システムの利用シーンとcloconの機能の関係性をフローとして以下に示す。</p>


	<p>作業の中で赤いものはcloconのユーザインターフェイスにより省略可能な作業である。</p>


	<p><img src="/attachments/download/821/business_flow1.png" alt="" /></p>


<hr />


	<a name="Clocon設計_可用性"></a>
<h1 >●可用性<a href="#Clocon設計_可用性" class="wiki-anchor">&para;</a></h1>


	<p>サービスを停止させないために冗長化を図る。</p>


	<a name="Clocon設計_cloconの冗長性"></a>
<h2 >cloconの冗長性<a href="#Clocon設計_cloconの冗長性" class="wiki-anchor">&para;</a></h2>


	<p>clocon自体はステートレスな作りとし全ての情報はDB等を利用する。<br />ただし、DBへのアクセスおよび、<br />kvmホストへのアクセスについてリトライ機構等を盛り込む必要がある。</p>


	<a name="Clocon設計_DBの冗長性"></a>
<h2 >DBの冗長性<a href="#Clocon設計_DBの冗長性" class="wiki-anchor">&para;</a></h2>


	<p>postgresのレプリケーションにより冗長化を行う。</p>


<hr />


	<a name="Clocon設計_性能拡張性"></a>
<h1 >●性能・拡張性<a href="#Clocon設計_性能拡張性" class="wiki-anchor">&para;</a></h1>


	<a name="Clocon設計_cloconのスケールアウト"></a>
<h2 >cloconのスケールアウト<a href="#Clocon設計_cloconのスケールアウト" class="wiki-anchor">&para;</a></h2>


	<p>clocon(内部のDBも含めて)はスケールアウトを考慮しない。<br />これはインスタンスの設定や操作は時間的に連続して行う可能性が低く、<br />ユーザ数も多くないと考えられるためである。<br />今後の展望としてWebのインターフェイスの実装がある。<br />上記のインターフェイスを用意した場合に、<br />cloconの拡張性についても検討することとする。</p>


	<p>！！！！！！！！！！！！！！機能の拡張性について言及する</p>


	<a name="Clocon設計_kvmのスケールアウト"></a>
<h2 >kvmのスケールアウト<a href="#Clocon設計_kvmのスケールアウト" class="wiki-anchor">&para;</a></h2>


	<p>kvmホストを追加した場合、DBへの情報の追加および<br />名前解決のための設定の追加が必要となる。<br />DBへの情報追加はcloconが持つ機能を利用して行う。<br />名前解決のための設定はDNSへの登録もしくはhostsファイルへの追記を意味する。</p>


<hr />


	<a name="Clocon設計_セキュリティ"></a>
<h1 >●セキュリティ<a href="#Clocon設計_セキュリティ" class="wiki-anchor">&para;</a></h1>


	<p>セキュリティについては異なる顧客のインスタンスや情報が同一の物理マシンやNWを流れるため厳格に分離する必要がある。<br />これを実現する技術としてVLANによるNWの分離を行う。<br />また、DBに関しては企業を一意に特定するIDを必ず付与することにより情報を混同させない。</p>


	<p>上記はLAN内の内容であるが、WAN側からの防御についても検討が必要である。<br />インスタンス唯一のWANインターフェイスとして仮想ルータが存在する。<br />仮想ルータにはパケットフィルタによって外部からの通信を遮断する。</p>


<hr />


	<a name="Clocon設計_運用保守性"></a>
<h1 >●運用・保守性<a href="#Clocon設計_運用保守性" class="wiki-anchor">&para;</a></h1>


	<p>平常時の監視方法および各種障害時の復旧方法を定義する。</p>


	<a name="Clocon設計_ログ"></a>
<h2 >ログ<a href="#Clocon設計_ログ" class="wiki-anchor">&para;</a></h2>


	<p>cloconのログは「/var/log/clocon/clocon_command.log」へ出力する。<br />一日おきにログローテートを実施する。<br />保管するログの世代は1ヶ月とする。<br />出力ログのフォーマットは以下とする。<br /><pre>

</pre></p>


	<a name="Clocon設計_ログメッセージ一覧"></a>
<h2 >ログメッセージ一覧<a href="#Clocon設計_ログメッセージ一覧" class="wiki-anchor">&para;</a></h2>


	<p>本システムが出力するログ内容を以下に記載する。</p>


	<a name="Clocon設計_監視"></a>
<h2 >監視<a href="#Clocon設計_監視" class="wiki-anchor">&para;</a></h2>


	<p>サービスに影響のある項目について過不足なく監視を行うものとする。<br />ここで、サービスとして以下を定義する。</p>


	<ul>
	<li>clocon関係の操作が正常にできること</li>
	</ul>


	<a name="Clocon設計_障害"></a>
<h2 >障害<a href="#Clocon設計_障害" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>DB
	<ul>
	<li>バックアップの取得方法</li>
		<li>フェールオーバ後の作業</li>
		<li>バックアップからのリストア方法</li>
	</ul></li>
	</ul>


<hr />


	<a name="Clocon設計_clocon技術調査"></a>
<h2 ><a href="#Clocon技術調査" class="wiki-page">clocon技術調査</a><a href="#Clocon設計_clocon技術調査" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>Contractor Project Initiation Complete &#38; Approved（要件定義）</li>
		<li>Requirements Completed &#38; Approved （基本設計）</li>
		<li>Design Completed &#38; Approved（詳細設計）</li>
		<li>Coding/Unit Test Completed &#38; Approved（開発・単体テスト）</li>
		<li>Integration/Testing Completed &#38; Approved（結合テスト）</li>
		<li>System and Acceptance Test Completed/Approved（システム（プロダクト）テスト）</li>
		<li>Field Testing Completed &#38; Approved （運用テスト）</li>
	</ul>


<hr />


	<a name="Clocon設計_現状設定のメモ"></a>
<h2 >現状設定のメモ<a href="#Clocon設計_現状設定のメモ" class="wiki-anchor">&para;</a></h2>


	<p>dhcpのインターフェイス指定</p>


<pre>
# egrep -v "(^#|^$)" /etc/sysconfig/dhcpd
DHCPDARGS=bond0
</pre>

	<p>nginxのpxeboot用設定</p>


<pre>
# mkdir -p /var/www/html/pxeboot
# egrep -v "(^#|^$)" /etc/nginx/conf.d/pxeboot.conf
server {
    listen       8888;
    server_name  10.16.33.1;
    location / {
        root   /var/www/html/pxeboot;
        index  index.html index.htm;
    }
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}
</pre>

	<p>一時的にipの設定</p>


<pre>
# ip addr add 10.16.33.1/24 dev bond0
# ip addr show
</pre>

	<p>現状はisoをマウントするが、まるごとコピーを行っても良い。</p>


<pre>
# mount -o loop ~/iso/CentOS-6.6-x86_64-bin-DVD1.iso /var/www/html/pxeboot/iso/
</pre>
<hr />
<a name="DB設計" />
<a name="DB設計_DB設計"></a>
<h1 >DB設計<a href="#DB設計_DB設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#DB設計_DB設計">DB設計</a></li><li><a href="#DB設計_Introduction">●Introduction</a><ul><li><a href="#DB設計_目的">目的</a></li><li><a href="#DB設計_定義範囲">定義範囲</a></li></ul>
</li><li><a href="#DB設計_構成">●構成</a><ul><li><a href="#DB設計_サーバ構成">サーバ構成</a></li><li><a href="#DB設計_パーティション構成">パーティション構成</a></li><li><a href="#DB設計_ネットワーク構成">ネットワーク構成</a></li><li><a href="#DB設計_導入パッケージ">導入パッケージ</a></li><li><a href="#DB設計_自動起動設定">自動起動設定</a></li><li><a href="#DB設計_ログ設定">ログ設定</a><ul><li><a href="#DB設計_ログローテ">ログローテ</a></li><li><a href="#DB設計_出力フォーマット">出力フォーマット</a></li></ul>
</li></ul>
</li><li><a href="#DB設計_システム設計">●システム設計</a><ul><li><a href="#DB設計_drbd設定">drbd設定</a><ul><li><a href="#DB設計_global_commonconf">global_common.conf</a></li><li><a href="#DB設計_postgresres">postgres.res</a></li></ul>
</li><li><a href="#DB設計_heartbeat設定">heartbeat設定</a><ul><li><a href="#DB設計_hacf">ha.cf</a></li><li><a href="#DB設計_authkeys">authkeys</a></li></ul>
</li><li><a href="#DB設計_postgresql設定">postgresql設定</a><ul><li><a href="#DB設計_postgresqlconf">postgresql.conf</a></li><li><a href="#DB設計_pg_hbaconf">pg_hba.conf</a></li></ul>
</li></ul>
</li><li><a href="#DB設計_冗長化">●冗長化</a><ul><li><a href="#DB設計_障害の分類と対応方法">障害の分類と対応方法</a><ul><li><a href="#DB設計_障害のパターンと障害検知方法">障害のパターンと障害検知方法</a></li></ul>
</li><li><a href="#DB設計_pacemaker設定">pacemaker設定</a></li></ul>
</li><li><a href="#DB設計_バックアップ">●バックアップ</a><ul><li><a href="#DB設計_バックアップ概要">バックアップ概要</a></li><li><a href="#DB設計_バックアップ設定">バックアップ設定</a></li><li><a href="#DB設計_リカバリ">リカバリ</a></li></ul></li></ul>


<hr />


	<a name="DB設計_Introduction"></a>
<h1 >●Introduction<a href="#DB設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="DB設計_目的"></a>
<h2 >目的<a href="#DB設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>本書はクラウドシステムで利用されるDBについて定義する。<br />DBは可用性を検討したシステムであることが求められる。<br />本書は仕様書の既読者を対象とする。</p>


	<a name="DB設計_定義範囲"></a>
<h2 >定義範囲<a href="#DB設計_定義範囲" class="wiki-anchor">&para;</a></h2>


	<p>定義範囲を以下に示す。<br />ルータの詳細は記述しないが、<br />障害分類を行う上で疎通確認対象となる。</p>


	<p><img src="/attachments/download/855/db_system_define_range.png" alt="" /></p>


<hr />


	<a name="DB設計_構成"></a>
<h1 >●構成<a href="#DB設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="DB設計_サーバ構成"></a>
<h2 >サーバ構成<a href="#DB設計_サーバ構成" class="wiki-anchor">&para;</a></h2>


	<p>冗長構成を考慮したDB構成を以下に示す。</p>


	<p><img src="/attachments/download/856/db_system_structure.png" alt="" /></p>


	<p>DBサーバは物理サーバ2台をHAクラスタとして稼働する。<br />サービス提供用にVIPを用意し、クライアントはVIPへの接続のみを意識する。<br />また、ローカルストレージをレプリケーションすることで、ディスクアクセスの排他制御等を考慮する必要がないシェアードナッシング構成となる。<br />これによりDBシステムのフェールオーバが可能となり、障害時のダウンタイムを数秒単位に抑えられる。</p>


	<p>DBサービスを提供するネットワークは冗長化し耐障害性を向上させる。<br />ハートビート通信を行うネットワークは物理サーバ同士の直接接続を行う。<br />ハートビート通信の障害はスプリットブレインを発生し重度のサーバ障害になるため、<br />これについても冗長化を行う。</p>


	<a name="DB設計_パーティション構成"></a>
<h2 >パーティション構成<a href="#DB設計_パーティション構成" class="wiki-anchor">&para;</a></h2>


	<p>他のサーバと比較しパーティション構成に違いはない。<br />ただし、DBサービスが利用するディスク領域についてはpacemakerによりactiveサーバ側のみがdrbd領域をマウントしている。</p>


	<a name="DB設計_ネットワーク構成"></a>
<h2 >ネットワーク構成<a href="#DB設計_ネットワーク構成" class="wiki-anchor">&para;</a></h2>


	<p>サービスネットワーク<br />マネジメントネットワーク<br />ストレージネットワーク</p>


	<a name="DB設計_導入パッケージ"></a>
<h2 >導入パッケージ<a href="#DB設計_導入パッケージ" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>パッケージ名</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>drbd84-utils.x86_64</td>
			<td>drbdadm等の管理コマンド</td>
		</tr>
		<tr>
			<td>kmod-drbd84.x86_64</td>
			<td>ストレージレプリケーションを実現するソフトウェアでシェアードナッシングを実現する。drbd本体</td>
		</tr>
		<tr>
			<td>pacemaker-1.0.13-2.el6.x86_64</td>
			<td>リソース制御機能を提供するパッケージ</td>
		</tr>
		<tr>
			<td>heartbeat-3.0.5-1.1.el6.x86_64</td>
			<td>クラスタ制御機能を提供するパッケージ</td>
		</tr>
		<tr>
			<td>postgresql93-server.x86_64</td>
			<td>RDBMSを提供するパッケージ</td>
		</tr>
	</table>




	<a name="DB設計_自動起動設定"></a>
<h2 >自動起動設定<a href="#DB設計_自動起動設定" class="wiki-anchor">&para;</a></h2>


	<a name="DB設計_ログ設定"></a>
<h2 >ログ設定<a href="#DB設計_ログ設定" class="wiki-anchor">&para;</a></h2>


	<p>postgresqlの場合、ログの出力先はデータディレクトリ配下となる。</p>


	<a name="DB設計_ログローテ"></a>
<h3 >ログローテ<a href="#DB設計_ログローテ" class="wiki-anchor">&para;</a></h3>


	<a name="DB設計_出力フォーマット"></a>
<h3 >出力フォーマット<a href="#DB設計_出力フォーマット" class="wiki-anchor">&para;</a></h3>


	<p>今回のDBは冗長構成になるため、<br />ログにどちらのサーバかを判断する情報を載せる必要がある。</p>


<hr />


	<a name="DB設計_システム設計"></a>
<h1 >●システム設計<a href="#DB設計_システム設計" class="wiki-anchor">&para;</a></h1>


	<a name="DB設計_drbd設定"></a>
<h2 >drbd設定<a href="#DB設計_drbd設定" class="wiki-anchor">&para;</a></h2>


	<a name="DB設計_global_commonconf"></a>
<h3 >global_common.conf<a href="#DB設計_global_commonconf" class="wiki-anchor">&para;</a></h3>


	<p>drbdのグローバル設定を行う設定ファイル。<br />/etc/drbd.d/global_common.conf<br /><pre>
global {
  usage-count no;
}
common {
  # 完全同期のCプロトコルを設定
  protocol C;
  # 基本的にpacemakerにて管理させるためハンドラの設定は行わない
  handlers {
  }
  startup {
    # 以下の設定がないと片系のノードの起動を待ち続ける
    wfc-timeout 10;
    degr-wfc-timeout 10;
    outdated-wfc-timeout 10;
  }
  options {
  }
  disk {
  }
  net {
  }
}
</pre></p>


	<a name="DB設計_postgresres"></a>
<h3 >postgres.res<a href="#DB設計_postgresres" class="wiki-anchor">&para;</a></h3>


	<p>今回アプリケーション毎にストレージレプリケーションを行うため、<br />アプリケーション名に.resというファイル名を用意する。<br />/etc/drbd.d/postgres.res<br /><pre>
resource postgres {
  # 同一サーバ内でdrbdサービスを並列で動作させる場合は、
  # device,diskの値を変更する必要がある。
  device /dev/drbd0;
  disk /dev/vg_drbd/lv_drbd;
  meta-disk internal;
  on XXX {
    address XXX.XXX.XXX.XXX:7788;
  }
  on XXX {
    address XXX.XXX.XXX.XXX:7788;
  }
}
</pre></p>


	<a name="DB設計_heartbeat設定"></a>
<h2 >heartbeat設定<a href="#DB設計_heartbeat設定" class="wiki-anchor">&para;</a></h2>


	<a name="DB設計_hacf"></a>
<h3 >ha.cf<a href="#DB設計_hacf" class="wiki-anchor">&para;</a></h3>


	<p>ha.cfは対向ノードのホスト名や、heartbeat通信方法等を規定する。</p>


	<p>/etc/ha.d/ha.cf<br /><pre>
pacemaker on
logfacility local1
keepalive 2
deadtime 30
warntime 10
initdead 120
udpport 694
auto_failback off
watchdog /dev/watchdog
ucast ethX XXX.XXX.XXX.XXX
node XXX1
node XXX2
</pre></p>


	<a name="DB設計_authkeys"></a>
<h3 >authkeys<a href="#DB設計_authkeys" class="wiki-anchor">&para;</a></h3>


	<p>authkeysは認証方法とキー設定を指定するファイルである。</p>


	<p>/etc/ha.d/authkeys<br /><pre>
auth 1
1 sha1 isb_cloud_iaas
</pre></p>


	<p>上記ファイルはアクセス権を8進表記にて600にする必要がある。</p>


	<a name="DB設計_postgresql設定"></a>
<h2 >postgresql設定<a href="#DB設計_postgresql設定" class="wiki-anchor">&para;</a></h2>


	<a name="DB設計_postgresqlconf"></a>
<h3 >postgresql.conf<a href="#DB設計_postgresqlconf" class="wiki-anchor">&para;</a></h3>


	<p>postgresqlはVIP向けにサービス提供するため、<br />リッスンするIPをVIPに設定する。</p>


	<a name="DB設計_pg_hbaconf"></a>
<h3 >pg_hba.conf<a href="#DB設計_pg_hbaconf" class="wiki-anchor">&para;</a></h3>


	<p>セキュリティを考慮してユーザ毎にセキュリティを設定する。</p>


<hr />


	<a name="DB設計_冗長化"></a>
<h1 >●冗長化<a href="#DB設計_冗長化" class="wiki-anchor">&para;</a></h1>


	<a name="DB設計_障害の分類と対応方法"></a>
<h2 >障害の分類と対応方法<a href="#DB設計_障害の分類と対応方法" class="wiki-anchor">&para;</a></h2>


	<p>冗長構成を検討する上で、まずは障害の内容とその対応方法を検討する。<br />2台のサーバ(node#1,2　#1をprimaryとする)でサービス(図の黄色線)、ハートビート(図の黒色線、ストレージレプリケーション用途にも利用し、2台のサーバを直接接続する)用のインターフェイスを持つ。</p>


	<p><img src="/attachments/download/845/db_system_fault_pattern.png" alt="" /></p>


	<a name="DB設計_障害のパターンと障害検知方法"></a>
<h3 >障害のパターンと障害検知方法<a href="#DB設計_障害のパターンと障害検知方法" class="wiki-anchor">&para;</a></h3>


	<p>障害に対してprimary, secondary間で異なる対応をすることは、<br />スプリットブレインを引き起こす要因の1つである。<br />そのため、障害パターンを整理し各ノードで検知方法を明確に定義することは重要である。<br />各ノードでの障害検知方法と障害パターンの判断基準を以下に示す。</p>


	<p>・node#1</p>


	<p>pattern 6についてはrouterと直接接続されている場合node#1のservice nwへのインターフェイスもダウンすることからVIPチェックがダウンする可能性もある。<br />上記の場合にpattern 1と6が判別不可能である。<br />その場合を考慮してpattern 6では1と同じ自動対策を実施する。<br />この結果pattern 6では両ノードがsecondary状態となり、routerの普及後手動対策が必要となってしまう。<br />両ノードをsecondary状態とすることの意味は状態を安定させることにあり、<br />これを行わないとprimary,secondaryのフラッピング(交互に切り替わる事)が起きてしまうことを防ぐ。<br />pattern 5と2,3同時は判別不可能であるが、何もしないため問題はない。<br />pattern 1,3同時はスプリットブレインを引き起こす可能性がある。<br />そのため状況に応じた対応が必要となる。<br />自身をprimaryとして認識している場合はサービスを停止させ(secondaryと認識させ)、現行primaryのnode#2のsecondaryとして動作させる。<br />自身をprimaryとして認識していない場合は通常の復旧を行う。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>障害pattern</th>
			<th>router ping</th>
			<th>VIP ping</th>
			<th>対向ノード service nw経由ping</th>
			<th>対向ノード heartbeat</th>
			<th>サービスへの影響</th>
			<th>障害への自動対策</th>
			<th>復旧後の手動対策</th>
		</tr>
		<tr>
			<td>正常</td>
			<td>○</td>
			<td>○</td>
			<td>○</td>
			<td>○</td>
			<td>無</td>
			<td>無</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 1</td>
			<td>×</td>
			<td>×</td>
			<td>×</td>
			<td>○</td>
			<td>有</td>
			<td>secondaryへ降格する</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 2</td>
			<td>○</td>
			<td>○</td>
			<td>×</td>
			<td>○</td>
			<td>無</td>
			<td>無</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 3</td>
			<td>○</td>
			<td>○</td>
			<td>○</td>
			<td>×</td>
			<td>無</td>
			<td>無</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 4</td>
			<td>-</td>
			<td>-</td>
			<td>-</td>
			<td>-</td>
			<td>有</td>
			<td>-</td>
			<td>ノードの復旧後secondaryとしてnode#2からデータ同期を行う</td>
		</tr>
		<tr>
			<td>pattern 5</td>
			<td>○</td>
			<td>○</td>
			<td>×</td>
			<td>×</td>
			<td>無</td>
			<td>無</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 6</td>
			<td>×</td>
			<td>○(直接接続だと×)</td>
			<td>×</td>
			<td>○</td>
			<td>甚大</td>
			<td>secondaryへ降格する</td>
			<td>routerの復旧後primaryへ昇格する</td>
		</tr>
		<tr>
			<td>pattern 1,3同時</td>
			<td>×</td>
			<td>○</td>
			<td>×</td>
			<td>×</td>
			<td>有</td>
			<td>secondaryへ降格する</td>
			<td>ケースバイケース</td>
		</tr>
		<tr>
			<td>pattern 2,3同時</td>
			<td>○</td>
			<td>○</td>
			<td>×</td>
			<td>×</td>
			<td>無</td>
			<td>無</td>
			<td>無</td>
		</tr>
	</table>




	<p>・node#2</p>


	<p>pattern 2と6が判別不可能であるが、何もしないため問題はない。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>障害pattern</th>
			<th>router ping</th>
			<th>VIP ping</th>
			<th>対向ノード service nw経由ping</th>
			<th>対向ノード heartbeat</th>
			<th>サービスへの影響</th>
			<th>障害への自動対策</th>
			<th>復旧後の手動対策</th>
		</tr>
		<tr>
			<td>正常</td>
			<td>○</td>
			<td>○</td>
			<td>○</td>
			<td>○</td>
			<td>無</td>
			<td>無</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 1</td>
			<td>○</td>
			<td>×</td>
			<td>×</td>
			<td>○</td>
			<td>有</td>
			<td>primaryへ昇格する</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 2</td>
			<td>×</td>
			<td>×</td>
			<td>×</td>
			<td>○</td>
			<td>無</td>
			<td>無</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 3</td>
			<td>○</td>
			<td>○</td>
			<td>○</td>
			<td>×</td>
			<td>無</td>
			<td>無</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 4</td>
			<td>○</td>
			<td>×</td>
			<td>×</td>
			<td>×</td>
			<td>有</td>
			<td>primaryへ昇格する</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 5</td>
			<td>-</td>
			<td>-</td>
			<td>-</td>
			<td>-</td>
			<td>無</td>
			<td>無</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 6</td>
			<td>×</td>
			<td>×</td>
			<td>×</td>
			<td>○</td>
			<td>甚大</td>
			<td>無</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 1,3同時</td>
			<td>○</td>
			<td>×</td>
			<td>×</td>
			<td>×</td>
			<td>有</td>
			<td>primaryへ昇格する</td>
			<td>無</td>
		</tr>
		<tr>
			<td>pattern 2,3同時</td>
			<td>×</td>
			<td>×</td>
			<td>×</td>
			<td>×</td>
			<td>無</td>
			<td>無</td>
			<td>無</td>
		</tr>
	</table>




	<p>障害への自動対策についてフェールオーバはシステムを複雑にすることから行わないものとする。<br />予防/回避策としてネットワーク障害についてはボンディングとリンクアグリゲーション、<br />ルータの障害についてはVRRPを用いたrouterの冗長化が挙げられる。</p>


	<p>また、上記の表を見ると「対向ノード service nw経由ping」は「障害への自動対策」へ影響を与えない。<br />そのため、「対向ノード service nw経由ping」はシステムに組み込む必要がないと言える。<br />「障害への自動対策」のトリガとなる要因をまとめると、primaryでは「router ping」、secondaryへは「router ping」および「VIP ping」である。</p>


	<a name="DB設計_pacemaker設定"></a>
<h2 >pacemaker設定<a href="#DB設計_pacemaker設定" class="wiki-anchor">&para;</a></h2>


	<p>pacemakerはcrmコマンドによるリソース設定を行う。<br />上記を踏まえ、以下の設定を入れる。</p>


<hr />


	<a name="DB設計_バックアップ"></a>
<h1 >●バックアップ<a href="#DB設計_バックアップ" class="wiki-anchor">&para;</a></h1>


	<a name="DB設計_バックアップ概要"></a>
<h2 >バックアップ概要<a href="#DB設計_バックアップ概要" class="wiki-anchor">&para;</a></h2>


	<p>DBサーバは冗長化を行っているものの、ヒューマンエラーによる対策は十分とはいえない。<br />そのため定期的なバックアップを取得し、いつでも状態を戻せる必要がある。<br />DBサーバにおいてバックアップ対象となるのはpostgresqlのデータバックアップである。<br />postgresqlにおいてバックアップ方法は以下の2通り存在する。</p>


	<ul>
	<li>ダンプコマンドによるダンプバックアップ</li>
		<li>ファイルシステムバックアップ(通称、ベースバックアップ)</li>
	</ul>


	<p>ダンプコマンドよるバックアップは単一のsqlファイル(もしくはpostgresql独自フォーマットファイル。リストアには専用コマンドが必要)を出力するため運用が容易である。<br />ベースバックアップ方式ではデータディレクトリをそのまま圧縮して利用するだけであるが、上記の方法に比べ運用の手順が増える。<br />運用コストを低下を優先しダンプコマンドによるsqlファイルバックアップを採用する。</p>


	<a name="DB設計_バックアップ設定"></a>
<h2 >バックアップ設定<a href="#DB設計_バックアップ設定" class="wiki-anchor">&para;</a></h2>


	<p>cronによるスケジューリングによりダンプとファイル圧縮を行い、nasサーバへ保管する。<br />毎日バックアップを取得するものとし、その際にファイル名に日時を付与することでリカバリポイントがわかるようにする。</p>


	<a name="DB設計_リカバリ"></a>
<h2 >リカバリ<a href="#DB設計_リカバリ" class="wiki-anchor">&para;</a></h2>


	<p>圧縮されたダンプファイルを解凍しsqlファイルの流し込みを行う。</p>
<hr />
<a name="Dns設計" />
<a name="Dns設計_Dns設計"></a>
<h1 >Dns設計<a href="#Dns設計_Dns設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#Dns設計_Dns設計">Dns設計</a></li><li><a href="#Dns設計_Introduction">●Introduction</a><ul><li><a href="#Dns設計_目的">目的</a></li></ul>
</li><li><a href="#Dns設計_構成">●構成</a><ul><li><a href="#Dns設計_サーバ構成">サーバ構成</a></li><li><a href="#Dns設計_パーティション構成">パーティション構成</a></li><li><a href="#Dns設計_導入パッケージ">導入パッケージ</a></li><li><a href="#Dns設計_システムサービス一覧">システムサービス一覧</a></li><li><a href="#Dns設計_ポートアサイン">ポートアサイン</a></li><li><a href="#Dns設計_ログ設定">ログ設定</a><ul><li><a href="#Dns設計_ログローテ">ログローテ</a></li></ul>
</li></ul>
</li><li><a href="#Dns設計_システム設計">●システム設計</a><ul><li><a href="#Dns設計_drbd設定">drbd設定</a><ul><li><a href="#Dns設計_drbdconf">drbd.conf</a></li><li><a href="#Dns設計_global_commonconf">global_common.conf</a></li><li><a href="#Dns設計_postgresres">postgres.res</a></li></ul>
</li><li><a href="#Dns設計_pacemaker設定">pacemaker設定</a><ul><li><a href="#Dns設計_corosyncconf">corosync.conf</a></li><li><a href="#Dns設計_pacemaker">pacemaker</a></li></ul>
</li><li><a href="#Dns設計_postgresql設定">postgresql設定</a><ul><li><a href="#Dns設計_カーネルパラメータの設定">カーネルパラメータの設定</a></li><li><a href="#Dns設計_postgresqlconf">postgresql.conf</a></li><li><a href="#Dns設計_pg_hbaconf">pg_hba.conf</a></li><li><a href="#Dns設計_データベースユーザ">データベース・ユーザ</a></li></ul>
</li><li><a href="#Dns設計_named設計">named設計</a><ul><li><a href="#Dns設計_bind-sdbのテーブル構造">bind-sdbのテーブル構造</a></li></ul>
</li></ul>
</li><li><a href="#Dns設計_冗長化">●冗長化</a></li><li><a href="#Dns設計_バックアップ">●バックアップ</a><ul><li><a href="#Dns設計_バックアップ概要">バックアップ概要</a></li><li><a href="#Dns設計_バックアップ設定">バックアップ設定</a></li><li><a href="#Dns設計_リストア">リストア</a></li></ul>
</li><li><a href="#Dns設計_運用手順">●運用手順</a><ul><li><a href="#Dns設計_レコードの登録削除">レコードの登録・削除</a></li><li><a href="#Dns設計_起動">起動</a></li><li><a href="#Dns設計_停止">停止</a></li><li><a href="#Dns設計_復旧">復旧</a></li></ul></li></ul>


<hr />


	<a name="Dns設計_Introduction"></a>
<h1 >●Introduction<a href="#Dns設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="Dns設計_目的"></a>
<h2 >目的<a href="#Dns設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>Iaasクラウド(以下、本システム)では事業者が外部公開するサービスのドメイン管理が必要となる。<br />本システムではドメインの動的登録が必要となるため、<br />外部DNSサービスを利用したドメイン管理のみではこの条件を達成することは難しい。<br />そのため、システム内部に動的に変更のあるドメイン配下全てを管理するDNSを設置する必要がある。<br />これに加え、近年のDNSへの脆弱性が多々問題となることから、<br />セキュリティについても十分検討したDNSの設計が必要となる。<br />これらを踏まえ、本書ではDNSの設計を定義する。</p>


<hr />


	<a name="Dns設計_構成"></a>
<h1 >●構成<a href="#Dns設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="Dns設計_サーバ構成"></a>
<h2 >サーバ構成<a href="#Dns設計_サーバ構成" class="wiki-anchor">&para;</a></h2>


	<p>DNSはクラコン上に構築する。<br />DNSのシステム構成とユースケース・通信のフローを以下に示す。</p>


	<p><img src="/attachments/download/1009/dns_flow.png" alt="" /></p>


	<p>上記からDNSが管理するレコード情報は以下となる。</p>


	<ul>
	<li>idc設備内部の正引き/逆引き</li>
		<li>事業者のインスタンスの正引き/逆引き</li>
		<li>idc設備外部公開部分の正引き/逆引き</li>
	</ul>


	<p>本システムのDNSでは動的な設定変更が必要になることから、<br />DBを用いたレコード管理を行う。</p>


	<a name="Dns設計_パーティション構成"></a>
<h2 >パーティション構成<a href="#Dns設計_パーティション構成" class="wiki-anchor">&para;</a></h2>


	<p>クラコンにはシステム領域以外のディスクが存在しないため、<br />以下のパーティションにDB領域を作成する。<br />また、このパーティションをDRBDで冗長化する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>ボリュームグループ名</th>
			<th>ロジカルボリューム名</th>
		</tr>
		<tr>
			<td>vg_system</td>
			<td>lv_postgres</td>
		</tr>
	</table>




	<p>上記で作成される/dev/vg_system/lv_postgresを/dev/drbd0としてレプリケーションをする。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>デバイスファイル</th>
			<th>マウントポイント</th>
			<th>LVM</th>
			<th>FSタイプ</th>
			<th>サイズ</th>
			<th>備考</th>
		</tr>
		<tr>
			<td>/dev/drbd0</td>
			<td>/var/lib/pgsql/9.4/data</td>
			<td>○</td>
			<td>ext4</td>
			<td>10GB</td>
			<td>postgresが利用するデータディレクトリ</td>
		</tr>
	</table>




	<a name="Dns設計_導入パッケージ"></a>
<h2 >導入パッケージ<a href="#Dns設計_導入パッケージ" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>パッケージ名</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>drbd84-utils.x86_64</td>
			<td>drbdadm等の管理コマンド</td>
		</tr>
		<tr>
			<td>kmod-drbd84.x86_64</td>
			<td>ストレージレプリケーションを行い、シェアードナッシングを実現する。drbd本体</td>
		</tr>
		<tr>
			<td>pacemaker-1.0.13-2.el6.x86_64</td>
			<td>リソース制御機能</td>
		</tr>
		<tr>
			<td>heartbeat-3.0.5-1.1.el6.x86_64</td>
			<td>クラスタ制御機能</td>
		</tr>
		<tr>
			<td>postgresql94-server.x86_64</td>
			<td>RDBMSの代表的なOSSであるpostgresql server</td>
		</tr>
		<tr>
			<td>postgresql94-libs-9.4.1</td>
			<td>postgresqlの開発関連ライブラリ</td>
		</tr>
		<tr>
			<td>postgresql94-contrib-9.4.1</td>
			<td>postgresqlの拡張機能</td>
		</tr>
		<tr>
			<td>postgresql94-9.4.1</td>
			<td>psqlコマンド</td>
		</tr>
		<tr>
			<td>bind</td>
			<td>DNSサーバ本体</td>
		</tr>
		<tr>
			<td>bind-sdb</td>
			<td>bindのDBアクセスを提供するパッケージ</td>
		</tr>
	</table>




	<a name="Dns設計_システムサービス一覧"></a>
<h2 >システムサービス一覧<a href="#Dns設計_システムサービス一覧" class="wiki-anchor">&para;</a></h2>


	<p>DNSは全てpacemakerにて管理するため、<br />関連するサービスの自動起動設定をoffにする。</p>


<pre>
corosync        0:off   1:off   2:off   3:off   4:off   5:off   6:off
drbd            0:off   1:off   2:off   3:off   4:off   5:off   6:off
pacemaker       0:off   1:off   2:off   3:off   4:off   5:off   6:off
postgresql-9.4  0:off   1:off   2:off   3:off   4:off   5:off   6:off
named           0:off   1:off   2:off   3:off   4:off   5:off   6:off
</pre>

	<a name="Dns設計_ポートアサイン"></a>
<h2 >ポートアサイン<a href="#Dns設計_ポートアサイン" class="wiki-anchor">&para;</a></h2>


	<p>DNSではハートビートパケットのため、<br />直接接続されたインターフェイスを利用する。<br />また、DBもそのインターフェイスを利用し、<br />VIPを設定してDNSの設定をクラコン#1,2間での際をなくす。<br />DNS自体は全てのインターフェイスからの接続を許可する。</p>


	<a name="Dns設計_ログ設定"></a>
<h2 >ログ設定<a href="#Dns設計_ログ設定" class="wiki-anchor">&para;</a></h2>


	<p>ログの出力先を以下に示す。</p>


<pre>
/var/log/named/named.log
/var/log/cluster/pacemaker.log
/var/log/cluster/corosync.log
/var/log/pgsql/postgresql-%Y%m%d.log # postgresql本体の機能で日付の付与、ローテーションを行う
</pre>

	<a name="Dns設計_ログローテ"></a>
<h3 >ログローテ<a href="#Dns設計_ログローテ" class="wiki-anchor">&para;</a></h3>


	<p>postgresのログに関してはpostgres.confの設定で行う。</p>


	<ul>
	<li>/var/log/named/named.log</li>
	</ul>


<pre>
/var/log/named/named.log {
    daily
    create 600 named named
    rotate 400
    ifempty
    missingok
    dateext
    compress
    postrotate
        /sbin/service named reload 2&gt; /dev/null &gt; /dev/null || true
    endscript
}
</pre>

	<ul>
	<li>/etc/logrotate.d/pacemaker</li>
	</ul>


<pre>
/var/log/cluster/pacemaker.log
/var/log/cluster/corosync.log {
    daily
    create 600 hacluster haclient
    rotate 400
    ifempty
    missingok
    dateext
    compress
    copytruncate
}
</pre>

<hr />


	<a name="Dns設計_システム設計"></a>
<h1 >●システム設計<a href="#Dns設計_システム設計" class="wiki-anchor">&para;</a></h1>


	<a name="Dns設計_drbd設定"></a>
<h2 >drbd設定<a href="#Dns設計_drbd設定" class="wiki-anchor">&para;</a></h2>


	<a name="Dns設計_drbdconf"></a>
<h3 >drbd.conf<a href="#Dns設計_drbdconf" class="wiki-anchor">&para;</a></h3>


	<p>デフォルトのまま利用する。</p>


	<ul>
	<li>/etc/drbd.conf</li>
	</ul>


<pre>
include "drbd.d/global_common.conf";
include "drbd.d/*.res";
</pre>

	<a name="Dns設計_global_commonconf"></a>
<h3 >global_common.conf<a href="#Dns設計_global_commonconf" class="wiki-anchor">&para;</a></h3>


	<p>drbdのグローバル設定を行う設定ファイル。</p>


	<ul>
	<li>/etc/drbd.d/global_common.conf</li>
	</ul>


<pre>
global {
  usage-count no;
}
common {
  # 完全同期のCプロトコルを設定
  protocol C;
  # 基本的にpacemakerにて管理させるためハンドラの設定は行わない
  handlers {
  }
  startup {
    # 以下の設定がないと片系のノードの起動を待ち続ける
    wfc-timeout 10;
    degr-wfc-timeout 10;
    outdated-wfc-timeout 10;
  }
  options {
  }
  disk {
  }
  net {
  }
}
</pre>

	<a name="Dns設計_postgresres"></a>
<h3 >postgres.res<a href="#Dns設計_postgresres" class="wiki-anchor">&para;</a></h3>


	<p>今回アプリケーション毎にストレージレプリケーションを行うため、<br />アプリケーション名に.resというファイル名を用意する。</p>


	<ul>
	<li>/etc/drbd.d/postgres.res</li>
	</ul>


<pre>
resource postgres {
  device /dev/drbd0;
  disk   /dev/vg_system/lv_postgres;
  meta-disk internal;
  on idc-occr01 {
    address 10.16.32.82:7788;
  }
  on idc-occr02 {
    address 10.16.32.83:7788;
  }
}
</pre>

	<a name="Dns設計_pacemaker設定"></a>
<h2 >pacemaker設定<a href="#Dns設計_pacemaker設定" class="wiki-anchor">&para;</a></h2>


	<a name="Dns設計_corosyncconf"></a>
<h3 >corosync.conf<a href="#Dns設計_corosyncconf" class="wiki-anchor">&para;</a></h3>


	<p>pacemakerのノード管理にはcorosyncを利用する。</p>


	<ul>
	<li>/etc/corosync/corosync.conf</li>
	</ul>


<pre>
compatibility: whitetank
service {
  name: pacemaker
  ver: 0
  use_mgmud: yes
}
totem {
  version: 2
  crypto_cipher: none
  crypto_hash: none
  secauth: off
  rrp_mode: none
  interface {
    ringnumber: 0
    bindnetaddr: 10.16.32.0
    mcastport: 5405
    ttl: 1
  }
  transport: udpu
}
logging {
  fileline: off
  to_stderr: no
  to_logfile: yes
  logfile: /var/log/cluster/corosync.log
  logfile_priority: info
  to_syslog: no
  debug: off
  timestamp: on
  logger_subsys {
    subsys: QUORUM
    debug: off
  }
}
nodelist {
  node {
    ring0_addr: 10.16.32.82
    nodeid: 1
  }
  node {
    ring0_addr: 10.16.32.83
    nodeid: 2
  }
}
quorum {
  provider: corosync_votequorum
  expected_votes: 2
}
</pre>

	<a name="Dns設計_pacemaker"></a>
<h3 >pacemaker<a href="#Dns設計_pacemaker" class="wiki-anchor">&para;</a></h3>


	<p>pacemakerでは管理するサービス等をリソースと呼ぶ。</p>


	<ul>
	<li>pacemakerリソース内容</li>
	</ul>


<pre>
property stonith-enabled="false" \
  no-quorum-policy="ignore" \
  crmd-transition-delay="2s" 
rsc_defaults resource-stickiness="INFINITY" \
  migration-threshold="1" 
primitive pr_ping2gw ocf:pacemaker:ping \
  params name="ping_to_gateway" host_list="10.16.33.254" multiplier="100" dampen="1" \
  op monitor interval="10s" timeout="60" \
  op start timeout="60" 
primitive pr_vip ocf:heartbeat:IPaddr2 \
  params ip=10.16.33.81 nic="bond0" cidr_netmask="24" \
  op monitor interval="10s" 
primitive pr_postgres_drbd ocf:linbit:drbd \
  params drbd_resource="postgres" \
  op start interval="0s" timeout="240s" \
  op stop interval="0s" timeout="100s" \
  op monitor interval="15s" timeout="60s" role="Master" \
  op monitor interval="30s" timeout="60s" role="Slave" 
primitive pr_fs_postgres_drbd ocf:heartbeat:Filesystem \
  params device="/dev/drbd0" directory="/var/lib/pgsql/9.4/data" fstype="ext4" \
  op start interval="0s" timeout="60s" \
  op stop interval="0s" timeout="60s" \
  op monitor interval="30s" timeout="40s" 
primitive pr_postgres ocf:heartbeat:pgsql \
  params pgctl="/usr/pgsql-9.4/bin/pg_ctl" \
  psql="/usr/pgsql-9.4/bin/psql" \
  pgdata="/var/lib/pgsql/9.4/data" \
  pghost="10.16.33.81" \
  op start interval="0s" timeout="120s" \
  op monitor interval="10s" timeout="60s" \
  op stop interval="0s" timeout="120s" 
ms ms_postgres_drbd pr_postgres_drbd \
  meta master-max="1" master-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true" \
  is-managed="true" target-role="Started" 
group grp_postgres pr_vip pr_fs_postgres_drbd pr_postgres
clone clo_ping2gw pr_ping2gw
location loc_ping grp_postgres \
  rule -inf: not_defined ping_to_gateway or ping_to_gateway lt 100
colocation col_postgres_on_drbd inf: grp_postgres ms_postgres_drbd:Master
order ord_postgres_after_drbd inf: ms_postgres_drbd:promote grp_postgres:start
</pre>

	<a name="Dns設計_postgresql設定"></a>
<h2 >postgresql設定<a href="#Dns設計_postgresql設定" class="wiki-anchor">&para;</a></h2>


	<a name="Dns設計_カーネルパラメータの設定"></a>
<h3 >カーネルパラメータの設定<a href="#Dns設計_カーネルパラメータの設定" class="wiki-anchor">&para;</a></h3>


	<p>postgresでは共有メモリを利用しており、<br />メモリを多く利用するオペレーションが発生した場合の性能に影響がある。<br />/etc/sysctl.confに以下の内容を追記する。<br /><a href="http://www.postgresql.jp/document/9.3/html/kernel-resources.html#SYSVIPC" class="external">参考</a></p>


<pre>
kernel.shmmax=17179869184 # 16GB以上であれば変更の必要なし
kernel.shmall=4194304
</pre>

	<a name="Dns設計_postgresqlconf"></a>
<h3 >postgresql.conf<a href="#Dns設計_postgresqlconf" class="wiki-anchor">&para;</a></h3>


	<p>zabbixのみがpostgresにアクセスするため基本的にはデフォルトの設定を利用する。<br />ログローテートの設定および、shared_buffesの設定を入れ込む。</p>


	<ul>
	<li>/var/lib/pgsql/9.4/data/postgresql.conf</li>
	</ul>


<pre>
listen_addresses = '10.16.33.81'
max_connections = 100
shared_buffers = 128MB # クラコンは他のサービスも多く稼働しているためメモリは低く設定する
dynamic_shared_memory_type = posix
log_destination = 'stderr'
logging_collector = on
log_directory = '/var/log/pgsql'
log_filename = 'postgresql-%Y%m%d.log'
log_truncate_on_rotation = on
log_rotation_age = 1d
log_rotation_size = 0
log_line_prefix = '[%m][%p][%u][%d] '
log_timezone = 'Japan'
datestyle = 'iso, ymd'
timezone = 'Japan'
lc_messages = 'ja_JP.UTF-8'
lc_monetary = 'ja_JP.UTF-8'
lc_numeric = 'ja_JP.UTF-8'
lc_time = 'ja_JP.UTF-8'
default_text_search_config = 'pg_catalog.simple'
</pre>

	<a name="Dns設計_pg_hbaconf"></a>
<h3 >pg_hba.conf<a href="#Dns設計_pg_hbaconf" class="wiki-anchor">&para;</a></h3>


	<p>DNSは直接接続のインターフェイスを利用するため<br />該当のインターフェイスおよびローカルからの接続をすべて許可する設定とする。</p>


	<ul>
	<li>/var/lib/pgsql/9.4/data/pg_hba.conf</li>
	</ul>


<pre>
local   all             all                                     trust
host    all             all             127.0.0.1/32            trust
host    all             all             10.16.33.0/24           trust
host    all             all             10.16.38.0/24           trust
host    all             all             10.16.39.0/24           trust
host    all             all             ::1/128                 trust
</pre>

	<a name="Dns設計_データベースユーザ"></a>
<h3 >データベース・ユーザ<a href="#Dns設計_データベースユーザ" class="wiki-anchor">&para;</a></h3>


	<p>以下のデータベースおよび、ユーザを作成する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>データベース名</th>
			<th>ユーザ名</th>
			<th>パスワード</th>
		</tr>
		<tr>
			<td>named</td>
			<td>named</td>
			<td>namednamed</td>
		</tr>
	</table>




	<a name="Dns設計_named設計"></a>
<h2 >named設計<a href="#Dns設計_named設計" class="wiki-anchor">&para;</a></h2>


	<p>セキュリティ等に関して以下を考慮して設定を行う。</p>


	<ul>
	<li>バージョン情報の非表示(「version none;」で可能)</li>
		<li>内部的な情報は問い合わせの範囲を制限する(DNS自体は公開する必要があるため、「ACL」や「allow-query」を内部zoneに設定して制限。もしくは「allow-recursion {"internal";};」を指定する)
	<ul>
	<li>internalにプライベートIPおよびループバックアドレスを指定する</li>
	</ul>
	</li>
		<li>ゾーン転送の禁止(今回、DBを利用している関係上ゾーン転送は不要である「allow-transfer {none;};」により全てを禁止とする)</li>
		<li>プロセスのユーザ指定(9系ではデフォルトでnamedユーザとなる)</li>
		<li>メッセージIDのランダマイズ(9系ではデフォルトでuse-id-poolが有効となる)</li>
		<li>DOS攻撃等で攻撃元がわかる場合の対処(blackhole {ipaddress/networkaddress;};)</li>
	</ul>


	<p>上記を踏まえ設定ファイルを以下に示す。</p>


	<ul>
	<li>/etc/named.conf</li>
	</ul>


	<p><a class="collapsible collapsed" href="#" id="collapse-a9ea375e-show" onclick="$(&#x27;#collapse-a9ea375e-show, #collapse-a9ea375e-hide&#x27;).toggle(); $(&#x27;#collapse-a9ea375e&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-a9ea375e-hide" onclick="$(&#x27;#collapse-a9ea375e-show, #collapse-a9ea375e-hide&#x27;).toggle(); $(&#x27;#collapse-a9ea375e&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-a9ea375e" style="display:none;"><pre>
### private ip and loopback ip
acl "acl_internal" { 10.16.33.0/24; 10.16.38.0/24; 10.16.39.0/24; 127.0.0.0/8; };
acl "acl_external" { 10.16.35.0/24; 10.16.37.0/24 };

options {
    ### バージョン情報の非表示
    version none;
    ### 外部公開するためanyを指定する。
    listen-on port 53 { any; };
    ### ipv6は利用しないので、とりあえずlocalhostのみを指定
    listen-on-v6 port 53 { ::1; };
    ### メッセージIDのランダマイズ（デフォルトで有効であるが明示的に指定する）
    use-id-pool yes;
    directory     "/var/named";
    dump-file     "/var/named/data/cache_dump.db";
    statistics-file "/var/named/data/named_stats.txt";
    memstatistics-file "/var/named/data/named_mem_stats.txt";
    ### allow-queryをinternalのみに制限すると管理するドメインの外部からの要求を拒否してしまう
    ### view毎に設定できるためグローバルとしてはセキュリティが高い方を選択する
    ### 上記の問題に対してallow-recursionは外部からの問い合わせについては非再帰的な要求として回答する
    allow-query     { "acl_internal"; "acl_external"; };
    allow-recursion { "acl_internal"; "acl_external"; };
    ### 攻撃元がわかる場合は以下に設定を入込み、拒否する
    blackhole {
    };
    ### 以下の設定がnoであるとそもそも再帰的な問い合わせを行わない view毎に設定できるためグローバルとしてはセキュリティが高い方を選択する
    ### 外部公開用のDNSであれば以下をnoに設定するだけでOK
    recursion no;
    ### 外部DNSサービスの登録(以下はgoogleのパブリックドメインであるが、外部DNSサービスとしては信頼性は高いかもしれない)
    forwarders {
        163.139.230.168;
        163.139.21.197;
    };
    ### forwardersで指定した上位DNSへ名前解決が失敗した場合にルートDNSへのリクエストを行わず、そのまま失敗を返却する
    forward only;
    ### ゾーン情報の転送禁止
    allow-transfer { none; };
    ### 未検討
    dnssec-enable yes;
    dnssec-validation yes;
    dnssec-lookaside auto;
    /* Path to ISC DLV key */
    bindkeys-file "/etc/named.iscdlv.key";
    managed-keys-directory "/var/named/dynamic";
};

logging {
    channel default_debug {
        file "/var/log/named/named.log";
        severity dynamic;
        print-time yes;
        print-severity yes;
        print-category yes;
    };
    #ログの出力方法をlog_defaultとして定義
    channel "log_default" {
        file "/var/log/named/named.log";
        severity info;
        print-time yes;
        print-severity yes;
        print-category yes;
    };
    channel "log_security" {
        file "/var/log/named/security.log";
        severity info;
        print-time yes;
        print-severity yes;
        print-category yes;
    };
    channel "log_queries" {
        file "/var/log/named/queries.log";
        severity info;
        print-time yes;
        print-severity yes;
        print-category yes;
    };
    category default { "log_default"; };
    category security { "log_security"; };
    category client { "log_security"; };
    ### "rndc querylog" コマンドでquerylogの有効無効を切り替えることができる
    ### "rndc status" コマンドで状況の確認ができる
    category queries { "log_queries"; };
};

include "/etc/named.root.key";

### 内部用
view "internal" {
    ### 以下に一致するクライアントのみがviewに設定されたものを参照可能
    match-clients { "acl_internal"; };
    ### 内部情報に関しては再帰問い合わせを無条件で許可する
    recursion yes;
    ### 内部のアドレスはhostsファイルで管理するが逆引きできないため登録する
    zone "idc.local" {
        type master;
        ### databaseのテーブル名はzone名の[.]や[-]を[_]に変換した名前とする
        database "pgsql named idc_local 10.16.33.81 named namednamed";
    };
    ### 内部/外部逆引き用
    zone "10.in-addr.arpa" {
        type master;
        ### 逆引き用のテーブル名はreverseを接頭語としてつける
        database "pgsql named reverse_10_in_addr_arpa 10.16.33.81 named namednamed";
    };
    ### テナント管理用
    zone "tenant.local" {
        type master;
        database "pgsql named tenant_local 10.16.33.81 named namednamed";
    };
    ### hintファイルはデフォルトを使用する
    zone "." IN {
        type hint;
        file "named.ca";
    };
};
### 外部用
view "external" {
    ### 以下に一致するクライアントのみがviewに設定されたものを参照可能
    match-clients { "acl_external"; };
    ### 内部情報に関しては再帰問い合わせを無条件で許可する
    recursion yes;
    ### テナント管理用
    zone "tenant.local" {
        type master;
        database "pgsql named tenant_local 10.16.33.81 named namednamed";
    };
    ### テナントは逆引きさせない
    ### 逆引きさせる場合、テーブルをかなり細かく分割する必要があり、
    ### 管理効率が悪くなる
    ### hintファイルはデフォルトを使用する
    zone "." IN {
        type hint;
        file "named.ca";
    };
};
### その他拒否
view "deny" {
    ### 外部公開するためクライアント制限を設けない
    match-clients { any; };
    ### 外部からの再帰問い合わせは行わない
    recursion no;
};
</pre></div></p>


	<ul>
	<li>/etc/sysconfig/named</li>
	</ul>


<pre>
ENABLE_SDB=yes
</pre>

	<a name="Dns設計_bind-sdbのテーブル構造"></a>
<h3 >bind-sdbのテーブル構造<a href="#Dns設計_bind-sdbのテーブル構造" class="wiki-anchor">&para;</a></h3>


	<p>今回のbind-sdbを利用している構成ではzoneファイルはDBのテーブルを意味する。<br />以下にテーブルの初期設定データを示す。</p>


	<p><a class="collapsible collapsed" href="#" id="collapse-833d99d2-show" onclick="$(&#x27;#collapse-833d99d2-show, #collapse-833d99d2-hide&#x27;).toggle(); $(&#x27;#collapse-833d99d2&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-833d99d2-hide" onclick="$(&#x27;#collapse-833d99d2-show, #collapse-833d99d2-hide&#x27;).toggle(); $(&#x27;#collapse-833d99d2&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-833d99d2" style="display:none;"><pre>
--
-- PostgreSQL database dump
--

SET statement_timeout = 0;
SET lock_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SET check_function_bodies = false;
SET client_min_messages = warning;

--
-- Name: plpgsql; Type: EXTENSION; Schema: -; Owner: 
--

CREATE EXTENSION IF NOT EXISTS plpgsql WITH SCHEMA pg_catalog;

--
-- Name: EXTENSION plpgsql; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION plpgsql IS 'PL/pgSQL procedural language';

SET search_path = public, pg_catalog;

SET default_tablespace = '';

SET default_with_oids = false;

--
-- Name: idc_local; Type: TABLE; Schema: public; Owner: named; Tablespace: 
--

CREATE TABLE idc_local (
    name character varying(255) DEFAULT NULL::character varying,
    ttl integer,
    rdtype character varying(255) DEFAULT NULL::character varying,
    rdata character varying(255) DEFAULT NULL::character varying
);

ALTER TABLE idc_local OWNER TO named;

--
-- Name: reverse_10_in_addr_arpa; Type: TABLE; Schema: public; Owner: named; Tablespace: 
--

CREATE TABLE reverse_10_in_addr_arpa (
    name character varying(255) DEFAULT NULL::character varying,
    ttl integer,
    rdtype character varying(255) DEFAULT NULL::character varying,
    rdata character varying(255) DEFAULT NULL::character varying
);

ALTER TABLE reverse_10_in_addr_arpa OWNER TO named;

--
-- Name: tenant_local; Type: TABLE; Schema: public; Owner: named; Tablespace: 
--

CREATE TABLE tenant_local (
    name character varying(255) DEFAULT NULL::character varying,
    ttl integer,
    rdtype character varying(255) DEFAULT NULL::character varying,
    rdata character varying(255) DEFAULT NULL::character varying
);

ALTER TABLE tenant_local OWNER TO named;

--
-- Data for Name: idc_local; Type: TABLE DATA; Schema: public; Owner: named
--

COPY idc_local (name, ttl, rdtype, rdata) FROM stdin;
idc.local       86400   SOA     ns1.idc.local. root.idc.local. 2014120101 3H 2M 1W 1D
idc.local       86400   NS      ns1.idc.local.
idc.local       86400   NS      ns2.idc.local.
ns1.idc.local    86400    A    10.16.33.82
ns2.idc.local    86400    A    10.16.33.83
idc-opfw01.idc.local    86400    A    10.16.33.50
idc-opfw02.idc.local    86400    A    10.16.33.51
idc-oprt01.idc.local    86400    A    10.16.17.2
idc-oprt02.idc.local    86400    A    10.16.17.3
idc-ol3s0a.idc.local    86400    A    10.16.17.17
idc-ol2s0a.idc.local    86400    A    10.16.17.33
idc-ol2s0b.idc.local    86400    A    10.16.17.34
idc-ol2s05.idc.local    86400    A    10.16.17.35
idc-ol2s0c.idc.local    86400    A    10.16.33.36
idc-ol2s08.idc.local    86400    A    10.16.33.37
idc-ol2s09.idc.local    86400    A    10.16.33.38
idc-olog01.idc.local    86400    A    10.16.17.69
idc-ontp01.idc.local    86400    A    10.16.17.73
idc-oist0a.idc.local    86400    A    10.16.17.129
idc-oist01.idc.local    86400    A    10.16.17.130
idc-oist02.idc.local    86400    A    10.16.17.131
idc-occr0a.idc.local    86400    A    10.16.33.81
idc-occr01.idc.local    86400    A    10.16.33.82
idc-occr02.idc.local    86400    A    10.16.33.83
idc-ohyp01.idc.local    86400    A    10.16.33.98
idc-ohyp02.idc.local    86400    A    10.16.33.99
idc-ohyp03.idc.local    86400    A    10.16.33.100
idc-ohyp04.idc.local    86400    A    10.16.33.101
idc-ohyp05.idc.local    86400    A    10.16.33.102
idc-ohyp06.idc.local    86400    A    10.16.33.103
idc-ohyp07.idc.local    86400    A    10.16.33.104
idc-omon0a.idc.local    86400    A    10.16.33.145
idc-omon01.idc.local    86400    A    10.16.33.146
idc-omon02.idc.local    86400    A    10.16.33.147
idc-onas01.idc.local    86400    A    10.16.33.194
idc-occr01i.idc.local    86400    A    10.16.34.82
idc-occr02i.idc.local    86400    A    10.16.34.83
idc-ohyp01i.idc.local    86400    A    10.16.34.98
idc-ohyp02i.idc.local    86400    A    10.16.34.99
idc-ohyp03i.idc.local    86400    A    10.16.34.100
idc-ohyp04i.idc.local    86400    A    10.16.34.101
idc-ohyp05i.idc.local    86400    A    10.16.34.102
idc-ohyp06i.idc.local    86400    A    10.16.34.103
idc-ohyp07i.idc.local    86400    A    10.16.34.104
idc-omon01i.idc.local    86400    A    10.16.34.146
idc-omon02i.idc.local    86400    A    10.16.34.147
idc-onas01i.idc.local    86400    A    10.16.34.194
idc-occr01v.idc.local    86400    A    10.16.35.82
idc-occr02v.idc.local    86400    A    10.16.35.83
idc-oist0a.idc.local    86400    A    10.16.24.129
idc-oist01.idc.local    86400    A    10.16.24.130
idc-oist02.idc.local    86400    A    10.16.24.131
idc-oist0a.idc.local    86400    A    10.16.25.129
idc-oist01.idc.local    86400    A    10.16.25.130
idc-oist02.idc.local    86400    A    10.16.25.131
idc-ohyp01s1.idc.local    86400    A    10.16.41.98
idc-ohyp02s1.idc.local    86400    A    10.16.41.99
idc-ohyp03s1.idc.local    86400    A    10.16.41.100
idc-ohyp04s1.idc.local    86400    A    10.16.41.101
idc-ohyp05s1.idc.local    86400    A    10.16.41.102
idc-ohyp06s1.idc.local    86400    A    10.16.41.103
idc-ohyp07s1.idc.local    86400    A    10.16.41.104
idc-ohyp01s1.idc.local    86400    A    10.16.42.98
idc-ohyp02s1.idc.local    86400    A    10.16.42.99
idc-ohyp03s1.idc.local    86400    A    10.16.42.100
idc-ohyp04s1.idc.local    86400    A    10.16.42.101
idc-ohyp05s1.idc.local    86400    A    10.16.42.102
idc-ohyp06s1.idc.local    86400    A    10.16.42.103
idc-ohyp07s1.idc.local    86400    A    10.16.42.104
idc-occr01h.idc.local    86400    A    10.16.32.82
idc-occr02h.idc.local    86400    A    10.16.32.83
idc-omon01h.idc.local    86400    A    10.16.32.146
idc-omon02h.idc.local    86400    A    10.16.32.147
idc-ovrt001.idc.local 86400 A 10.16.38.1
idc-ovrt002.idc.local 86400 A 10.16.38.2
idc-ovrt003.idc.local 86400 A 10.16.38.3
idc-ovrt004.idc.local 86400 A 10.16.38.4
idc-ovrt005.idc.local 86400 A 10.16.38.5
idc-ovrt006.idc.local 86400 A 10.16.38.6
idc-ovrt007.idc.local 86400 A 10.16.38.7
idc-ovrt008.idc.local 86400 A 10.16.38.8
idc-ovrt009.idc.local 86400 A 10.16.38.9
idc-ovrt010.idc.local 86400 A 10.16.38.10
idc-ovrt011.idc.local 86400 A 10.16.38.11
idc-ovrt012.idc.local 86400 A 10.16.38.12
idc-ovrt013.idc.local 86400 A 10.16.38.13
idc-ovrt014.idc.local 86400 A 10.16.38.14
idc-ovrt015.idc.local 86400 A 10.16.38.15
idc-ovrt016.idc.local 86400 A 10.16.38.16
idc-ovrt017.idc.local 86400 A 10.16.38.17
idc-ovrt018.idc.local 86400 A 10.16.38.18
idc-ovrt019.idc.local 86400 A 10.16.38.19
idc-ovrt020.idc.local 86400 A 10.16.38.20
idc-ovrt021.idc.local 86400 A 10.16.38.21
idc-ovrt022.idc.local 86400 A 10.16.38.22
idc-ovrt023.idc.local 86400 A 10.16.38.23
idc-ovrt024.idc.local 86400 A 10.16.38.24
idc-ovrt025.idc.local 86400 A 10.16.38.25
idc-ovrt026.idc.local 86400 A 10.16.38.26
idc-ovrt027.idc.local 86400 A 10.16.38.27
idc-ovrt028.idc.local 86400 A 10.16.38.28
idc-ovrt029.idc.local 86400 A 10.16.38.29
idc-ovrt030.idc.local 86400 A 10.16.38.30
idc-ovrt031.idc.local 86400 A 10.16.38.31
idc-ovrt032.idc.local 86400 A 10.16.38.32
idc-ovrt033.idc.local 86400 A 10.16.38.33
idc-ovrt034.idc.local 86400 A 10.16.38.34
idc-ovrt035.idc.local 86400 A 10.16.38.35
idc-ovrt036.idc.local 86400 A 10.16.38.36
idc-ovrt037.idc.local 86400 A 10.16.38.37
idc-ovrt038.idc.local 86400 A 10.16.38.38
idc-ovrt039.idc.local 86400 A 10.16.38.39
idc-ovrt040.idc.local 86400 A 10.16.38.40
idc-ovrt041.idc.local 86400 A 10.16.38.41
idc-ovrt042.idc.local 86400 A 10.16.38.42
idc-ovrt043.idc.local 86400 A 10.16.38.43
idc-ovrt044.idc.local 86400 A 10.16.38.44
idc-ovrt045.idc.local 86400 A 10.16.38.45
idc-ovrt046.idc.local 86400 A 10.16.38.46
idc-ovrt047.idc.local 86400 A 10.16.38.47
idc-ovrt048.idc.local 86400 A 10.16.38.48
idc-ovrt049.idc.local 86400 A 10.16.38.49
idc-ovrt050.idc.local 86400 A 10.16.38.50
idc-ovrt051.idc.local 86400 A 10.16.38.51
idc-ovrt052.idc.local 86400 A 10.16.38.52
idc-ovrt053.idc.local 86400 A 10.16.38.53
idc-ovrt054.idc.local 86400 A 10.16.38.54
idc-ovrt055.idc.local 86400 A 10.16.38.55
idc-ovrt056.idc.local 86400 A 10.16.38.56
idc-ovrt057.idc.local 86400 A 10.16.38.57
idc-ovrt058.idc.local 86400 A 10.16.38.58
idc-ovrt059.idc.local 86400 A 10.16.38.59
idc-ovrt060.idc.local 86400 A 10.16.38.60
idc-ovrt061.idc.local 86400 A 10.16.38.61
idc-ovrt062.idc.local 86400 A 10.16.38.62
idc-ovrt063.idc.local 86400 A 10.16.38.63
idc-ovrt064.idc.local 86400 A 10.16.38.64
idc-ovrt065.idc.local 86400 A 10.16.38.65
idc-ovrt066.idc.local 86400 A 10.16.38.66
idc-ovrt067.idc.local 86400 A 10.16.38.67
idc-ovrt068.idc.local 86400 A 10.16.38.68
idc-ovrt069.idc.local 86400 A 10.16.38.69
idc-ovrt070.idc.local 86400 A 10.16.38.70
idc-ovrt071.idc.local 86400 A 10.16.38.71
idc-ovrt072.idc.local 86400 A 10.16.38.72
idc-ovrt073.idc.local 86400 A 10.16.38.73
idc-ovrt074.idc.local 86400 A 10.16.38.74
idc-ovrt075.idc.local 86400 A 10.16.38.75
idc-ovrt076.idc.local 86400 A 10.16.38.76
idc-ovrt077.idc.local 86400 A 10.16.38.77
idc-ovrt078.idc.local 86400 A 10.16.38.78
idc-ovrt079.idc.local 86400 A 10.16.38.79
idc-ovrt080.idc.local 86400 A 10.16.38.80
idc-ovrt081.idc.local 86400 A 10.16.38.81
idc-ovrt082.idc.local 86400 A 10.16.38.82
idc-ovrt083.idc.local 86400 A 10.16.38.83
idc-ovrt084.idc.local 86400 A 10.16.38.84
idc-ovrt085.idc.local 86400 A 10.16.38.85
idc-ovrt086.idc.local 86400 A 10.16.38.86
idc-ovrt087.idc.local 86400 A 10.16.38.87
idc-ovrt088.idc.local 86400 A 10.16.38.88
idc-ovrt089.idc.local 86400 A 10.16.38.89
idc-ovrt090.idc.local 86400 A 10.16.38.90
idc-ovrt091.idc.local 86400 A 10.16.38.91
idc-ovrt092.idc.local 86400 A 10.16.38.92
idc-ovrt093.idc.local 86400 A 10.16.38.93
idc-ovrt094.idc.local 86400 A 10.16.38.94
idc-ovrt095.idc.local 86400 A 10.16.38.95
idc-ovrt096.idc.local 86400 A 10.16.38.96
idc-ovrt097.idc.local 86400 A 10.16.38.97
idc-ovrt098.idc.local 86400 A 10.16.38.98
idc-ovrt099.idc.local 86400 A 10.16.38.99
idc-ovrt100.idc.local 86400 A 10.16.38.100
idc-ovrt101.idc.local 86400 A 10.16.38.101
idc-ovrt102.idc.local 86400 A 10.16.38.102
idc-ovrt103.idc.local 86400 A 10.16.38.103
idc-ovrt104.idc.local 86400 A 10.16.38.104
idc-ovrt105.idc.local 86400 A 10.16.38.105
idc-ovrt106.idc.local 86400 A 10.16.38.106
idc-ovrt107.idc.local 86400 A 10.16.38.107
idc-ovrt108.idc.local 86400 A 10.16.38.108
idc-ovrt109.idc.local 86400 A 10.16.38.109
idc-ovrt110.idc.local 86400 A 10.16.38.110
idc-ovrt111.idc.local 86400 A 10.16.38.111
idc-ovrt112.idc.local 86400 A 10.16.38.112
idc-ovrt113.idc.local 86400 A 10.16.38.113
idc-ovrt114.idc.local 86400 A 10.16.38.114
idc-ovrt115.idc.local 86400 A 10.16.38.115
idc-ovrt116.idc.local 86400 A 10.16.38.116
idc-ovrt117.idc.local 86400 A 10.16.38.117
idc-ovrt118.idc.local 86400 A 10.16.38.118
idc-ovrt119.idc.local 86400 A 10.16.38.119
idc-ovrt120.idc.local 86400 A 10.16.38.120
idc-ovrt121.idc.local 86400 A 10.16.38.121
idc-ovrt122.idc.local 86400 A 10.16.38.122
idc-ovrt123.idc.local 86400 A 10.16.38.123
idc-ovrt124.idc.local 86400 A 10.16.38.124
idc-ovrt125.idc.local 86400 A 10.16.38.125
idc-ovrt126.idc.local 86400 A 10.16.38.126
idc-ovrt127.idc.local 86400 A 10.16.38.127
idc-ovrt128.idc.local 86400 A 10.16.38.128
idc-ovrt129.idc.local 86400 A 10.16.38.129
idc-ovrt130.idc.local 86400 A 10.16.38.130
idc-ovrt131.idc.local 86400 A 10.16.38.131
idc-ovrt132.idc.local 86400 A 10.16.38.132
idc-ovrt133.idc.local 86400 A 10.16.38.133
idc-ovrt134.idc.local 86400 A 10.16.38.134
idc-ovrt135.idc.local 86400 A 10.16.38.135
idc-ovrt136.idc.local 86400 A 10.16.38.136
idc-ovrt137.idc.local 86400 A 10.16.38.137
idc-ovrt138.idc.local 86400 A 10.16.38.138
idc-ovrt139.idc.local 86400 A 10.16.38.139
idc-ovrt140.idc.local 86400 A 10.16.38.140
idc-ovrt141.idc.local 86400 A 10.16.38.141
idc-ovrt142.idc.local 86400 A 10.16.38.142
idc-ovrt143.idc.local 86400 A 10.16.38.143
idc-ovrt144.idc.local 86400 A 10.16.38.144
idc-ovrt145.idc.local 86400 A 10.16.38.145
idc-ovrt146.idc.local 86400 A 10.16.38.146
idc-ovrt147.idc.local 86400 A 10.16.38.147
idc-ovrt148.idc.local 86400 A 10.16.38.148
idc-ovrt149.idc.local 86400 A 10.16.38.149
idc-ovrt150.idc.local 86400 A 10.16.38.150
idc-ovrt151.idc.local 86400 A 10.16.38.151
idc-ovrt152.idc.local 86400 A 10.16.38.152
idc-ovrt153.idc.local 86400 A 10.16.38.153
idc-ovrt154.idc.local 86400 A 10.16.38.154
idc-ovrt155.idc.local 86400 A 10.16.38.155
idc-ovrt156.idc.local 86400 A 10.16.38.156
idc-ovrt157.idc.local 86400 A 10.16.38.157
idc-ovrt158.idc.local 86400 A 10.16.38.158
idc-ovrt159.idc.local 86400 A 10.16.38.159
idc-ovrt160.idc.local 86400 A 10.16.38.160
idc-ovrt161.idc.local 86400 A 10.16.38.161
idc-ovrt162.idc.local 86400 A 10.16.38.162
idc-ovrt163.idc.local 86400 A 10.16.38.163
idc-ovrt164.idc.local 86400 A 10.16.38.164
idc-ovrt165.idc.local 86400 A 10.16.38.165
idc-ovrt166.idc.local 86400 A 10.16.38.166
idc-ovrt167.idc.local 86400 A 10.16.38.167
idc-ovrt168.idc.local 86400 A 10.16.38.168
idc-ovrt169.idc.local 86400 A 10.16.38.169
idc-ovrt170.idc.local 86400 A 10.16.38.170
idc-ovrt171.idc.local 86400 A 10.16.38.171
idc-ovrt172.idc.local 86400 A 10.16.38.172
idc-ovrt173.idc.local 86400 A 10.16.38.173
idc-ovrt174.idc.local 86400 A 10.16.38.174
idc-ovrt175.idc.local 86400 A 10.16.38.175
idc-ovrt176.idc.local 86400 A 10.16.38.176
idc-ovrt177.idc.local 86400 A 10.16.38.177
idc-ovrt178.idc.local 86400 A 10.16.38.178
idc-ovrt179.idc.local 86400 A 10.16.38.179
idc-ovrt180.idc.local 86400 A 10.16.38.180
idc-ovrt181.idc.local 86400 A 10.16.38.181
idc-ovrt182.idc.local 86400 A 10.16.38.182
idc-ovrt183.idc.local 86400 A 10.16.38.183
idc-ovrt184.idc.local 86400 A 10.16.38.184
idc-ovrt185.idc.local 86400 A 10.16.38.185
idc-ovrt186.idc.local 86400 A 10.16.38.186
idc-ovrt187.idc.local 86400 A 10.16.38.187
idc-ovrt188.idc.local 86400 A 10.16.38.188
idc-ovrt189.idc.local 86400 A 10.16.38.189
idc-ovrt190.idc.local 86400 A 10.16.38.190
idc-ovrt191.idc.local 86400 A 10.16.38.191
idc-ovrt192.idc.local 86400 A 10.16.38.192
idc-ovrt193.idc.local 86400 A 10.16.38.193
idc-ovrt194.idc.local 86400 A 10.16.38.194
idc-ovrt195.idc.local 86400 A 10.16.38.195
idc-ovrt196.idc.local 86400 A 10.16.38.196
idc-ovrt197.idc.local 86400 A 10.16.38.197
idc-ovrt198.idc.local 86400 A 10.16.38.198
idc-ovrt199.idc.local 86400 A 10.16.38.199
idc-ovrt200.idc.local 86400 A 10.16.38.200
idc-ovrt201.idc.local 86400 A 10.16.38.201
idc-ovrt202.idc.local 86400 A 10.16.38.202
idc-ovrt203.idc.local 86400 A 10.16.38.203
idc-ovrt204.idc.local 86400 A 10.16.38.204
idc-ovrt205.idc.local 86400 A 10.16.38.205
idc-ovrt206.idc.local 86400 A 10.16.38.206
idc-ovrt207.idc.local 86400 A 10.16.38.207
idc-ovrt208.idc.local 86400 A 10.16.38.208
idc-ovrt209.idc.local 86400 A 10.16.38.209
idc-ovrt210.idc.local 86400 A 10.16.38.210
idc-ovrt211.idc.local 86400 A 10.16.38.211
idc-ovrt212.idc.local 86400 A 10.16.38.212
idc-ovrt213.idc.local 86400 A 10.16.38.213
idc-ovrt214.idc.local 86400 A 10.16.38.214
idc-ovrt215.idc.local 86400 A 10.16.38.215
idc-ovrt216.idc.local 86400 A 10.16.38.216
idc-ovrt217.idc.local 86400 A 10.16.38.217
idc-ovrt218.idc.local 86400 A 10.16.38.218
idc-ovrt219.idc.local 86400 A 10.16.38.219
idc-ovrt220.idc.local 86400 A 10.16.38.220
idc-ovrt221.idc.local 86400 A 10.16.38.221
idc-ovrt222.idc.local 86400 A 10.16.38.222
idc-ovrt223.idc.local 86400 A 10.16.38.223
idc-ovrt224.idc.local 86400 A 10.16.38.224
idc-ovrt225.idc.local 86400 A 10.16.38.225
idc-ovrt226.idc.local 86400 A 10.16.38.226
idc-ovrt227.idc.local 86400 A 10.16.38.227
idc-ovrt228.idc.local 86400 A 10.16.38.228
idc-ovrt229.idc.local 86400 A 10.16.38.229
idc-ovrt230.idc.local 86400 A 10.16.38.230
idc-ovrt231.idc.local 86400 A 10.16.38.231
idc-ovrt232.idc.local 86400 A 10.16.38.232
idc-ovrt233.idc.local 86400 A 10.16.38.233
idc-ovrt234.idc.local 86400 A 10.16.38.234
idc-ovrt235.idc.local 86400 A 10.16.38.235
idc-ovrt236.idc.local 86400 A 10.16.38.236
idc-ovrt237.idc.local 86400 A 10.16.38.237
idc-ovrt238.idc.local 86400 A 10.16.38.238
idc-ovrt239.idc.local 86400 A 10.16.38.239
idc-ovrt240.idc.local 86400 A 10.16.38.240
idc-ovrt241.idc.local 86400 A 10.16.38.241
idc-ovrt242.idc.local 86400 A 10.16.38.242
idc-ovrt243.idc.local 86400 A 10.16.38.243
idc-ovrt244.idc.local 86400 A 10.16.38.244
idc-ovrt245.idc.local 86400 A 10.16.38.245
idc-ovrt246.idc.local 86400 A 10.16.38.246
idc-ovrt247.idc.local 86400 A 10.16.38.247
idc-ovrt248.idc.local 86400 A 10.16.38.248
idc-ovrt249.idc.local 86400 A 10.16.38.249
idc-ovrt250.idc.local 86400 A 10.16.38.250
idc-ovrt251.idc.local 86400 A 10.16.38.251
idc-ovrt252.idc.local 86400 A 10.16.38.252
idc-ovrt253.idc.local 86400 A 10.16.38.253
idc-ompx001.idc.local 86400 A 10.16.39.1
idc-ompx002.idc.local 86400 A 10.16.39.2
idc-ompx003.idc.local 86400 A 10.16.39.3
idc-ompx004.idc.local 86400 A 10.16.39.4
idc-ompx005.idc.local 86400 A 10.16.39.5
idc-ompx006.idc.local 86400 A 10.16.39.6
idc-ompx007.idc.local 86400 A 10.16.39.7
idc-ompx008.idc.local 86400 A 10.16.39.8
idc-ompx009.idc.local 86400 A 10.16.39.9
idc-ompx010.idc.local 86400 A 10.16.39.10
idc-ompx011.idc.local 86400 A 10.16.39.11
idc-ompx012.idc.local 86400 A 10.16.39.12
idc-ompx013.idc.local 86400 A 10.16.39.13
idc-ompx014.idc.local 86400 A 10.16.39.14
idc-ompx015.idc.local 86400 A 10.16.39.15
idc-ompx016.idc.local 86400 A 10.16.39.16
idc-ompx017.idc.local 86400 A 10.16.39.17
idc-ompx018.idc.local 86400 A 10.16.39.18
idc-ompx019.idc.local 86400 A 10.16.39.19
idc-ompx020.idc.local 86400 A 10.16.39.20
idc-ompx021.idc.local 86400 A 10.16.39.21
idc-ompx022.idc.local 86400 A 10.16.39.22
idc-ompx023.idc.local 86400 A 10.16.39.23
idc-ompx024.idc.local 86400 A 10.16.39.24
idc-ompx025.idc.local 86400 A 10.16.39.25
idc-ompx026.idc.local 86400 A 10.16.39.26
idc-ompx027.idc.local 86400 A 10.16.39.27
idc-ompx028.idc.local 86400 A 10.16.39.28
idc-ompx029.idc.local 86400 A 10.16.39.29
idc-ompx030.idc.local 86400 A 10.16.39.30
idc-ompx031.idc.local 86400 A 10.16.39.31
idc-ompx032.idc.local 86400 A 10.16.39.32
idc-ompx033.idc.local 86400 A 10.16.39.33
idc-ompx034.idc.local 86400 A 10.16.39.34
idc-ompx035.idc.local 86400 A 10.16.39.35
idc-ompx036.idc.local 86400 A 10.16.39.36
idc-ompx037.idc.local 86400 A 10.16.39.37
idc-ompx038.idc.local 86400 A 10.16.39.38
idc-ompx039.idc.local 86400 A 10.16.39.39
idc-ompx040.idc.local 86400 A 10.16.39.40
idc-ompx041.idc.local 86400 A 10.16.39.41
idc-ompx042.idc.local 86400 A 10.16.39.42
idc-ompx043.idc.local 86400 A 10.16.39.43
idc-ompx044.idc.local 86400 A 10.16.39.44
idc-ompx045.idc.local 86400 A 10.16.39.45
idc-ompx046.idc.local 86400 A 10.16.39.46
idc-ompx047.idc.local 86400 A 10.16.39.47
idc-ompx048.idc.local 86400 A 10.16.39.48
idc-ompx049.idc.local 86400 A 10.16.39.49
idc-ompx050.idc.local 86400 A 10.16.39.50
idc-ompx051.idc.local 86400 A 10.16.39.51
idc-ompx052.idc.local 86400 A 10.16.39.52
idc-ompx053.idc.local 86400 A 10.16.39.53
idc-ompx054.idc.local 86400 A 10.16.39.54
idc-ompx055.idc.local 86400 A 10.16.39.55
idc-ompx056.idc.local 86400 A 10.16.39.56
idc-ompx057.idc.local 86400 A 10.16.39.57
idc-ompx058.idc.local 86400 A 10.16.39.58
idc-ompx059.idc.local 86400 A 10.16.39.59
idc-ompx060.idc.local 86400 A 10.16.39.60
idc-ompx061.idc.local 86400 A 10.16.39.61
idc-ompx062.idc.local 86400 A 10.16.39.62
idc-ompx063.idc.local 86400 A 10.16.39.63
idc-ompx064.idc.local 86400 A 10.16.39.64
idc-ompx065.idc.local 86400 A 10.16.39.65
idc-ompx066.idc.local 86400 A 10.16.39.66
idc-ompx067.idc.local 86400 A 10.16.39.67
idc-ompx068.idc.local 86400 A 10.16.39.68
idc-ompx069.idc.local 86400 A 10.16.39.69
idc-ompx070.idc.local 86400 A 10.16.39.70
idc-ompx071.idc.local 86400 A 10.16.39.71
idc-ompx072.idc.local 86400 A 10.16.39.72
idc-ompx073.idc.local 86400 A 10.16.39.73
idc-ompx074.idc.local 86400 A 10.16.39.74
idc-ompx075.idc.local 86400 A 10.16.39.75
idc-ompx076.idc.local 86400 A 10.16.39.76
idc-ompx077.idc.local 86400 A 10.16.39.77
idc-ompx078.idc.local 86400 A 10.16.39.78
idc-ompx079.idc.local 86400 A 10.16.39.79
idc-ompx080.idc.local 86400 A 10.16.39.80
idc-ompx081.idc.local 86400 A 10.16.39.81
idc-ompx082.idc.local 86400 A 10.16.39.82
idc-ompx083.idc.local 86400 A 10.16.39.83
idc-ompx084.idc.local 86400 A 10.16.39.84
idc-ompx085.idc.local 86400 A 10.16.39.85
idc-ompx086.idc.local 86400 A 10.16.39.86
idc-ompx087.idc.local 86400 A 10.16.39.87
idc-ompx088.idc.local 86400 A 10.16.39.88
idc-ompx089.idc.local 86400 A 10.16.39.89
idc-ompx090.idc.local 86400 A 10.16.39.90
idc-ompx091.idc.local 86400 A 10.16.39.91
idc-ompx092.idc.local 86400 A 10.16.39.92
idc-ompx093.idc.local 86400 A 10.16.39.93
idc-ompx094.idc.local 86400 A 10.16.39.94
idc-ompx095.idc.local 86400 A 10.16.39.95
idc-ompx096.idc.local 86400 A 10.16.39.96
idc-ompx097.idc.local 86400 A 10.16.39.97
idc-ompx098.idc.local 86400 A 10.16.39.98
idc-ompx099.idc.local 86400 A 10.16.39.99
idc-ompx100.idc.local 86400 A 10.16.39.100
idc-ompx101.idc.local 86400 A 10.16.39.101
idc-ompx102.idc.local 86400 A 10.16.39.102
idc-ompx103.idc.local 86400 A 10.16.39.103
idc-ompx104.idc.local 86400 A 10.16.39.104
idc-ompx105.idc.local 86400 A 10.16.39.105
idc-ompx106.idc.local 86400 A 10.16.39.106
idc-ompx107.idc.local 86400 A 10.16.39.107
idc-ompx108.idc.local 86400 A 10.16.39.108
idc-ompx109.idc.local 86400 A 10.16.39.109
idc-ompx110.idc.local 86400 A 10.16.39.110
idc-ompx111.idc.local 86400 A 10.16.39.111
idc-ompx112.idc.local 86400 A 10.16.39.112
idc-ompx113.idc.local 86400 A 10.16.39.113
idc-ompx114.idc.local 86400 A 10.16.39.114
idc-ompx115.idc.local 86400 A 10.16.39.115
idc-ompx116.idc.local 86400 A 10.16.39.116
idc-ompx117.idc.local 86400 A 10.16.39.117
idc-ompx118.idc.local 86400 A 10.16.39.118
idc-ompx119.idc.local 86400 A 10.16.39.119
idc-ompx120.idc.local 86400 A 10.16.39.120
idc-ompx121.idc.local 86400 A 10.16.39.121
idc-ompx122.idc.local 86400 A 10.16.39.122
idc-ompx123.idc.local 86400 A 10.16.39.123
idc-ompx124.idc.local 86400 A 10.16.39.124
idc-ompx125.idc.local 86400 A 10.16.39.125
idc-ompx126.idc.local 86400 A 10.16.39.126
idc-ompx127.idc.local 86400 A 10.16.39.127
idc-ompx128.idc.local 86400 A 10.16.39.128
idc-ompx129.idc.local 86400 A 10.16.39.129
idc-ompx130.idc.local 86400 A 10.16.39.130
idc-ompx131.idc.local 86400 A 10.16.39.131
idc-ompx132.idc.local 86400 A 10.16.39.132
idc-ompx133.idc.local 86400 A 10.16.39.133
idc-ompx134.idc.local 86400 A 10.16.39.134
idc-ompx135.idc.local 86400 A 10.16.39.135
idc-ompx136.idc.local 86400 A 10.16.39.136
idc-ompx137.idc.local 86400 A 10.16.39.137
idc-ompx138.idc.local 86400 A 10.16.39.138
idc-ompx139.idc.local 86400 A 10.16.39.139
idc-ompx140.idc.local 86400 A 10.16.39.140
idc-ompx141.idc.local 86400 A 10.16.39.141
idc-ompx142.idc.local 86400 A 10.16.39.142
idc-ompx143.idc.local 86400 A 10.16.39.143
idc-ompx144.idc.local 86400 A 10.16.39.144
idc-ompx145.idc.local 86400 A 10.16.39.145
idc-ompx146.idc.local 86400 A 10.16.39.146
idc-ompx147.idc.local 86400 A 10.16.39.147
idc-ompx148.idc.local 86400 A 10.16.39.148
idc-ompx149.idc.local 86400 A 10.16.39.149
idc-ompx150.idc.local 86400 A 10.16.39.150
idc-ompx151.idc.local 86400 A 10.16.39.151
idc-ompx152.idc.local 86400 A 10.16.39.152
idc-ompx153.idc.local 86400 A 10.16.39.153
idc-ompx154.idc.local 86400 A 10.16.39.154
idc-ompx155.idc.local 86400 A 10.16.39.155
idc-ompx156.idc.local 86400 A 10.16.39.156
idc-ompx157.idc.local 86400 A 10.16.39.157
idc-ompx158.idc.local 86400 A 10.16.39.158
idc-ompx159.idc.local 86400 A 10.16.39.159
idc-ompx160.idc.local 86400 A 10.16.39.160
idc-ompx161.idc.local 86400 A 10.16.39.161
idc-ompx162.idc.local 86400 A 10.16.39.162
idc-ompx163.idc.local 86400 A 10.16.39.163
idc-ompx164.idc.local 86400 A 10.16.39.164
idc-ompx165.idc.local 86400 A 10.16.39.165
idc-ompx166.idc.local 86400 A 10.16.39.166
idc-ompx167.idc.local 86400 A 10.16.39.167
idc-ompx168.idc.local 86400 A 10.16.39.168
idc-ompx169.idc.local 86400 A 10.16.39.169
idc-ompx170.idc.local 86400 A 10.16.39.170
idc-ompx171.idc.local 86400 A 10.16.39.171
idc-ompx172.idc.local 86400 A 10.16.39.172
idc-ompx173.idc.local 86400 A 10.16.39.173
idc-ompx174.idc.local 86400 A 10.16.39.174
idc-ompx175.idc.local 86400 A 10.16.39.175
idc-ompx176.idc.local 86400 A 10.16.39.176
idc-ompx177.idc.local 86400 A 10.16.39.177
idc-ompx178.idc.local 86400 A 10.16.39.178
idc-ompx179.idc.local 86400 A 10.16.39.179
idc-ompx180.idc.local 86400 A 10.16.39.180
idc-ompx181.idc.local 86400 A 10.16.39.181
idc-ompx182.idc.local 86400 A 10.16.39.182
idc-ompx183.idc.local 86400 A 10.16.39.183
idc-ompx184.idc.local 86400 A 10.16.39.184
idc-ompx185.idc.local 86400 A 10.16.39.185
idc-ompx186.idc.local 86400 A 10.16.39.186
idc-ompx187.idc.local 86400 A 10.16.39.187
idc-ompx188.idc.local 86400 A 10.16.39.188
idc-ompx189.idc.local 86400 A 10.16.39.189
idc-ompx190.idc.local 86400 A 10.16.39.190
idc-ompx191.idc.local 86400 A 10.16.39.191
idc-ompx192.idc.local 86400 A 10.16.39.192
idc-ompx193.idc.local 86400 A 10.16.39.193
idc-ompx194.idc.local 86400 A 10.16.39.194
idc-ompx195.idc.local 86400 A 10.16.39.195
idc-ompx196.idc.local 86400 A 10.16.39.196
idc-ompx197.idc.local 86400 A 10.16.39.197
idc-ompx198.idc.local 86400 A 10.16.39.198
idc-ompx199.idc.local 86400 A 10.16.39.199
idc-ompx200.idc.local 86400 A 10.16.39.200
idc-ompx201.idc.local 86400 A 10.16.39.201
idc-ompx202.idc.local 86400 A 10.16.39.202
idc-ompx203.idc.local 86400 A 10.16.39.203
idc-ompx204.idc.local 86400 A 10.16.39.204
idc-ompx205.idc.local 86400 A 10.16.39.205
idc-ompx206.idc.local 86400 A 10.16.39.206
idc-ompx207.idc.local 86400 A 10.16.39.207
idc-ompx208.idc.local 86400 A 10.16.39.208
idc-ompx209.idc.local 86400 A 10.16.39.209
idc-ompx210.idc.local 86400 A 10.16.39.210
idc-ompx211.idc.local 86400 A 10.16.39.211
idc-ompx212.idc.local 86400 A 10.16.39.212
idc-ompx213.idc.local 86400 A 10.16.39.213
idc-ompx214.idc.local 86400 A 10.16.39.214
idc-ompx215.idc.local 86400 A 10.16.39.215
idc-ompx216.idc.local 86400 A 10.16.39.216
idc-ompx217.idc.local 86400 A 10.16.39.217
idc-ompx218.idc.local 86400 A 10.16.39.218
idc-ompx219.idc.local 86400 A 10.16.39.219
idc-ompx220.idc.local 86400 A 10.16.39.220
idc-ompx221.idc.local 86400 A 10.16.39.221
idc-ompx222.idc.local 86400 A 10.16.39.222
idc-ompx223.idc.local 86400 A 10.16.39.223
idc-ompx224.idc.local 86400 A 10.16.39.224
idc-ompx225.idc.local 86400 A 10.16.39.225
idc-ompx226.idc.local 86400 A 10.16.39.226
idc-ompx227.idc.local 86400 A 10.16.39.227
idc-ompx228.idc.local 86400 A 10.16.39.228
idc-ompx229.idc.local 86400 A 10.16.39.229
idc-ompx230.idc.local 86400 A 10.16.39.230
idc-ompx231.idc.local 86400 A 10.16.39.231
idc-ompx232.idc.local 86400 A 10.16.39.232
idc-ompx233.idc.local 86400 A 10.16.39.233
idc-ompx234.idc.local 86400 A 10.16.39.234
idc-ompx235.idc.local 86400 A 10.16.39.235
idc-ompx236.idc.local 86400 A 10.16.39.236
idc-ompx237.idc.local 86400 A 10.16.39.237
idc-ompx238.idc.local 86400 A 10.16.39.238
idc-ompx239.idc.local 86400 A 10.16.39.239
idc-ompx240.idc.local 86400 A 10.16.39.240
idc-ompx241.idc.local 86400 A 10.16.39.241
idc-ompx242.idc.local 86400 A 10.16.39.242
idc-ompx243.idc.local 86400 A 10.16.39.243
idc-ompx244.idc.local 86400 A 10.16.39.244
idc-ompx245.idc.local 86400 A 10.16.39.245
idc-ompx246.idc.local 86400 A 10.16.39.246
idc-ompx247.idc.local 86400 A 10.16.39.247
idc-ompx248.idc.local 86400 A 10.16.39.248
idc-ompx249.idc.local 86400 A 10.16.39.249
idc-ompx250.idc.local 86400 A 10.16.39.250
idc-ompx251.idc.local 86400 A 10.16.39.251
idc-ompx252.idc.local 86400 A 10.16.39.252
idc-ompx253.idc.local 86400 A 10.16.39.253
\.

--
-- Data for Name: reverse_10_in_addr_arpa; Type: TABLE DATA; Schema: public; Owner: named
--

COPY reverse_10_in_addr_arpa (name, ttl, rdtype, rdata) FROM stdin;
10.in-addr.arpa    86400    SOA    ns1.idc.local. root.idc.local. 2015040701 3H 2M 1W 1D
0.10.in-addr.arpa    86400    SOA    ns1.idc.local. root.idc.local. 2015040701 3H 2M 1W 1D
0.0.10.in-addr.arpa    86400    SOA    ns1.idc.local. root.idc.local. 2015040701 3H 2M 1W 1D
10.in-addr.arpa    86400    NS    ns1.idc.local.
0.10.in-addr.arpa    86400    NS    ns1.idc.local.
0.0.10.in-addr.arpa    86400    NS    ns1.idc.local.
2.17.16.10.in-addr.arpa    86400    PTR    idc-oprt01.idc.local
3.17.16.10.in-addr.arpa    86400    PTR    idc-oprt02.idc.local
50.33.16.10.in-addr.arpa    86400    PTR    idc-opfw01.idc.local
51.33.16.10.in-addr.arpa    86400    PTR    idc-opfw02.idc.local
17.17.16.10.in-addr.arpa    86400    PTR    idc-ol3s0a.idc.local
33.17.16.10.in-addr.arpa    86400    PTR    idc-ol2s0a.idc.local
34.17.16.10.in-addr.arpa    86400    PTR    idc-ol2s0b.idc.local
35.17.16.10.in-addr.arpa    86400    PTR    idc-ol2s05.idc.local
36.33.16.10.in-addr.arpa    86400    PTR    idc-ol2s0c.idc.local
37.33.16.10.in-addr.arpa    86400    PTR    idc-ol2s08.idc.local
38.33.16.10.in-addr.arpa    86400    PTR    idc-ol2s09.idc.local
69.17.16.10.in-addr.arpa    86400    PTR    idc-olog01.idc.local
73.17.16.10.in-addr.arpa    86400    PTR    idc-ontp01.idc.local
129.17.16.10.in-addr.arpa    86400    PTR    idc-oist0a.idc.local
130.17.16.10.in-addr.arpa    86400    PTR    idc-oist01.idc.local
131.17.16.10.in-addr.arpa    86400    PTR    idc-oist02.idc.local
81.33.16.10.in-addr.arpa    86400    PTR    idc-occr0a.idc.local
82.33.16.10.in-addr.arpa    86400    PTR    idc-occr01.idc.local
83.33.16.10.in-addr.arpa    86400    PTR    idc-occr02.idc.local
98.33.16.10.in-addr.arpa    86400    PTR    idc-ohyp01.idc.local
99.33.16.10.in-addr.arpa    86400    PTR    idc-ohyp02.idc.local
100.33.16.10.in-addr.arpa    86400    PTR    idc-ohyp03.idc.local
101.33.16.10.in-addr.arpa    86400    PTR    idc-ohyp04.idc.local
102.33.16.10.in-addr.arpa    86400    PTR    idc-ohyp05.idc.local
103.33.16.10.in-addr.arpa    86400    PTR    idc-ohyp06.idc.local
104.33.16.10.in-addr.arpa    86400    PTR    idc-ohyp07.idc.local
145.33.16.10.in-addr.arpa    86400    PTR    idc-omon0a.idc.local
146.33.16.10.in-addr.arpa    86400    PTR    idc-omon01.idc.local
147.33.16.10.in-addr.arpa    86400    PTR    idc-omon02.idc.local
194.33.16.10.in-addr.arpa    86400    PTR    idc-onas01.idc.local
82.34.16.10.in-addr.arpa    86400    PTR    idc-occr01i.idc.local
83.34.16.10.in-addr.arpa    86400    PTR    idc-occr02i.idc.local
98.34.16.10.in-addr.arpa    86400    PTR    idc-ohyp01i.idc.local
99.34.16.10.in-addr.arpa    86400    PTR    idc-ohyp02i.idc.local
100.34.16.10.in-addr.arpa    86400    PTR    idc-ohyp03i.idc.local
101.34.16.10.in-addr.arpa    86400    PTR    idc-ohyp04i.idc.local
102.34.16.10.in-addr.arpa    86400    PTR    idc-ohyp05i.idc.local
103.34.16.10.in-addr.arpa    86400    PTR    idc-ohyp06i.idc.local
104.34.16.10.in-addr.arpa    86400    PTR    idc-ohyp07i.idc.local
146.34.16.10.in-addr.arpa    86400    PTR    idc-omon01i.idc.local
147.34.16.10.in-addr.arpa    86400    PTR    idc-omon02i.idc.local
194.34.16.10.in-addr.arpa    86400    PTR    idc-onas01i.idc.local
82.35.16.10.in-addr.arpa    86400    PTR    idc-occr01v.idc.local
83.35.16.10.in-addr.arpa    86400    PTR    idc-occr02v.idc.local
129.24.16.10.in-addr.arpa    86400    PTR    idc-oist0a.idc.local
130.24.16.10.in-addr.arpa    86400    PTR    idc-oist01.idc.local
131.24.16.10.in-addr.arpa    86400    PTR    idc-oist02.idc.local
129.25.16.10.in-addr.arpa    86400    PTR    idc-oist0a.idc.local
130.25.16.10.in-addr.arpa    86400    PTR    idc-oist01.idc.local
131.25.16.10.in-addr.arpa    86400    PTR    idc-oist02.idc.local
98.41.16.10.in-addr.arpa    86400    PTR    idc-ohyp01s1.idc.local
99.41.16.10.in-addr.arpa    86400    PTR    idc-ohyp02s1.idc.local
100.41.16.10.in-addr.arpa    86400    PTR    idc-ohyp03s1.idc.local
101.41.16.10.in-addr.arpa    86400    PTR    idc-ohyp04s1.idc.local
102.41.16.10.in-addr.arpa    86400    PTR    idc-ohyp05s1.idc.local
103.41.16.10.in-addr.arpa    86400    PTR    idc-ohyp06s1.idc.local
104.41.16.10.in-addr.arpa    86400    PTR    idc-ohyp07s1.idc.local
98.42.16.10.in-addr.arpa    86400    PTR    idc-ohyp01s1.idc.local
99.42.16.10.in-addr.arpa    86400    PTR    idc-ohyp02s1.idc.local
100.42.16.10.in-addr.arpa    86400    PTR    idc-ohyp03s1.idc.local
101.42.16.10.in-addr.arpa    86400    PTR    idc-ohyp04s1.idc.local
102.42.16.10.in-addr.arpa    86400    PTR    idc-ohyp05s1.idc.local
103.42.16.10.in-addr.arpa    86400    PTR    idc-ohyp06s1.idc.local
104.42.16.10.in-addr.arpa    86400    PTR    idc-ohyp07s1.idc.local
82.32.16.10.in-addr.arpa    86400    PTR    idc-occr01h.idc.local
83.32.16.10.in-addr.arpa    86400    PTR    idc-occr02h.idc.local
146.32.16.10.in-addr.arpa    86400    PTR    idc-omon01h.idc.local
147.32.16.10.in-addr.arpa    86400    PTR    idc-omon02h.idc.local
1.38.16.10.in-addr.arpa 86400   PTR     idc-ovrt001.idc.local
2.38.16.10.in-addr.arpa 86400   PTR     idc-ovrt002.idc.local
3.38.16.10.in-addr.arpa 86400   PTR     idc-ovrt003.idc.local
4.38.16.10.in-addr.arpa 86400   PTR     idc-ovrt004.idc.local
5.38.16.10.in-addr.arpa 86400   PTR     idc-ovrt005.idc.local
6.38.16.10.in-addr.arpa 86400   PTR     idc-ovrt006.idc.local
7.38.16.10.in-addr.arpa 86400   PTR     idc-ovrt007.idc.local
8.38.16.10.in-addr.arpa 86400   PTR     idc-ovrt008.idc.local
9.38.16.10.in-addr.arpa 86400   PTR     idc-ovrt009.idc.local
10.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt010.idc.local
11.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt011.idc.local
12.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt012.idc.local
13.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt013.idc.local
14.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt014.idc.local
15.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt015.idc.local
16.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt016.idc.local
17.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt017.idc.local
18.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt018.idc.local
19.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt019.idc.local
20.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt020.idc.local
21.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt021.idc.local
22.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt022.idc.local
23.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt023.idc.local
24.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt024.idc.local
25.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt025.idc.local
26.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt026.idc.local
27.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt027.idc.local
28.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt028.idc.local
29.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt029.idc.local
30.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt030.idc.local
31.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt031.idc.local
32.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt032.idc.local
33.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt033.idc.local
34.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt034.idc.local
35.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt035.idc.local
36.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt036.idc.local
37.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt037.idc.local
38.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt038.idc.local
39.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt039.idc.local
40.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt040.idc.local
41.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt041.idc.local
42.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt042.idc.local
43.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt043.idc.local
44.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt044.idc.local
45.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt045.idc.local
46.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt046.idc.local
47.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt047.idc.local
48.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt048.idc.local
49.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt049.idc.local
50.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt050.idc.local
51.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt051.idc.local
52.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt052.idc.local
53.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt053.idc.local
54.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt054.idc.local
55.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt055.idc.local
56.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt056.idc.local
57.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt057.idc.local
58.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt058.idc.local
59.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt059.idc.local
60.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt060.idc.local
61.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt061.idc.local
62.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt062.idc.local
63.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt063.idc.local
64.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt064.idc.local
65.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt065.idc.local
66.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt066.idc.local
67.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt067.idc.local
68.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt068.idc.local
69.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt069.idc.local
70.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt070.idc.local
71.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt071.idc.local
72.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt072.idc.local
73.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt073.idc.local
74.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt074.idc.local
75.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt075.idc.local
76.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt076.idc.local
77.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt077.idc.local
78.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt078.idc.local
79.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt079.idc.local
80.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt080.idc.local
81.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt081.idc.local
82.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt082.idc.local
83.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt083.idc.local
84.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt084.idc.local
85.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt085.idc.local
86.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt086.idc.local
87.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt087.idc.local
88.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt088.idc.local
89.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt089.idc.local
90.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt090.idc.local
91.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt091.idc.local
92.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt092.idc.local
93.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt093.idc.local
94.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt094.idc.local
95.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt095.idc.local
96.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt096.idc.local
97.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt097.idc.local
98.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt098.idc.local
99.38.16.10.in-addr.arpa        86400   PTR     idc-ovrt099.idc.local
100.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt100.idc.local
101.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt101.idc.local
102.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt102.idc.local
103.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt103.idc.local
104.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt104.idc.local
105.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt105.idc.local
106.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt106.idc.local
107.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt107.idc.local
108.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt108.idc.local
109.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt109.idc.local
110.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt110.idc.local
111.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt111.idc.local
112.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt112.idc.local
113.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt113.idc.local
114.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt114.idc.local
115.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt115.idc.local
116.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt116.idc.local
117.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt117.idc.local
118.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt118.idc.local
119.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt119.idc.local
120.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt120.idc.local
121.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt121.idc.local
122.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt122.idc.local
123.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt123.idc.local
124.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt124.idc.local
125.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt125.idc.local
126.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt126.idc.local
127.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt127.idc.local
128.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt128.idc.local
129.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt129.idc.local
130.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt130.idc.local
131.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt131.idc.local
132.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt132.idc.local
133.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt133.idc.local
134.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt134.idc.local
135.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt135.idc.local
136.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt136.idc.local
137.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt137.idc.local
138.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt138.idc.local
139.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt139.idc.local
140.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt140.idc.local
141.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt141.idc.local
142.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt142.idc.local
143.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt143.idc.local
144.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt144.idc.local
145.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt145.idc.local
146.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt146.idc.local
147.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt147.idc.local
148.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt148.idc.local
149.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt149.idc.local
150.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt150.idc.local
151.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt151.idc.local
152.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt152.idc.local
153.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt153.idc.local
154.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt154.idc.local
155.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt155.idc.local
156.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt156.idc.local
157.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt157.idc.local
158.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt158.idc.local
159.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt159.idc.local
160.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt160.idc.local
161.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt161.idc.local
162.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt162.idc.local
163.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt163.idc.local
164.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt164.idc.local
165.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt165.idc.local
166.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt166.idc.local
167.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt167.idc.local
168.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt168.idc.local
169.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt169.idc.local
170.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt170.idc.local
171.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt171.idc.local
172.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt172.idc.local
173.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt173.idc.local
174.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt174.idc.local
175.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt175.idc.local
176.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt176.idc.local
177.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt177.idc.local
178.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt178.idc.local
179.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt179.idc.local
180.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt180.idc.local
181.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt181.idc.local
182.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt182.idc.local
183.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt183.idc.local
184.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt184.idc.local
185.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt185.idc.local
186.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt186.idc.local
187.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt187.idc.local
188.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt188.idc.local
189.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt189.idc.local
190.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt190.idc.local
191.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt191.idc.local
192.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt192.idc.local
193.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt193.idc.local
194.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt194.idc.local
195.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt195.idc.local
196.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt196.idc.local
197.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt197.idc.local
198.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt198.idc.local
199.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt199.idc.local
200.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt200.idc.local
201.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt201.idc.local
202.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt202.idc.local
203.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt203.idc.local
204.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt204.idc.local
205.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt205.idc.local
206.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt206.idc.local
207.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt207.idc.local
208.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt208.idc.local
209.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt209.idc.local
210.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt210.idc.local
211.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt211.idc.local
212.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt212.idc.local
213.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt213.idc.local
214.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt214.idc.local
215.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt215.idc.local
216.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt216.idc.local
217.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt217.idc.local
218.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt218.idc.local
219.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt219.idc.local
220.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt220.idc.local
221.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt221.idc.local
222.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt222.idc.local
223.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt223.idc.local
224.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt224.idc.local
225.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt225.idc.local
226.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt226.idc.local
227.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt227.idc.local
228.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt228.idc.local
229.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt229.idc.local
230.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt230.idc.local
231.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt231.idc.local
232.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt232.idc.local
233.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt233.idc.local
234.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt234.idc.local
235.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt235.idc.local
236.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt236.idc.local
237.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt237.idc.local
238.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt238.idc.local
239.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt239.idc.local
240.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt240.idc.local
241.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt241.idc.local
242.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt242.idc.local
243.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt243.idc.local
244.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt244.idc.local
245.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt245.idc.local
246.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt246.idc.local
247.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt247.idc.local
248.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt248.idc.local
249.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt249.idc.local
250.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt250.idc.local
251.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt251.idc.local
252.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt252.idc.local
253.38.16.10.in-addr.arpa       86400   PTR     idc-ovrt253.idc.local
1.39.16.10.in-addr.arpa 86400   PTR     idc-ompx001.idc.local
2.39.16.10.in-addr.arpa 86400   PTR     idc-ompx002.idc.local
3.39.16.10.in-addr.arpa 86400   PTR     idc-ompx003.idc.local
4.39.16.10.in-addr.arpa 86400   PTR     idc-ompx004.idc.local
5.39.16.10.in-addr.arpa 86400   PTR     idc-ompx005.idc.local
6.39.16.10.in-addr.arpa 86400   PTR     idc-ompx006.idc.local
7.39.16.10.in-addr.arpa 86400   PTR     idc-ompx007.idc.local
8.39.16.10.in-addr.arpa 86400   PTR     idc-ompx008.idc.local
9.39.16.10.in-addr.arpa 86400   PTR     idc-ompx009.idc.local
10.39.16.10.in-addr.arpa        86400   PTR     idc-ompx010.idc.local
11.39.16.10.in-addr.arpa        86400   PTR     idc-ompx011.idc.local
12.39.16.10.in-addr.arpa        86400   PTR     idc-ompx012.idc.local
13.39.16.10.in-addr.arpa        86400   PTR     idc-ompx013.idc.local
14.39.16.10.in-addr.arpa        86400   PTR     idc-ompx014.idc.local
15.39.16.10.in-addr.arpa        86400   PTR     idc-ompx015.idc.local
16.39.16.10.in-addr.arpa        86400   PTR     idc-ompx016.idc.local
17.39.16.10.in-addr.arpa        86400   PTR     idc-ompx017.idc.local
18.39.16.10.in-addr.arpa        86400   PTR     idc-ompx018.idc.local
19.39.16.10.in-addr.arpa        86400   PTR     idc-ompx019.idc.local
20.39.16.10.in-addr.arpa        86400   PTR     idc-ompx020.idc.local
21.39.16.10.in-addr.arpa        86400   PTR     idc-ompx021.idc.local
22.39.16.10.in-addr.arpa        86400   PTR     idc-ompx022.idc.local
23.39.16.10.in-addr.arpa        86400   PTR     idc-ompx023.idc.local
24.39.16.10.in-addr.arpa        86400   PTR     idc-ompx024.idc.local
25.39.16.10.in-addr.arpa        86400   PTR     idc-ompx025.idc.local
26.39.16.10.in-addr.arpa        86400   PTR     idc-ompx026.idc.local
27.39.16.10.in-addr.arpa        86400   PTR     idc-ompx027.idc.local
28.39.16.10.in-addr.arpa        86400   PTR     idc-ompx028.idc.local
29.39.16.10.in-addr.arpa        86400   PTR     idc-ompx029.idc.local
30.39.16.10.in-addr.arpa        86400   PTR     idc-ompx030.idc.local
31.39.16.10.in-addr.arpa        86400   PTR     idc-ompx031.idc.local
32.39.16.10.in-addr.arpa        86400   PTR     idc-ompx032.idc.local
33.39.16.10.in-addr.arpa        86400   PTR     idc-ompx033.idc.local
34.39.16.10.in-addr.arpa        86400   PTR     idc-ompx034.idc.local
35.39.16.10.in-addr.arpa        86400   PTR     idc-ompx035.idc.local
36.39.16.10.in-addr.arpa        86400   PTR     idc-ompx036.idc.local
37.39.16.10.in-addr.arpa        86400   PTR     idc-ompx037.idc.local
38.39.16.10.in-addr.arpa        86400   PTR     idc-ompx038.idc.local
39.39.16.10.in-addr.arpa        86400   PTR     idc-ompx039.idc.local
40.39.16.10.in-addr.arpa        86400   PTR     idc-ompx040.idc.local
41.39.16.10.in-addr.arpa        86400   PTR     idc-ompx041.idc.local
42.39.16.10.in-addr.arpa        86400   PTR     idc-ompx042.idc.local
43.39.16.10.in-addr.arpa        86400   PTR     idc-ompx043.idc.local
44.39.16.10.in-addr.arpa        86400   PTR     idc-ompx044.idc.local
45.39.16.10.in-addr.arpa        86400   PTR     idc-ompx045.idc.local
46.39.16.10.in-addr.arpa        86400   PTR     idc-ompx046.idc.local
47.39.16.10.in-addr.arpa        86400   PTR     idc-ompx047.idc.local
48.39.16.10.in-addr.arpa        86400   PTR     idc-ompx048.idc.local
49.39.16.10.in-addr.arpa        86400   PTR     idc-ompx049.idc.local
50.39.16.10.in-addr.arpa        86400   PTR     idc-ompx050.idc.local
51.39.16.10.in-addr.arpa        86400   PTR     idc-ompx051.idc.local
52.39.16.10.in-addr.arpa        86400   PTR     idc-ompx052.idc.local
53.39.16.10.in-addr.arpa        86400   PTR     idc-ompx053.idc.local
54.39.16.10.in-addr.arpa        86400   PTR     idc-ompx054.idc.local
55.39.16.10.in-addr.arpa        86400   PTR     idc-ompx055.idc.local
56.39.16.10.in-addr.arpa        86400   PTR     idc-ompx056.idc.local
57.39.16.10.in-addr.arpa        86400   PTR     idc-ompx057.idc.local
58.39.16.10.in-addr.arpa        86400   PTR     idc-ompx058.idc.local
59.39.16.10.in-addr.arpa        86400   PTR     idc-ompx059.idc.local
60.39.16.10.in-addr.arpa        86400   PTR     idc-ompx060.idc.local
61.39.16.10.in-addr.arpa        86400   PTR     idc-ompx061.idc.local
62.39.16.10.in-addr.arpa        86400   PTR     idc-ompx062.idc.local
63.39.16.10.in-addr.arpa        86400   PTR     idc-ompx063.idc.local
64.39.16.10.in-addr.arpa        86400   PTR     idc-ompx064.idc.local
65.39.16.10.in-addr.arpa        86400   PTR     idc-ompx065.idc.local
66.39.16.10.in-addr.arpa        86400   PTR     idc-ompx066.idc.local
67.39.16.10.in-addr.arpa        86400   PTR     idc-ompx067.idc.local
68.39.16.10.in-addr.arpa        86400   PTR     idc-ompx068.idc.local
69.39.16.10.in-addr.arpa        86400   PTR     idc-ompx069.idc.local
70.39.16.10.in-addr.arpa        86400   PTR     idc-ompx070.idc.local
71.39.16.10.in-addr.arpa        86400   PTR     idc-ompx071.idc.local
72.39.16.10.in-addr.arpa        86400   PTR     idc-ompx072.idc.local
73.39.16.10.in-addr.arpa        86400   PTR     idc-ompx073.idc.local
74.39.16.10.in-addr.arpa        86400   PTR     idc-ompx074.idc.local
75.39.16.10.in-addr.arpa        86400   PTR     idc-ompx075.idc.local
76.39.16.10.in-addr.arpa        86400   PTR     idc-ompx076.idc.local
77.39.16.10.in-addr.arpa        86400   PTR     idc-ompx077.idc.local
78.39.16.10.in-addr.arpa        86400   PTR     idc-ompx078.idc.local
79.39.16.10.in-addr.arpa        86400   PTR     idc-ompx079.idc.local
80.39.16.10.in-addr.arpa        86400   PTR     idc-ompx080.idc.local
81.39.16.10.in-addr.arpa        86400   PTR     idc-ompx081.idc.local
82.39.16.10.in-addr.arpa        86400   PTR     idc-ompx082.idc.local
83.39.16.10.in-addr.arpa        86400   PTR     idc-ompx083.idc.local
84.39.16.10.in-addr.arpa        86400   PTR     idc-ompx084.idc.local
85.39.16.10.in-addr.arpa        86400   PTR     idc-ompx085.idc.local
86.39.16.10.in-addr.arpa        86400   PTR     idc-ompx086.idc.local
87.39.16.10.in-addr.arpa        86400   PTR     idc-ompx087.idc.local
88.39.16.10.in-addr.arpa        86400   PTR     idc-ompx088.idc.local
89.39.16.10.in-addr.arpa        86400   PTR     idc-ompx089.idc.local
90.39.16.10.in-addr.arpa        86400   PTR     idc-ompx090.idc.local
91.39.16.10.in-addr.arpa        86400   PTR     idc-ompx091.idc.local
92.39.16.10.in-addr.arpa        86400   PTR     idc-ompx092.idc.local
93.39.16.10.in-addr.arpa        86400   PTR     idc-ompx093.idc.local
94.39.16.10.in-addr.arpa        86400   PTR     idc-ompx094.idc.local
95.39.16.10.in-addr.arpa        86400   PTR     idc-ompx095.idc.local
96.39.16.10.in-addr.arpa        86400   PTR     idc-ompx096.idc.local
97.39.16.10.in-addr.arpa        86400   PTR     idc-ompx097.idc.local
98.39.16.10.in-addr.arpa        86400   PTR     idc-ompx098.idc.local
99.39.16.10.in-addr.arpa        86400   PTR     idc-ompx099.idc.local
100.39.16.10.in-addr.arpa       86400   PTR     idc-ompx100.idc.local
101.39.16.10.in-addr.arpa       86400   PTR     idc-ompx101.idc.local
102.39.16.10.in-addr.arpa       86400   PTR     idc-ompx102.idc.local
103.39.16.10.in-addr.arpa       86400   PTR     idc-ompx103.idc.local
104.39.16.10.in-addr.arpa       86400   PTR     idc-ompx104.idc.local
105.39.16.10.in-addr.arpa       86400   PTR     idc-ompx105.idc.local
106.39.16.10.in-addr.arpa       86400   PTR     idc-ompx106.idc.local
107.39.16.10.in-addr.arpa       86400   PTR     idc-ompx107.idc.local
108.39.16.10.in-addr.arpa       86400   PTR     idc-ompx108.idc.local
109.39.16.10.in-addr.arpa       86400   PTR     idc-ompx109.idc.local
110.39.16.10.in-addr.arpa       86400   PTR     idc-ompx110.idc.local
111.39.16.10.in-addr.arpa       86400   PTR     idc-ompx111.idc.local
112.39.16.10.in-addr.arpa       86400   PTR     idc-ompx112.idc.local
113.39.16.10.in-addr.arpa       86400   PTR     idc-ompx113.idc.local
114.39.16.10.in-addr.arpa       86400   PTR     idc-ompx114.idc.local
115.39.16.10.in-addr.arpa       86400   PTR     idc-ompx115.idc.local
116.39.16.10.in-addr.arpa       86400   PTR     idc-ompx116.idc.local
117.39.16.10.in-addr.arpa       86400   PTR     idc-ompx117.idc.local
118.39.16.10.in-addr.arpa       86400   PTR     idc-ompx118.idc.local
119.39.16.10.in-addr.arpa       86400   PTR     idc-ompx119.idc.local
120.39.16.10.in-addr.arpa       86400   PTR     idc-ompx120.idc.local
121.39.16.10.in-addr.arpa       86400   PTR     idc-ompx121.idc.local
122.39.16.10.in-addr.arpa       86400   PTR     idc-ompx122.idc.local
123.39.16.10.in-addr.arpa       86400   PTR     idc-ompx123.idc.local
124.39.16.10.in-addr.arpa       86400   PTR     idc-ompx124.idc.local
125.39.16.10.in-addr.arpa       86400   PTR     idc-ompx125.idc.local
126.39.16.10.in-addr.arpa       86400   PTR     idc-ompx126.idc.local
127.39.16.10.in-addr.arpa       86400   PTR     idc-ompx127.idc.local
128.39.16.10.in-addr.arpa       86400   PTR     idc-ompx128.idc.local
129.39.16.10.in-addr.arpa       86400   PTR     idc-ompx129.idc.local
130.39.16.10.in-addr.arpa       86400   PTR     idc-ompx130.idc.local
131.39.16.10.in-addr.arpa       86400   PTR     idc-ompx131.idc.local
132.39.16.10.in-addr.arpa       86400   PTR     idc-ompx132.idc.local
133.39.16.10.in-addr.arpa       86400   PTR     idc-ompx133.idc.local
134.39.16.10.in-addr.arpa       86400   PTR     idc-ompx134.idc.local
135.39.16.10.in-addr.arpa       86400   PTR     idc-ompx135.idc.local
136.39.16.10.in-addr.arpa       86400   PTR     idc-ompx136.idc.local
137.39.16.10.in-addr.arpa       86400   PTR     idc-ompx137.idc.local
138.39.16.10.in-addr.arpa       86400   PTR     idc-ompx138.idc.local
139.39.16.10.in-addr.arpa       86400   PTR     idc-ompx139.idc.local
140.39.16.10.in-addr.arpa       86400   PTR     idc-ompx140.idc.local
141.39.16.10.in-addr.arpa       86400   PTR     idc-ompx141.idc.local
142.39.16.10.in-addr.arpa       86400   PTR     idc-ompx142.idc.local
143.39.16.10.in-addr.arpa       86400   PTR     idc-ompx143.idc.local
144.39.16.10.in-addr.arpa       86400   PTR     idc-ompx144.idc.local
145.39.16.10.in-addr.arpa       86400   PTR     idc-ompx145.idc.local
146.39.16.10.in-addr.arpa       86400   PTR     idc-ompx146.idc.local
147.39.16.10.in-addr.arpa       86400   PTR     idc-ompx147.idc.local
148.39.16.10.in-addr.arpa       86400   PTR     idc-ompx148.idc.local
149.39.16.10.in-addr.arpa       86400   PTR     idc-ompx149.idc.local
150.39.16.10.in-addr.arpa       86400   PTR     idc-ompx150.idc.local
151.39.16.10.in-addr.arpa       86400   PTR     idc-ompx151.idc.local
152.39.16.10.in-addr.arpa       86400   PTR     idc-ompx152.idc.local
153.39.16.10.in-addr.arpa       86400   PTR     idc-ompx153.idc.local
154.39.16.10.in-addr.arpa       86400   PTR     idc-ompx154.idc.local
155.39.16.10.in-addr.arpa       86400   PTR     idc-ompx155.idc.local
156.39.16.10.in-addr.arpa       86400   PTR     idc-ompx156.idc.local
157.39.16.10.in-addr.arpa       86400   PTR     idc-ompx157.idc.local
158.39.16.10.in-addr.arpa       86400   PTR     idc-ompx158.idc.local
159.39.16.10.in-addr.arpa       86400   PTR     idc-ompx159.idc.local
160.39.16.10.in-addr.arpa       86400   PTR     idc-ompx160.idc.local
161.39.16.10.in-addr.arpa       86400   PTR     idc-ompx161.idc.local
162.39.16.10.in-addr.arpa       86400   PTR     idc-ompx162.idc.local
163.39.16.10.in-addr.arpa       86400   PTR     idc-ompx163.idc.local
164.39.16.10.in-addr.arpa       86400   PTR     idc-ompx164.idc.local
165.39.16.10.in-addr.arpa       86400   PTR     idc-ompx165.idc.local
166.39.16.10.in-addr.arpa       86400   PTR     idc-ompx166.idc.local
167.39.16.10.in-addr.arpa       86400   PTR     idc-ompx167.idc.local
168.39.16.10.in-addr.arpa       86400   PTR     idc-ompx168.idc.local
169.39.16.10.in-addr.arpa       86400   PTR     idc-ompx169.idc.local
170.39.16.10.in-addr.arpa       86400   PTR     idc-ompx170.idc.local
171.39.16.10.in-addr.arpa       86400   PTR     idc-ompx171.idc.local
172.39.16.10.in-addr.arpa       86400   PTR     idc-ompx172.idc.local
173.39.16.10.in-addr.arpa       86400   PTR     idc-ompx173.idc.local
174.39.16.10.in-addr.arpa       86400   PTR     idc-ompx174.idc.local
175.39.16.10.in-addr.arpa       86400   PTR     idc-ompx175.idc.local
176.39.16.10.in-addr.arpa       86400   PTR     idc-ompx176.idc.local
177.39.16.10.in-addr.arpa       86400   PTR     idc-ompx177.idc.local
178.39.16.10.in-addr.arpa       86400   PTR     idc-ompx178.idc.local
179.39.16.10.in-addr.arpa       86400   PTR     idc-ompx179.idc.local
180.39.16.10.in-addr.arpa       86400   PTR     idc-ompx180.idc.local
181.39.16.10.in-addr.arpa       86400   PTR     idc-ompx181.idc.local
182.39.16.10.in-addr.arpa       86400   PTR     idc-ompx182.idc.local
183.39.16.10.in-addr.arpa       86400   PTR     idc-ompx183.idc.local
184.39.16.10.in-addr.arpa       86400   PTR     idc-ompx184.idc.local
185.39.16.10.in-addr.arpa       86400   PTR     idc-ompx185.idc.local
186.39.16.10.in-addr.arpa       86400   PTR     idc-ompx186.idc.local
187.39.16.10.in-addr.arpa       86400   PTR     idc-ompx187.idc.local
188.39.16.10.in-addr.arpa       86400   PTR     idc-ompx188.idc.local
189.39.16.10.in-addr.arpa       86400   PTR     idc-ompx189.idc.local
190.39.16.10.in-addr.arpa       86400   PTR     idc-ompx190.idc.local
191.39.16.10.in-addr.arpa       86400   PTR     idc-ompx191.idc.local
192.39.16.10.in-addr.arpa       86400   PTR     idc-ompx192.idc.local
193.39.16.10.in-addr.arpa       86400   PTR     idc-ompx193.idc.local
194.39.16.10.in-addr.arpa       86400   PTR     idc-ompx194.idc.local
195.39.16.10.in-addr.arpa       86400   PTR     idc-ompx195.idc.local
196.39.16.10.in-addr.arpa       86400   PTR     idc-ompx196.idc.local
197.39.16.10.in-addr.arpa       86400   PTR     idc-ompx197.idc.local
198.39.16.10.in-addr.arpa       86400   PTR     idc-ompx198.idc.local
199.39.16.10.in-addr.arpa       86400   PTR     idc-ompx199.idc.local
200.39.16.10.in-addr.arpa       86400   PTR     idc-ompx200.idc.local
201.39.16.10.in-addr.arpa       86400   PTR     idc-ompx201.idc.local
202.39.16.10.in-addr.arpa       86400   PTR     idc-ompx202.idc.local
203.39.16.10.in-addr.arpa       86400   PTR     idc-ompx203.idc.local
204.39.16.10.in-addr.arpa       86400   PTR     idc-ompx204.idc.local
205.39.16.10.in-addr.arpa       86400   PTR     idc-ompx205.idc.local
206.39.16.10.in-addr.arpa       86400   PTR     idc-ompx206.idc.local
207.39.16.10.in-addr.arpa       86400   PTR     idc-ompx207.idc.local
208.39.16.10.in-addr.arpa       86400   PTR     idc-ompx208.idc.local
209.39.16.10.in-addr.arpa       86400   PTR     idc-ompx209.idc.local
210.39.16.10.in-addr.arpa       86400   PTR     idc-ompx210.idc.local
211.39.16.10.in-addr.arpa       86400   PTR     idc-ompx211.idc.local
212.39.16.10.in-addr.arpa       86400   PTR     idc-ompx212.idc.local
213.39.16.10.in-addr.arpa       86400   PTR     idc-ompx213.idc.local
214.39.16.10.in-addr.arpa       86400   PTR     idc-ompx214.idc.local
215.39.16.10.in-addr.arpa       86400   PTR     idc-ompx215.idc.local
216.39.16.10.in-addr.arpa       86400   PTR     idc-ompx216.idc.local
217.39.16.10.in-addr.arpa       86400   PTR     idc-ompx217.idc.local
218.39.16.10.in-addr.arpa       86400   PTR     idc-ompx218.idc.local
219.39.16.10.in-addr.arpa       86400   PTR     idc-ompx219.idc.local
220.39.16.10.in-addr.arpa       86400   PTR     idc-ompx220.idc.local
221.39.16.10.in-addr.arpa       86400   PTR     idc-ompx221.idc.local
222.39.16.10.in-addr.arpa       86400   PTR     idc-ompx222.idc.local
223.39.16.10.in-addr.arpa       86400   PTR     idc-ompx223.idc.local
224.39.16.10.in-addr.arpa       86400   PTR     idc-ompx224.idc.local
225.39.16.10.in-addr.arpa       86400   PTR     idc-ompx225.idc.local
226.39.16.10.in-addr.arpa       86400   PTR     idc-ompx226.idc.local
227.39.16.10.in-addr.arpa       86400   PTR     idc-ompx227.idc.local
228.39.16.10.in-addr.arpa       86400   PTR     idc-ompx228.idc.local
229.39.16.10.in-addr.arpa       86400   PTR     idc-ompx229.idc.local
230.39.16.10.in-addr.arpa       86400   PTR     idc-ompx230.idc.local
231.39.16.10.in-addr.arpa       86400   PTR     idc-ompx231.idc.local
232.39.16.10.in-addr.arpa       86400   PTR     idc-ompx232.idc.local
233.39.16.10.in-addr.arpa       86400   PTR     idc-ompx233.idc.local
234.39.16.10.in-addr.arpa       86400   PTR     idc-ompx234.idc.local
235.39.16.10.in-addr.arpa       86400   PTR     idc-ompx235.idc.local
236.39.16.10.in-addr.arpa       86400   PTR     idc-ompx236.idc.local
237.39.16.10.in-addr.arpa       86400   PTR     idc-ompx237.idc.local
238.39.16.10.in-addr.arpa       86400   PTR     idc-ompx238.idc.local
239.39.16.10.in-addr.arpa       86400   PTR     idc-ompx239.idc.local
240.39.16.10.in-addr.arpa       86400   PTR     idc-ompx240.idc.local
241.39.16.10.in-addr.arpa       86400   PTR     idc-ompx241.idc.local
242.39.16.10.in-addr.arpa       86400   PTR     idc-ompx242.idc.local
243.39.16.10.in-addr.arpa       86400   PTR     idc-ompx243.idc.local
244.39.16.10.in-addr.arpa       86400   PTR     idc-ompx244.idc.local
245.39.16.10.in-addr.arpa       86400   PTR     idc-ompx245.idc.local
246.39.16.10.in-addr.arpa       86400   PTR     idc-ompx246.idc.local
247.39.16.10.in-addr.arpa       86400   PTR     idc-ompx247.idc.local
248.39.16.10.in-addr.arpa       86400   PTR     idc-ompx248.idc.local
249.39.16.10.in-addr.arpa       86400   PTR     idc-ompx249.idc.local
250.39.16.10.in-addr.arpa       86400   PTR     idc-ompx250.idc.local
251.39.16.10.in-addr.arpa       86400   PTR     idc-ompx251.idc.local
252.39.16.10.in-addr.arpa       86400   PTR     idc-ompx252.idc.local
253.39.16.10.in-addr.arpa       86400   PTR     idc-ompx253.idc.local
\.

--
-- Data for Name: tenant_local; Type: TABLE DATA; Schema: public; Owner: named
--

COPY tenant_local (name, ttl, rdtype, rdata) FROM stdin;
tenant.local    86400   NS      ns1.tenant.local.
tenant.local    86400   NS      ns2.tenant.local.
ns2.tenant.local        86400   A       10.16.33.83
ns1.tenant.local        86400   A       10.16.33.82
tenant.local    86400   SOA     ns1.tenant.local. root.idc.local. 2014120101 3H 2M 1W 1D
\.

--
-- Name: public; Type: ACL; Schema: -; Owner: postgres
--

REVOKE ALL ON SCHEMA public FROM PUBLIC;
REVOKE ALL ON SCHEMA public FROM postgres;
GRANT ALL ON SCHEMA public TO postgres;
GRANT ALL ON SCHEMA public TO PUBLIC;

--
-- PostgreSQL database dump complete
--

</pre></div></p>


	<p>また、仮想ルータやzabbix proxy追加時のスクリプトを以下に示す。</p>


<pre>
$ psql named named -h 10.16.33.81 -c "insert into idc_local values ('idc-ovrt013.idc.local','86400','A','10.16.38.13');" 
$ psql named named -h 10.16.33.81 -c "insert into reverse_10_in_addr_arpa values ('13.38.16.10.in-addr.arpa','86400','PTR','idc-ovrt013.idc.local');" 
</pre>

<hr />


	<a name="Dns設計_冗長化"></a>
<h1 >●冗長化<a href="#Dns設計_冗長化" class="wiki-anchor">&para;</a></h1>


	<p>pacemakerによりnamedサービスの冗長化および、<br />drbdによるDBのレプリケーションを行う。<br />リソースの関係を以下に示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>active</th>
			<th>standby</th>
		</tr>
		<tr>
			<td>ping to gateway</td>
			<td>ping to gateway</td>
		</tr>
		<tr>
			<td>vip</td>
			<td>-</td>
		</tr>
		<tr>
			<td>drbd primary</td>
			<td>drbd secondary</td>
		</tr>
		<tr>
			<td>fs mount</td>
			<td>-</td>
		</tr>
		<tr>
			<td>postgres</td>
			<td>-</td>
		</tr>
	</table>




<hr />


	<a name="Dns設計_バックアップ"></a>
<h1 >●バックアップ<a href="#Dns設計_バックアップ" class="wiki-anchor">&para;</a></h1>


	<a name="Dns設計_バックアップ概要"></a>
<h2 >バックアップ概要<a href="#Dns設計_バックアップ概要" class="wiki-anchor">&para;</a></h2>


	<p>DBサーバは冗長化を行っているものの、ヒューマンエラーによる対策は十分とはいえない。<br />そのため定期的なバックアップを取得し、いつでも状態を戻せる必要がある。<br />DBサーバにおいてバックアップ対象となるのはpostgresqlのデータバックアップである。<br />postgresqlにおいてバックアップ方法は以下の2通り存在する。</p>


	<ul>
	<li>ダンプコマンドによるダンプバックアップ</li>
		<li>ファイルシステムバックアップ(通称、ベースバックアップ)</li>
	</ul>


	<p>ダンプコマンドよるバックアップは単一のsqlファイル(もしくはpostgresql独自フォーマットファイル。リストアには専用コマンドが必要)を出力するため運用が容易である。<br />ベースバックアップ方式ではデータディレクトリをそのまま圧縮して利用するだけであるが、上記の方法に比べ運用の手順が増える。<br />運用コストを低下を優先しダンプコマンドによるsqlファイルバックアップを採用する。</p>


	<a name="Dns設計_バックアップ設定"></a>
<h2 >バックアップ設定<a href="#Dns設計_バックアップ設定" class="wiki-anchor">&para;</a></h2>


	<p>cronによるスケジューリングによりダンプとファイル圧縮を行い、nasサーバへ保管する。<br />毎日バックアップを取得するものとし、その際にファイル名に日時を付与することでリカバリポイントがわかるようにする。</p>


	<p>手動で実行する場合はプライマリノードで以下を実行する。</p>


<pre>
# /usr/local/bin/backup_idc.sh
</pre>

	<a name="Dns設計_リストア"></a>
<h2 >リストア<a href="#Dns設計_リストア" class="wiki-anchor">&para;</a></h2>


	<p>DB領域がデータ不整合等で復旧できない場合は再構築が必要となる。<br />バックアップはダンプファイルであるため、DB領域を初期化後ダンプファイルを流し込む。<br />まずはdns,pacemakerを停止する。<br />その後、drbdだけを起動しマウントを行った上で作業を行う。</p>


<pre>
### drbdを起動する。
# sudo /etc/init.d/drbd start
# sudo drbdadm primary postgres
# sudo mount /dev/drbd0 /var/lib/pgsql/9.4/data
### 現状のDB領域をできれば別の場所に保管しておく。
# sudo tar czf data.tar.gz /var/lib/pgsql/9.4/data
### 削除前に設定ファイルだけは保持しておく
# sudo cp -p /var/lib/pgsql/9.4/data/{pg_hba,postgresql}.conf /tmp
# sudo rm -rf /var/lib/pgsql/9.4/data/*
# sudo /etc/init.d/postgresql-9.4 initdb
# sudo mv /tmp/{pg_hba,postgresql}.conf /var/lib/pgsql/9.4/data
# sudo ip addr add 10.16.33.81 dev bond0
# sudo /etc/init.d/postgresql-9.4 start
</pre>

	<p>圧縮されたダンプファイルを解凍しsqlファイルの流し込みを行う。</p>


<pre>
# zcat backup_db_YYYY-MM-DD.gz | psql postgres postgres
</pre>

	<p>完了後、起動したサービス等を停止する。</p>


<pre>
# sudo /etc/init.d/postgresql-9.4 stop
# sudo ip addr del 10.16.33.81 dev bond0
# sudo umount /var/lib/pgsql/9.4/data
# sudo /etc/init.d/drbd stop
</pre>

	<p>起動手順に従いサービスを起動する。</p>


<hr />


	<a name="Dns設計_運用手順"></a>
<h1 >●運用手順<a href="#Dns設計_運用手順" class="wiki-anchor">&para;</a></h1>


	<a name="Dns設計_レコードの登録削除"></a>
<h2 >レコードの登録・削除<a href="#Dns設計_レコードの登録削除" class="wiki-anchor">&para;</a></h2>


	<p>DNSにレコードを追加(削除)する場合は以下のスクリプトを実行すること。<br />事業者用、管理用に分かれている。<br />どちらも逆引き設定まで行う。</p>


<pre>
### 管理用のDNS登録を行う場合
# ./printInternalDns.sh 13-test 10.16.211.1
insert into idc_local values ('13-test.idc.local','86400','A','10.16.211.1');
insert into reverse_10_in_addr_arpa values ('1.211.16.10.in-addr.arpa','86400','PTR','13-test.idc.local');
### 削除する場合は以下
# ./printInternalDns.sh 13-test 10.16.211.1 del
delete from idc_local where name = '13-test.idc.local';
delete from reverse_10_in_addr_arpa where name = '1.211.16.10.in-addr.arpa';
### 事業者のDNS登録を行う場合
# ./printExternalDns.sh 13-test 10.16.211.1
insert into tenant_local values ('13-test.tenant.local','86400','A','10.16.211.1');
insert into reverse_10_in_addr_arpa values ('1.211.16.10.in-addr.arpa','86400','PTR','13-test.tenant.local');
### 削除する場合は以下
# ./printExternalDns.sh 13-test 10.16.211.1 del
delete from tenant_local where name = '13-test.tenant.local';
delete from reverse_10_in_addr_arpa where name = '1.211.16.10.in-addr.arpa';
### psqlにて上記のSQLを実行する
# psql -h 10.16.33.81 named named
</pre>

	<a name="Dns設計_起動"></a>
<h2 >起動<a href="#Dns設計_起動" class="wiki-anchor">&para;</a></h2>


	<p>DNSはpacemakerで管理されるDBを見に行くため、<br />pacemakerから起動を行う。<br />以下のコマンドをクラコンの両系にて実行する。</p>


<pre>
# sudo initctl start pacemaker.combined
### 起動確認 各リソースがStarted idc-occr0[1 or 2]となればOK
# sudo crm_mon
Last updated: Fri May  8 09:34:21 2015
Last change: Thu Apr 30 10:28:56 2015
Stack: corosync
Current DC: idc-occr01 (1) - partition with quorum
Version: 1.1.12-561c4cf
2 Nodes configured
7 Resources configured

Online: [ idc-occr01 idc-occr02 ]

 Resource Group: grp_postgres
     pr_vip     (ocf::heartbeat:IPaddr2):    Started idc-occr01
     pr_fs_postgres_drbd        (ocf::heartbeat:Filesystem):    Started idc-occr01
     pr_postgres        (lsb:postgresql-9.4):   Started idc-occr01
 Master/Slave Set: ms_postgres_drbd [pr_postgres_drbd]
     Masters: [ idc-occr01 ]
     Slaves: [ idc-occr02 ]
 Clone Set: clo_ping2gw [pr_ping2gw]
     Started: [ idc-occr01 idc-occr02 ]
</pre>

	<p>上記を確認後、以下のコマンドを実行しnamedを起動する。</p>


<pre>
# sudo /etc/init.d/named start
### 以下のコマンドでプロセスの起動を確認する
# sudo /etc/init.d/named status
version: 9.8.2rc1-RedHat-9.8.2-0.30.rc1.el6_6.1 (version.bind/txt/ch disabled)
CPUs found: 4
worker threads: 4
number of zones: 5
debug level: 0
xfers running: 0
xfers deferred: 0
soa queries in progress: 0
query logging is ON
recursive clients: 0/0/1000
tcp clients: 0/100
server is up and running
named-sdb (pid  17385) を実行中...
</pre>

	<p>以下のコマンドを実行し、両サーバにて名前解決できることを確認する。</p>


<pre>
# dig @idc-occr01 www.google.com

; &lt;&lt;&gt;&gt; DiG 9.8.2rc1-RedHat-9.8.2-0.30.rc1.el6_6.1 &lt;&lt;&gt;&gt; @idc-occr01 www.google.com
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 56872
;; flags: qr rd ra; QUERY: 1, ANSWER: 5, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;www.google.com.            IN    A

;; ANSWER SECTION:
www.google.com.        88    IN    A    173.194.126.241
www.google.com.        88    IN    A    173.194.126.240
www.google.com.        88    IN    A    173.194.126.242
www.google.com.        88    IN    A    173.194.126.243
www.google.com.        88    IN    A    173.194.126.244

;; Query time: 0 msec
;; SERVER: 10.16.33.82#53(10.16.33.82)
;; WHEN: Fri May  8 09:41:24 2015
;; MSG SIZE  rcvd: 112
# dig @idc-occr02 www.google.com

; &lt;&lt;&gt;&gt; DiG 9.8.2rc1-RedHat-9.8.2-0.30.rc1.el6_6.1 &lt;&lt;&gt;&gt; @idc-occr02 www.google.com
; (1 server found)
;; global options: +cmd
;; Got answer:
;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 54250
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;www.google.com.            IN    A

;; ANSWER SECTION:
www.google.com.        26    IN    A    216.58.220.164

;; Query time: 0 msec
;; SERVER: 10.16.33.83#53(10.16.33.83)
;; WHEN: Fri May  8 09:42:04 2015
;; MSG SIZE  rcvd: 48
</pre>

	<a name="Dns設計_停止"></a>
<h2 >停止<a href="#Dns設計_停止" class="wiki-anchor">&para;</a></h2>


	<p>まずは両ノードでnamedを停止する。</p>


<pre>
# sudo /etc/init.d/named stop
</pre>

	<p>次にcrm_monコマンドで現在のセカンダリノードを特定し、セカンダリノードからpacemakerの停止を行う。<br />プライマリでもpacemakerを停止後、crm_monコマンドでリソース状態が取得できないことを確認する。</p>


<pre>
# sudo initctl start pacemaker.combined
# sudo crm_mon
</pre>

	<a name="Dns設計_復旧"></a>
<h2 >復旧<a href="#Dns設計_復旧" class="wiki-anchor">&para;</a></h2>


	<p>障害発生時にまず状況の確認を行う。<br />両ノードにて以下のコマンドを実行する。</p>


<pre>
# sudo /etc/init.d/named status
# sudo crm_mon
</pre>

	<p>上記のコマンド実行後、障害箇所を特定しログを確認する。<br />状況によりケースバイケースだが、<br />スプリットブレイン状態の場合は以下の手順を実行する。</p>


	<ol>
	<li>VIPに接続してどちらに飛んでいるか確認
<a class="collapsible collapsed" href="#" id="collapse-a9a8954f-show" onclick="$(&#x27;#collapse-a9a8954f-show, #collapse-a9a8954f-hide&#x27;).toggle(); $(&#x27;#collapse-a9a8954f&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-a9a8954f-hide" onclick="$(&#x27;#collapse-a9a8954f-show, #collapse-a9a8954f-hide&#x27;).toggle(); $(&#x27;#collapse-a9a8954f&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-a9a8954f" style="display:none;"><pre>
###### 対象のサーバ以外から
# ping 10.16.33.81
</pre><br /><pre>
###### 対象のサーバ両方で待ち受けて来た方
# tcpdump -i bond0 host 10.16.33.81
</pre></div></li>
		<li>pingが飛んでこなかった方を停止
<a class="collapsible collapsed" href="#" id="collapse-5e2b2891-show" onclick="$(&#x27;#collapse-5e2b2891-show, #collapse-5e2b2891-hide&#x27;).toggle(); $(&#x27;#collapse-5e2b2891&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-5e2b2891-hide" onclick="$(&#x27;#collapse-5e2b2891-show, #collapse-5e2b2891-hide&#x27;).toggle(); $(&#x27;#collapse-5e2b2891&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-5e2b2891" style="display:none;"><pre>
# initctl stop pacemaker.combined
</pre></div></li>
		<li>pacemakerを停止させてもサービスが落ちない場合があるので必ず各プロセスを確認すること。</li>
		<li>停止させた側のDRBDなど初期化するなりしてクラスタへ再参加。
<a class="collapsible collapsed" href="#" id="collapse-ab71c1e5-show" onclick="$(&#x27;#collapse-ab71c1e5-show, #collapse-ab71c1e5-hide&#x27;).toggle(); $(&#x27;#collapse-ab71c1e5&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-ab71c1e5-hide" onclick="$(&#x27;#collapse-ab71c1e5-show, #collapse-ab71c1e5-hide&#x27;).toggle(); $(&#x27;#collapse-ab71c1e5&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-ab71c1e5" style="display:none;"><p>停止させた側(secondary)で<br /><pre>
# /etc/init.d/drbd start
# drbdadm down all
# drbdadm attach all
# drbdadm invalidate all
# drbdadm up all
# drbdadm connect all
</pre><br />primaryで再接続<br /><pre>
# drbdadm connect all
</pre></p></div></li>
		<li>同期が完了した後drbdサービスを停止させpacemakerを起動。</li>
	</ol>


	<p>FSシステムの故障等が発生した場合はバックアップからリストアを行う。</p>
<hr />
<a name="Drbd" />
<a name="Drbd_Drbd"></a>
<h1 >Drbd<a href="#Drbd_Drbd" class="wiki-anchor">&para;</a></h1>


<hr />


	<a name="Drbd_インストール"></a>
<h2 >インストール<a href="#Drbd_インストール" class="wiki-anchor">&para;</a></h2>


<pre>
# yum -y install http://www.elrepo.org/elrepo-release-6-6.el6.elrepo.noarch.rpm
# yum -y install drbd84-utils.x86_64 kmod-drbd84.x86_64
</pre>

	<a name="Drbd_利用まで"></a>
<h2 >利用まで<a href="#Drbd_利用まで" class="wiki-anchor">&para;</a></h2>


<pre>
# fdisk /dev/sdb #lvm領域を作成する
# pvcreate /dev/sdb1
# vgcreate drbd /dev/sdb1
# lvcreate drbd -l 100%FREE
# cat resource01.res
resource resource01 {
  device /dev/drbd0;                         &lt;- 必ずdrbd*というデバイスファイル名でなければならない。
  disk   /dev/drbd/lvol0;
  meta-disk internal;
  on kajiro1 {
    address 192.168.12.127:7788;
  }
  on kajiro2 {
    address 192.168.12.129:7788;
  }
}
</pre>

	<p>設計時検討事項</p>


	<ul>
	<li>スプリットブレイン時の挙動と回復方法</li>
		<li>データ転送プロトコル(同期、非同期)</li>
		<li>pacemakerでの管理</li>
		<li>障害時の挙動確認</li>
		<li>lvmとの関連</li>
	</ul>


	<p>スループットの測定</p>


<pre>
# cat test_drbd.sh
TEST_RESOURCE=resource01
TEST_DEVICE=$(drbdadm sh-dev $TEST_RESOURCE)
TEST_LL_DEVICE=$(drbdadm sh-ll-dev $TEST_RESOURCE)
drbdadm primary $TEST_RESOURCE
for i in $(seq 5); do
  dd if=/dev/zero of=$TEST_DEVICE bs=512M count=1 oflag=direct
done
drbdadm down $TEST_RESOURCE
for i in $(seq 5); do
  dd if=/dev/zero of=$TEST_LL_DEVICE bs=512M count=1 oflag=direct
done
# ./test_drbd.sh
1+0 records in
1+0 records out
536870912 bytes (537 MB) copied, 15.6488 s, 34.3 MB/s
1+0 records in
1+0 records out
536870912 bytes (537 MB) copied, 13.4448 s, 39.9 MB/s
1+0 records in
1+0 records out
536870912 bytes (537 MB) copied, 13.8258 s, 38.8 MB/s
1+0 records in
1+0 records out
536870912 bytes (537 MB) copied, 12.9786 s, 41.4 MB/s
1+0 records in
1+0 records out
536870912 bytes (537 MB) copied, 13.9119 s, 38.6 MB/s
resource01: State change failed: (-12) Device is held open by someone
additional info from kernel:
failed to demote
Command 'drbdsetup-84 down resource01' terminated with exit code 11
1+0 records in
1+0 records out
536870912 bytes (537 MB) copied, 6.35292 s, 84.5 MB/s
1+0 records in
1+0 records out
536870912 bytes (537 MB) copied, 6.70686 s, 80.0 MB/s
1+0 records in
1+0 records out
536870912 bytes (537 MB) copied, 6.36364 s, 84.4 MB/s
1+0 records in
1+0 records out
536870912 bytes (537 MB) copied, 6.34128 s, 84.7 MB/s
1+0 records in
1+0 records out
536870912 bytes (537 MB) copied, 6.52361 s, 82.3 MB/s
</pre>

	<p>待ち時間の測定</p>


<pre>
# cat test_drbd_.sh
TEST_RESOURCE=resource01
TEST_DEVICE=$(drbdadm sh-dev $TEST_RESOURCE)
TEST_LL_DEVICE=$(drbdadm sh-ll-dev $TEST_RESOURCE)
drbdadm primary $TEST_RESOURCE
dd if=/dev/zero of=$TEST_DEVICE bs=512 count=1000 oflag=direct
drbdadm down $TEST_RESOURCE
dd if=/dev/zero of=$TEST_LL_DEVICE bs=512 count=1000 oflag=direct

# ./test_drbd_.sh
1000+0 records in
1000+0 records out
512000 bytes (512 kB) copied, 0.733595 s, 698 kB/s
resource01: State change failed: (-12) Device is held open by someone
additional info from kernel:
failed to demote
Command 'drbdsetup-84 down resource01' terminated with exit code 11
1000+0 records in
1000+0 records out
512000 bytes (512 kB) copied, 0.36626 s, 1.4 MB/s
</pre>
<hr />
<a name="Eqltune" />
<a name="Eqltune_Eqltune"></a>
<h1 >Eqltune<a href="#Eqltune_Eqltune" class="wiki-anchor">&para;</a></h1>


<pre>
[root@idc-ohyp05 suga]# eqltune -v
Dell EqualLogic 'eqltune' version 1.3.0 Build 388434
Copyright (c) 2010-2014 Dell Inc.

Checking your Linux system for optimal iSCSI performance...

======================================================================
Block Devices
======================================================================
To make BlockIO settings persist across reboots, our HIT kit has already
installed a default set of udev rules in:
  /etc/udev/rules.d/99-eqlsd.rules

sdb
---
/sys/block/sdb/device/timeout=60 is ok

Total for sdb: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok 

sdc
---
/sys/block/sdc/device/timeout=60 is ok

Total for sdc: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok 

sdd
---
/sys/block/sdd/device/timeout=60 is ok

Total for sdd: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok 

sde
---
/sys/block/sde/device/timeout=60 is ok

Total for sde: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok 

======================================================================
Sysctl Tunables
======================================================================
To make sysctl values persistent across reboots, they can be added
to /etc/sysctl.conf

ARP Flux
--------
IPv4 ARP ignore: A value of 1 means NICs will ignore any ARP requests
not directed explicitly to them.  The default value of 0 will allow NICs to
reply to ARP requests on ANY other NIC, which can provide false positives
to externally-originating ping tests, as well as unexpected ARP resets.

IPv4 ARP announce: A value of 2 means NICs will only announce their own IP
address, not the address of other NICs on the same subnet. Any other value
can cause ARP flux or inconsistent caches out on the network in a
multiple-NIC-per-subnet configuration.

The actual value computed by Linux is the maximum of the interface-specific
value (net.ipv4.conf.$NIC.setting) and the global override value
(net.ipv4.conf.all.setting), so set your values accordingly in
/etc/sysctl.conf.  An easy solution is to add these lines:
  # Prevent ARP Flux for multiple NICs on the same subnet:
  net.ipv4.conf.all.arp_ignore = 1
  net.ipv4.conf.all.arp_announce = 2

If you have set up source-based routing and have set up arp filtering instead
(net.ipv4.conf.*.arp_filter), you can disregard this section.

net/ipv4/conf/eth7/arp_ignore=1 is ok

net/ipv4/conf/eth7/arp_announce=2 is ok

net/ipv4/conf/eth3/arp_ignore=1 is ok

net/ipv4/conf/eth3/arp_announce=2 is ok

Total for ARP Flux: 0/4 critical, 0/4 warnings, 0/4 suggestions, 4/4 ok 

RP Filter
---------
IPv4 Return Path Filtering: A value of 0 disables reverse path filtering
and a value of 2 enables loose reverse path filtering, either of which
allows all inbound packets to be processed by the stack in a multiple-NIC-
per-subnet configuration.

The actual value used by Linux is the maximum of the interface-specific
value (net.ipv4.conf.$NIC.setting) and the global override value
(net.ipv4.conf.all.setting), so set your values accordingly in
/etc/sysctl.conf.  An easy solution is to add these lines:
  # Loosen RP Filter to alow multiple iSCSI connections
  net.ipv4.conf.all.rp_filter = 2

net/ipv4/conf/eth7/rp_filter=2 is ok

net/ipv4/conf/eth3/rp_filter=2 is ok

Total for RP Filter: 0/2 critical, 0/2 warnings, 0/2 suggestions, 2/2 ok 

Network Buffers
---------------
Increasing the default and maximum network buffer sizes will provide
better performance, especially on systems with large RAM resources.

Suggestion: net.core.rmem_default=124928 is ok, but may be tuned to any value greater than 129024
    Default socket RX buffer size

Suggestion: net.core.rmem_max=124928 is ok, but may be tuned to any value greater than 131071
    Maximum socket RX buffer size

Suggestion: net.core.wmem_default=124928 is ok, but may be tuned to any value greater than 129024
    Default socket TX buffer size

Suggestion: net.core.wmem_max=124928 is ok, but may be tuned to any value greater than 131071
    Maximum socket TX buffer size

Suggestion: net.ipv4.tcp_rmem[0]=4096 is ok, but may be tuned to any value greater than 4096
    IPv4 TCP minimum socket RX buffer size

Suggestion: net.ipv4.tcp_rmem[1]=87380 is ok, but may be tuned to any value greater than 87380
    IPv4 TCP default socket RX buffer size

Suggestion: net.ipv4.tcp_rmem[2]=4194304 is ok, but may be tuned to any value greater than 4194304
    IPv4 TCP maximum socket RX buffer size

Suggestion: net.ipv4.tcp_wmem[0]=4096 is ok, but may be tuned to any value greater than 4096
    IPv4 TCP minimum socket TX buffer size

Suggestion: net.ipv4.tcp_wmem[1]=16384 is ok, but may be tuned to any value greater than 16384
    IPv4 TCP default socket TX buffer size

Suggestion: net.ipv4.tcp_wmem[2]=4194304 is ok, but may be tuned to any value greater than 4194304
    IPv4 TCP maximum socket TX buffer size

Total for Network Buffers: 0/10 critical, 0/10 warnings, 10/10 suggestions, 0/10 ok 

Scheduler
---------
Suggestion: kernel.sched_compat_yield=0 is ok, but may be tuned to one of ['0', '1']
    Setting this to 1 will grant more CPU to high-CPU tasks, at the cost
    of other non-CPU intensive tasks.

Total for Scheduler: 0/1 critical, 0/1 warnings, 1/1 suggestions, 0/1 ok 

======================================================================
Ethernet Devices
======================================================================

eth7
----
eth7 Generic Receive Offload=off is ok

eth7 Flow control=on is ok

eth7 MTU=9000 is ok

Total for eth7: 0/3 critical, 0/3 warnings, 0/3 suggestions, 3/3 ok 

eth3
----
eth3 Generic Receive Offload=off is ok

eth3 Flow control=on is ok

eth3 MTU=9000 is ok

Total for eth3: 0/3 critical, 0/3 warnings, 0/3 suggestions, 3/3 ok 

======================================================================
iSCSI Settings
======================================================================
The default settings in /etc/iscsi/iscsid.conf are propagated to
individual nodes on discovery or redescovery.

The following command will re-discover all existing nodes (Warning: resets
any per-node settings back to the defaults in iscsid.conf, including
'node.startup'):
  iscsiadm -m discovery -t st -p &lt;portal&gt;

iscsid.conf defaults
--------------------
These settings must be manually edited in /etc/iscsi/iscsid.conf, but
will only take effect for newly-discovered nodes.

Warning: node.startup=automatic, should be manual
    If node.startup is 'automatic', ALL discovered nodes will be logged
    in at boot.  If by default this is 'manual', you can designate only those
    nodes you actually want to auto-login on a case-by-case basis with ehcmcli:
      ehcmcli login --login-at-boot --target &lt;target&gt; [--portal &lt;portal&gt;]

node.session.iscsi.FastAbort=No is ok

Warning: node.session.initial_login_retry_max=8, should be 12
    More retries will make it more likely that login will succeed at boot,
    at the cost of a slighly longer time to actually fail.

Suggestion: node.conn[0].iscsi.MaxRecvDataSegmentLength=262144 is ok, but may be tuned to between 65536 and 524288
    A lower value improves latency at the cost of higher IO throughput

Warning: node.session.cmds_max=128, should be 1024
    Maximum number of queued iSCSI commands per session.
    Must be an even power of 2.

Warning: node.session.queue_depth=32, should be 128
    The device queue depth

node.conn[0].timeo.noop_out_interval=5 is ok

Total for iscsid.conf defaults: 0/7 critical, 4/7 warnings, 1/7 suggestions, 2/7 ok 

======================================================================
External Utility Settings
======================================================================
A collection of miscellaneous settings in important external utilities, such
as LVM and Multipathd.

Blacklists
----------
LVM device filter=set is ok

Total for Blacklists: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok 

======================================================================
EqualLogic Host Tools
======================================================================
This summarizes whether the EqualLogic host tools have detected warnings
or errors on the running system.

Running system checks
---------------------
To see more details and an explanation of how to fix these runtime issues,
please issue the command:
  ehcmcli status

dm-switch module=present is ok

ehcmd warnings=none is ok

Total for Running system checks: 0/2 critical, 0/2 warnings, 0/2 suggestions, 2/2 ok 

eqlvolume checks
----------------
The eqlvolume utility locates mount points that are good candidates for rethinning
and locates mount points that have been incorrectly mounted with the "-o discard" 
option.  For more information about these issues, please enter:
  eqlvolume help

eqlvolume group access check=0 is ok

eqlvolume rethin recommendations=0 is ok

eqlvolume discard usage errors=0 is ok

Total for eqlvolume checks: 0/3 critical, 0/3 warnings, 0/3 suggestions, 3/3 ok 

======================================================================
Overall: 0/40 critical, 4/40 warnings, 12/40 suggestions, 24/40 ok
======================================================================
[root@idc-ohyp05 suga]# 

</pre>

<hr />


	<a name="Eqltune_チューニング"></a>
<h2 >チューニング<a href="#Eqltune_チューニング" class="wiki-anchor">&para;</a></h2>
<hr />
<a name="Fio" />
<a name="Fio_Fio"></a>
<h1 >Fio<a href="#Fio_Fio" class="wiki-anchor">&para;</a></h1>


	<p>設定ファイルサンプル</p>


<pre>
# cat fio.conf 
### direct is 0, use memory buffer 
[global]
ioengine=libaio
direct=1
directory=/tmp
filename=fio.bin
size=1G
bs=4k

[seq read 4k 1G]
rw=read

#[seq write 4k 1G]
#rw=write

#[seq read write 4k 1G]
#rw=readwrite

#[randum read 4k 1G]
#rw=randread

#[randum write 4k 1G]
#rw=randwrite

#[randum read write 4k 1G]
#rw=randrw

</pre>
<hr />
<a name="Firewall" />
<a name="Firewall_firewallHA構築メモ"></a>
<h2 >firewall(HA)構築メモ<a href="#Firewall_firewallHA構築メモ" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Firewall_Firewall設定"></a>
<h3 >Firewall設定<a href="#Firewall_Firewall設定" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>rethカウント数の設定<br /><pre>
root@idc-opfw01# set chassis cluster reth-count 12
</pre></li>
	</ul>


	<ul>
	<li>redundancy groupの設定<br /><pre>
root@idc-opfw01# set chassis cluster redundancy-group 1 node 0  priority 200
root@idc-opfw01# set chassis cluster redundancy-group 1 node 1  priority 100
</pre></li>
	</ul>


	<ul>
	<li>rethインターフェースの作成<br /><pre>
root@idc-opfw01# set interfaces reth0 vlan-tagging
root@idc-opfw01# set interfaces reth0 redundant-ether-options redundancy-group 1
oot@idc-opfw01# set interfaces reth0 redundant-ether-options lacp active
oot@idc-opfw01# set interfaces reth0 redundant-ether-options lacp periodic fast
root@idc-opfw01# set interfaces reth0 unit 0 vlan-id 20
root@idc-opfw01# set interfaces reth0 unit 0 family inet address 10.16.36.254/24
root@idc-opfw01# set chassis cluster redundancy-group 1 interface-monitor ge-0/0/4 weight 255
root@idc-opfw01# set chassis cluster redundancy-group 1 interface-monitor ge-5/0/4 weight 255
</pre></li>
	</ul>


	<ul>
	<li>セキュリティゾーンの作成<br /><pre>
root@idc-opfw01# set security zones security-zone cloud_service_internet interfaces reth0
root@idc-opfw01# set security zones security-zone Dialer5 interfaces reth2
</pre></li>
	</ul>


	<ul>
	<li>セキュリティポリシーの作成<br /><pre>
set security policies from-zone cloud_service_internet to-zone Dialer5 policy permit_all match source-address any
set security policies from-zone cloud_service_internet to-zone Dialer5 policy permit_all match destination-address any
set security policies from-zone cloud_service_internet to-zone Dialer5 policy permit_all match application any
set security policies from-zone cloud_service_internet to-zone Dialer5 policy permit_all then permit
set security policies from-zone cloud_service_internet to-zone Dialer5 policy permit_all then count
set security policies from-zone Dialer5 to-zone cloud_service_internet policy permit_all match source-address any
set security policies from-zone Dialer5 to-zone cloud_service_internet policy permit_all match destination-address any
set security policies from-zone Dialer5 to-zone cloud_service_internet policy permit_all match application any
set security policies from-zone Dialer5 to-zone cloud_service_internet policy permit_all then permit
set security policies from-zone Dialer5 to-zone cloud_service_internet policy permit_all then count
</pre></li>
	</ul>


	<ul>
	<li>バーチャルルータの作成<br /><pre>
root@idc-opfw01# set routing-instances cloud_service_internet instance-type virtual-router
root@idc-opfw01# set routing-instances cloud_service_internet interface reth0
root@idc-opfw01# set routing-instances cloud_service_internet interface reth2
</pre></li>
	</ul>


	<a name="Firewall_L３スイッチ側仮想ルータの作成設定"></a>
<h3 >L３スイッチ側　仮想ルータの作成/設定<a href="#Firewall_L３スイッチ側仮想ルータの作成設定" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>FW接続用仮想ルータの作成<br /><pre>
root@idc-opfw01# set routing-instances RI_Dialer5 instance-type virtual-router
</pre></li>
	</ul>


	<ul>
	<li>vlanインターフェースの作成＆仮想ルータへのvlanマッピング<br /><pre>
root@idc-opfw01# set interfaces vlan unit 20 family inet address 10.16.36.1/24
root@idc-opfw01# set vlans vlan20 vlan-id 20
root@idc-opfw01# set vlans vlan20 l3-interface vlan.20

root@idc-opfw01# set routing-instances RI_Dialer5 interface vlan.20
</pre></li>
	</ul>


	<ul>
	<li>ISBクラウドデフォルトネットワーク用仮想ルータの作成<br /><pre>
root@idc-opfw01# set routing-instances RI_isb-cloud instance-type virtual-router
</pre></li>
	</ul>


	<ul>
	<li>vlanインターフェースの作成＆仮想ルータへのvlanマッピング<br /><pre>
root@idc-opfw01# set interfaces vlan unit 19 family inet address 10.16.35.254/24
root@idc-opfw01# set vlans vlan19 vlan-id 19 
root@idc-opfw01# set vlans vlan19 l3-interface vlan.19

root@idc-opfw01# set routing-instances RI_isb-cloud interface vlan.19
</pre></li>
	</ul>


	<ul>
	<li>FW接続用仮想ルータに適用するファイアウォールフィルタの作成<br /><pre>
root@idc-opfw01# set firewall family inet filter FL_Dialer5-to-isb-cloud term TM_isb-cloud from destination-address 10.16.35.0/24
root@idc-opfw01# set firewall family inet filter FL_Dialer5-to-isb-cloud term TM_isb-cloud then routing-instance RI_isb-cloud
root@idc-opfw01# set firewall family inet filter FL_Dialer5-to-isb-cloud term TM_default then accept
</pre></li>
	</ul>


	<ul>
	<li>ISBクラウドデフォルトネットワーク用仮想ルータに適用するファイアウォールフィルタの作成<br /><pre>
root@idc-opfw01# set firewall family inet filter FL_isb-cloud term TM_internal from destination-address 10.0.0.0/8
root@idc-opfw01# set firewall family inet filter FL_isb-cloud term TM_internal from destination-address 172.16.0.0/12
root@idc-opfw01# set firewall family inet filter FL_isb-cloud term TM_internal from destination-address 192.168.0.0/16
root@idc-opfw01# set firewall family inet filter FL_isb-cloud term TM_internal then accept
root@idc-opfw01# set firewall family inet filter FL_isb-cloud term TM_Dialer5 then routing-instance RI_Dialer5
</pre></li>
	</ul>


	<a name="Firewall_L2スイッチ側vlanの作成"></a>
<h3 >L2スイッチ側　vlanの作成<a href="#Firewall_L2スイッチ側vlanの作成" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>vlan作成<br /><pre>
root@idc-opfw01# vlans vlan20 vlan-id 20
root@idc-opfw01# vlans vlan19 vlan-id 19
</pre></li>
	</ul>


	<ul>
	<li>ae作成<br /><pre>
set interfaces ae8 aggregated-ether-options lacp passive
set interfaces ae8 aggregated-ether-options lacp periodic fast
set interfaces ae8 unit 0 family ethernet-switching port-mode trunk
set interfaces ae8 unit 0 family ethernet-switching vlan members all
set interfaces ae8 unit 0 family ethernet-switching filter input FL_basic
set interfaces ae9 aggregated-ether-options lacp passive
set interfaces ae9 aggregated-ether-options lacp periodic fast
set interfaces ae9 unit 0 family ethernet-switching port-mode trunk
set interfaces ae9 unit 0 family ethernet-switching vlan members all
set interfaces ae9 unit 0 family ethernet-switching filter input FL_basic
</pre></li>
	</ul>


	<ul>
	<li>aeをインターフェースにマッピング<br /><pre>
set interfaces ge-0/0/28 ether-options 802.3ad ae8
set interfaces ge-0/0/30 ether-options 802.3ad ae9
set interfaces ge-1/0/28 ether-options 802.3ad ae8
set interfaces ge-1/0/30 ether-options 802.3ad ae9
</pre></li>
	</ul>


	<ul>
	<li>QoS設定<br /><pre>
set class-of-service interfaces ae8 scheduler-map SMAP_management
set class-of-service interfaces ae8 unit 0 forwarding-class CLS_management
set class-of-service interfaces ae9 scheduler-map SMAP_service
set class-of-service interfaces ae9 unit 0 forwarding-class CLS_service
</pre></li>
	</ul>
<hr />
<a name="Hadoop設計" />
<a name="Hadoop設計_Hadoop設計"></a>
<h1 >Hadoop設計<a href="#Hadoop設計_Hadoop設計" class="wiki-anchor">&para;</a></h1>


	<a name="Hadoop設計_かなっぺ作本番構築手順メモ"></a>
<h2 >かなっぺ作　本番構築手順メモ<a href="#Hadoop設計_かなっぺ作本番構築手順メモ" class="wiki-anchor">&para;</a></h2>


	<p><a class="collapsible collapsed" href="#" id="collapse-2d82ce82-show" onclick="$(&#x27;#collapse-2d82ce82-show, #collapse-2d82ce82-hide&#x27;).toggle(); $(&#x27;#collapse-2d82ce82&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-2d82ce82-hide" onclick="$(&#x27;#collapse-2d82ce82-show, #collapse-2d82ce82-hide&#x27;).toggle(); $(&#x27;#collapse-2d82ce82&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-2d82ce82" style="display:none;"><p>=================================<br />本構築手順書について
================================</p>


	<p>Hadoop &#38; ZooKeeper クラスタ環境の構築手順概要となる。<br />個々のコマンド詳細は必要に応じて別途用意し、ここでは主軸となる要素を記載する。</p>


	<p>=================================<br />名称について
=================================<br />本手順書内のHadoop関連デーモンの名称は以下の通りとする。</p>


	<p>NN: NameNode<br />DN: DataNode<br />JN: JournalNode<br />JT: JobTracker<br />TT: TaskTacker<br />HM: HBase Mastr<br />HR: HBase Regionserver<br />ZK: ZooKeeper</p>


	<p>NN,JT,HM,ZKが動作する管理サーバをマスタ、<br />DN,TT,HRが動作する実行サーバをスレーブとする。</p>


	<p>ジャーナルノード専用マシンではZKとJNが稼働する。</p>


	<p>=================================<br />構築作業対象
=================================</p>


	<p>マスタマシン 2台<br />ジャーナルノード専用 1台<br />スレーブマシン 3もしくは4台</p>


	<p>=================================<br />作業前提
=================================</p>


	<p>1. OSレイヤー、Hadoop関連含め、設定ファイルは適切な値に編集されたものが用意されていること。一時場所として/tmpに配置しておく。必要に応じて格納ディレクトリを作成する。</p>


	<p>2. 構築は主にスクリプトを使用し、必要箇所は手動コマンドで補うこととする。<br />   スクリプトファイル名は以下の通り。処理内容は詳細は若干異なるものの、基本はOS基本設定、JDK &#38; hadoopインストール、環境    変数セット、ローカルディレクトリ作成となる。</p>


	<pre><code>primary_master_setup.sh<br />   secondary_master_setup.sh<br />   slave_setup.sh<br />   journalnode_setup.sh</code></pre>


	<pre><code>hdfs-init.sh ※HDFS上の初期ディレクトリ作成。アクティブマスタで実行。</code></pre>


	<p>　<br />　マスタマシンでのスクリプト実行場所は/root/workとする。<br />　同じディレクトリ内に、プライマリマスタ以外のホスト名を列挙したファイルallhostと、スレーブマシンホスト名を列挙したファイルslavehostが存在すること。<br />　</p>


	<p>=================================<br />OS基本設定
=================================</p>


	<p>以下の対応はクラスタ内全てのマシンで共通とし、スクリプト内または手動にて実行する。<br />本手順書では概要のみ述べる。</p>


	<p>1. SELinix無効化。<br />2. 2台のマスタマシンから、スレーブ、ジャーナルノードサーバに鍵認証によるSSHログイン（root権限、パスフレーズなし）が実行できるようセットする。<br />   すべてのマシンにopenssh-clientsをインストールする必要がある。</p>


	<p>3. /etc/hostsファイルにマスタ・スレーブ・ZKに対応するホスト名&#38;IPアドレスを記述。<br />　*IPアドレスはプライベートIPを記述する。<br />　*すべてのマシンで共通の内容にすること。</p>


	<p>（以降はスクリプト内にて実施）</p>


	<p>4. iptables停止、自動起動Off。<br />5. NTPデーモンの起動。</p>


	<p>6. 以下ファイル編集により、各種ユーザのファイルディスクリプタ、カーネルパラメータを変更する。<br />　</p>


	<pre><code>/etc/security/limits.conf<br />    /etc/sysctl.conf</code></pre>


	<p>　  カーネルパラメータ変更反映にはsysctl -pを実行する。</p>


	<p>7. 基本ツールのインストール<br />　 初期状態ではbind-utils、wgetなど必要なツールが入っていないのでインストール。</p>


	<p>8. JDKのインストール</p>


	<p>9. Java環境変数の読み込み<br />　 /etc/profile.d/java.shに以下記述し、root含め各Userの.bashrcに読み込ませる。</p>
	<pre><code>export JAVA_HOME=/usr/java/jdk1.7.0_11<br />    export PATH=$JAVA_HOME/bin:$PATH<br />    export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar</code></pre>


	<pre><code>.bashrcに以下追記<br />    . /etc/profile.d/java.sh</code></pre>


	<p>=================================<br />Hadoop HAクラスタ構築概要
=================================</p>


	<p>以下の順に作業を進める。</p>


	<p>1. プライマリマスタマシンでprimary_master_setup.sh実行。<br />2. セカンダリマスタマシンでsecondary_master_setup.sh実行。</p>


	<pre><code>- 手動でhdfsユーザのSSH鍵認証セッティングを行う。</code></pre>


	<p>3. ジャーナルノードマシンでjournalnode_setup.sh実行。<br />4. スレーブマシンでslave_setup.sh実行。</p>


	<pre><code>- プライマリマスタのHadoop関連設定ファイルを全てのマシンに配布する。</code></pre>


	<p>この時点で全てのマシンが以下の状態となる。</p>


	<pre><code>- JDK &#38; Hadoop関連サービスがインストール済。<br />  - Java, Hadoop周りの環境変数が整っている。<br />  - 適宜設定ファイルが置かれている。<br />  - 適宜ローカルディレクトリが作成されている。</code></pre>


	<p>5. 上記セットアップ完了後、以下の順に初期化 &#38; デーモン起動を行う。</p>


	<pre><code>1) ZK初期化＆ 起動（マスタマシン＋ジャーナルノード専用マシンの3台）</code></pre>


	<ol>
	<li>service zookeeper-server init --myid=1</li>
		<li>service zookeeper-server init --myid=2</li>
		<li>service zookeeper-server init --myid=3</li>
	</ol>


	<ol>
	<li> service zookeeper-server start</li>
	</ol>


	<pre><code>2) hdfs-zkfc, mapred-zkfcの初期化実施（アクティブマスタのみ）</code></pre>


	<ol>
	<li>sudo -u hdfs hdfs zkfc -formatZK</li>
		<li>sudo -u mapred hadoop mrzkfc -formatZK</li>
	</ol>


	<pre><code>3) hdfs-zkfcを起動(マスタ2台)</code></pre>


	<ol>
	<li>service hadoop-hdfs-zkfc start</li>
	</ol>


	<pre><code>4) JNを起動（ZKマシン3台）</code></pre>


	<ol>
	<li>service hadoop-hdfs-journalnode start</li>
	</ol>


	<pre><code>5) HDFS初期化実施 &#38; NN起動（アクティブマスタのみ）</code></pre>


	<ol>
	<li>sudo -u hdfs hdfs namenode -format</li>
		<li>service hadoop-hdfs-namenode start</li>
	</ol>


	<pre><code>6) アクティブマスタのメタ情報引き継ぎ実行 &#38; NN起動（スタンバイマスタ）</code></pre>


	<ol>
	<li>sudo -u hdfs hdfs namenode -bootstrapStandby</li>
		<li>service hadoop-hdfs-namenode start</li>
	</ol>


	<pre><code>7) NNのステータスをチェック。active/standbyが適宜返されればOK。</code></pre>


	<ol>
	<li>sudo -u hdfs hdfs haadmin -getServiceState &lt;ID_1&gt;</li>
		<li>sudo -u hdfs hdfs haadmin -getServiceState &lt;ID_2&gt;</li>
	</ol>


	<pre><code>8) HDFSの基本ディレクトリ作成するため、hdfs-init.sh スクリプトを実行</code></pre>


	<pre><code>9) 上記問題なければ、DNを起動（スレーブ3台）</code></pre>


	<ol>
	<li>for h in `cat /root/work/slavehost`; do ssh $h "service hadoop-hdfs-datanode start"; done</li>
	</ol>


	<pre><code>スレーブでログを確認し、マスタを認識していることを確認する。</code></pre>


	<pre><code>10) mapred-zkfcを起動。1号機から起動する (マスタ2台) </code></pre>


	<ol>
	<li>service hadoop-0.20-mapreduce-zkfc start</li>
	</ol>


	<pre><code>11) JTを起動。1号機から起動する (マスタ2台) </code></pre>


	<ol>
	<li>service hadoop-0.20-mapreduce-jobtrackerha start</li>
	</ol>


	<pre><code>12) JTのステータスをチェック。active/standbyが適宜返されればOK。</code></pre>


	<ol>
	<li>sudo -u mapred hadoop mrhaadmin -getServiceState &lt;ID_1&gt;</li>
		<li>sudo -u mapred hadoop mrhaadmin -getServiceState &lt;ID_2&gt;</li>
	</ol>


	<pre><code>13) 上記問題なければ、TTを起動（スレーブ3台）</code></pre>


	<ol>
	<li>for h in `cat /root/work/slavehost`; do ssh $h "service hadoop-0.20-mapreduce-tasktracker start"; done</li>
	</ol>


	<pre><code>スレーブでTTのログを確認し、マスタを認識していることを確認する。</code></pre>


	<p>6. HBase Masterの起動。1号機から起動する（マスタ2台）</p>


	<ol>
	<li>service hbase-master start</li>
	</ol>


	<p>7. HBase Regionserverの起動（スレーブ3台）</p>


	<ol>
	<li>service hbase-regionserver start</li>
	</ol>


	<pre><code>スレーブでRegionServerのログを確認し、マスタを認識していることを確認する。</code></pre></div></p>
<hr />
<a name="HTTPProxy設計" />
<a name="HTTPProxy設計_HTTPProxy設計"></a>
<h1 >HTTPProxy設計<a href="#HTTPProxy設計_HTTPProxy設計" class="wiki-anchor">&para;</a></h1>


	<p>本システムでは、各サーバが直接インターネットに接続できる環境を用意するためHTTPProxyを構築する。<br />HTTPProxyはsquidで構築する。</p>


	<a name="HTTPProxy設計_ACLについて"></a>
<h2 >ACLについて<a href="#HTTPProxy設計_ACLについて" class="wiki-anchor">&para;</a></h2>


	<p>本HTTPProxyは内部公開用のProxyのため特定のアドレス帯のみアクセスできるようにACLを記述する</p>


<pre>
acl managenet src 10.16.33.0/255.255.255.0

http_access allow localnet
http_access allow managenet
http_access allow localhost
http_access deny all
</pre>

	<a name="HTTPProxy設計_待ち受けポートを変更する"></a>
<h2 >待ち受けポートを変更する<a href="#HTTPProxy設計_待ち受けポートを変更する" class="wiki-anchor">&para;</a></h2>


<pre>
http_port 8080
</pre>

	<a name="HTTPProxy設計_アクセス元を知られないようにする"></a>
<h2 >アクセス元を知られないようにする<a href="#HTTPProxy設計_アクセス元を知られないようにする" class="wiki-anchor">&para;</a></h2>


	<p>アクセス元を隠蔽するために以下の設定を行う</p>


<pre>
forwarded_for off　←　追加(プロキシサーバーを使用している端末のローカルIPアドレスを隠蔽化)
</pre>

	<a name="HTTPProxy設計_キャッシュ機能は使用しない"></a>
<h2 >キャッシュ機能は使用しない<a href="#HTTPProxy設計_キャッシュ機能は使用しない" class="wiki-anchor">&para;</a></h2>


<pre>
no_cache deny all
</pre>
<hr />
<a name="Internet_Firewall詳細設計" />
<a name="Internet_Firewall詳細設計_Internet-Firewall詳細設計"></a>
<h1 >Internet Firewall詳細設計<a href="#Internet_Firewall詳細設計_Internet-Firewall詳細設計" class="wiki-anchor">&para;</a></h1>


	<a name="Internet_Firewall詳細設計_基本設定"></a>
<h2 >基本設定<a href="#Internet_Firewall詳細設計_基本設定" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>スイッチのWeb管理<br />　Webブラウザからの設定および管理を可能とする為、http接続設定を有効化する。</li>
		<li>NTPサーバ<br />　NTPサーバはクラウド基盤NTPサーバを指定する。</li>
		<li>シスログ管理<br />　シスログ機能を使用し、クラウド基盤のシスログサーバへログを転送する。ログレベルは、ファシリティは以下の通りとする。<br />*<strong><b></strong>*</b>**<strong>***検討中</strong>*********************</li>
		<li>管理用IPアドレス<br />　Chassis Cluster構成の物理スイッチそれぞれに対し、一つの管理用IPアドレスを付与する。</li>
	</ul>


<hr />


	<a name="Internet_Firewall詳細設計_ルーティング設計"></a>
<h2 >ルーティング設計<a href="#Internet_Firewall詳細設計_ルーティング設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>PPPoE接続<br />インターネット環境からISBクラウド設備へのインターネットゲートウェイとして機能し、LANネットワークへルーティングする。インターネット接続にはPPPoE接続を使用し本デヴァイスにて管理する。なお、インターネット回線（フレッツ）の使用上、PPpoE接続経路は物理的に1対1のインターフェースで接続する必要がある為、ONUとゲートウェイルータの間にL2スイッチをはさみ、ルーテッドポートとL2スイッチポートを1対1で結線する。以下にPPPoE接続一覧を示す。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>セッション</th>
			<th>利用ユーザ</th>
			<th> 説明</th>
		</tr>
		<tr>
			<td>pppoe-channel-4</td>
			<td>ISBクラウド設備,一部の事業者</td>
			<td>ISBクラウド設備および一部ユーザクラウド環境用のインターネットセッション。</td>
		</tr>
		<tr>
			<td>pppoe-channel-5</td>
			<td>事業者</td>
			<td>ユーザクラウド環境用のインターネットセッション。</td>
		</tr>
	</table>




	<ul>
	<li>ISBクラウド設備用のグローバルIPアドレス管理およびローカルIPアドレスへのNAT<br />ISBクラウド設備用に付与されるグローバルIPアドレスはファイアウォール自身で管理する。また、グローバルIPアドレス宛の通信をローカルIPアドレスへNATする。なお、グローバルIPアドレス帯は、ISPより提供された元々のサブネットマスクである/28をさらに分割し、/29のアドレス帯（ホストアドレス６つ）を２セット作成し使用する。一つはISBクラウド設備用として使用し、もう一つは、一部ユーザクラウド環境用に割り当てる。</li>
	</ul>


	<ul>
	<li>ユーザクラウド環境へのグローバルIPアドレスのルーティング<br />クラウド環境を利用するユーザに付与されるグローバルIPアドレスは、ユーザ毎に作成される仮想ルータ上で管理する為、グローバルIPアドレスを下位ネットワークへ「降ろす（ルーティング）」イメージとなる。また、グローバルIPアドレスからローカルアドレスへのNATも仮想ルータにて行う為、グローバルIPアドレス宛の通信をそのまま下へルーティングするだけとなる。ファイアウォール本体にグローバルIPアドレスは付与せず、仮想ルータとの通信専用のローカルネットワークを使用する。ONUとの接続ポイントとなるWAN側インターフェースも「unnumbered」として設定し、グローバルIPアドレスは付与しない。グローバルIPアドレスおよびローカルIPアドレス一覧については、「【共通】ネットワークアドレス管理台帳.xlsx」を参照。</li>
	</ul>


	<ul>
	<li>ISBクラウド設備およびユーザクラウド環境接続ネットワークの管理<br />ISBクラウド設備およびユーザクラウド環境は、それぞれ異なるPPPoEセッションを使用する為、インターネット接続の経路を分割するネットワーク設計とする。基本的にはPPPoEセッションの数だけ専用のネットワーク経路を作成することとし、さらにISBクラウド設備で使用するグローバルアドレス帯は２つに分かれる為、これも経路を分割、合計３つのインターネット接続経路を構成する。以下に構成概要を示す。</li>
	</ul>


	<p><img src="/attachments/download/1355/%E3%83%AB%E3%83%BC%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0%E6%A7%8B%E6%88%90%E6%A6%82%E8%A6%81.png" alt="" /></p>


<hr />


	<a name="Internet_Firewall詳細設計_侵入検知設計"></a>
<h3 >侵入検知設計<a href="#Internet_Firewall詳細設計_侵入検知設計" class="wiki-anchor">&para;</a></h3>


	<p>IPS/IDS機能により、インターネットからの不審なアクセスを検知する。なお、本デヴァイスでは「検知」機能のみとし、遮断は行わない。</p>


<hr />


	<a name="Internet_Firewall詳細設計_インターフェース設計"></a>
<h2 >インターフェース設計<a href="#Internet_Firewall詳細設計_インターフェース設計" class="wiki-anchor">&para;</a></h2>


	<p>本デヴァイスを構成する上でのインターフェースに対する各種設計を以下の通り定義する。なお、インターフェースの各設計は後述するデヴァイスのクラスタ化を前提とした設計とする。</p>


	<ul>
	<li>物理インターフェース種別<br />本デヴァイスにおける物理インターフェース種別を以下の通り定義する。また、本デヴァイスでは物理インターフェースの種別（メディアタイプ）毎に表記が異なる為、以下の一覧に併せて記載する。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>インターフェース種別</th>
			<th>メディアタイプ</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>コントロールリンク(HAリンク)</td>
			<td>fxp1</td>
			<td>クラスタ内のノード間におけるクラスタ状態、コンフィグ情報通信用ポート。デヴァイス間でクロス接続する。</td>
		</tr>
		<tr>
			<td>ファブリックリンク</td>
			<td>fab*</td>
			<td>クラスタ内のノード間でデータプレーン情報とトラフィックを交換する為のポート。デヴァイス間でクロス接続する。</td>
		</tr>
		<tr>
			<td>管理用ポート</td>
			<td>fxp0</td>
			<td>アクティブデヴァイス,バックアップデヴァイスそれぞれに設定する管理アクセス用ポート。</td>
		</tr>
		<tr>
			<td>アップリンク</td>
			<td>ge-*/*/*</td>
			<td>上位ネットワーク機器に接続されるポート。インターネット環境へ接続する為のPPPoE接続用インターフェースとし、冗長化ポートとして構成する。</td>
		</tr>
		<tr>
			<td>ダウンリンク</td>
			<td>ge-*/*/*</td>
			<td>下位ネットワーク機器に接続されるポート。ファイアウォールルールで許可されたトラフィックを下位のサーバへ転送する為の経路。冗長化ポートとして構成する。</td>
		</tr>
	</table>




	<ul>
	<li>インターフェース冗長化<br />サービス系ネットワークの通信経路となるアップリンク/ダウンリンクについては、インターフェース冗長化技術である「リダンダントインターフェース(reth」を使用する。rethは仮想インターフェースとして作成され、冗長化インターフェースとして定義したいそれぞれの機器の物理インターフェース(ge-*/*/*)にバインドし使用する。なお、本デヴァイスで使用可能なreth数は「12」とする。また、それぞれのrethは後述する機器冗長化と組み合わせて使用する目的で「rethグループ」として一つにまとめる。以下にreth一覧を示す。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>reth名</th>
			<th>rethグループNumber</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>reth0</td>
			<td>1</td>
			<td>クラウドサービス用インターネットに繋がるインタフェースに適用する。</td>
		</tr>
		<tr>
			<td>reth1</td>
			<td>1</td>
			<td>ユーザ用インターネットに繋がるインターフェースに適用する。</td>
		</tr>
		<tr>
			<td>reth2</td>
			<td>1</td>
			<td>ISBクラウド設備PPPoE接続用インターフェースに適用する。</td>
		</tr>
		<tr>
			<td>reth3</td>
			<td>1</td>
			<td>ユーザクラウド環境PPPoE接続用インターフェースに適用する。</td>
		</tr>
		<tr>
			<td>reth4</td>
			<td>1</td>
			<td>ユーザ用インターネットに繋がるインターフェースに適用する。</td>
		</tr>
	</table>




	<ul>
	<li>vlan<br />本デヴァイスで作成するvlan、利用用途を以下に示す。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>vlan-id</th>
			<th>ネットワーク種別</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>20</td>
			<td>クラウドサービス用インターネット</td>
			<td>ISBクラウド設備がインターネットへ接続する為のネットワーク。仮想ルータが持つデフォルトネットワークからNATされた後のネットワーク。</td>
		</tr>
		<tr>
			<td>21</td>
			<td>ユーザ用インターネット </td>
			<td>クラウド事業者がインターネットへ接続する為のネットワーク。仮想ルータが持つデフォルトネットワークからNATされた後のネットワーク。</td>
		</tr>
		<tr>
			<td>93</td>
			<td>ユーザ用インターネット </td>
			<td>クラウド事業者がインターネットへ接続する為のネットワーク。仮想ルータが持つデフォルトネットワークからNATされた後のネットワーク。</td>
		</tr>
		<tr>
			<td>2051</td>
			<td>ISBクラウド設備PPPoE接続用ネットワーク</td>
			<td>上位L2スイッチと接続する為のネットワーク。その経路のみに対して適用されるvlanを持ち、ONUと1対1で通信する。</td>
		</tr>
		<tr>
			<td>2048</td>
			<td>ユーザクラウド環境PPPoE接続用ネットワーク</td>
			<td>上位L2スイッチと接続する為のネットワーク。その経路のみに対して適用されるvlanを持ち、ONUと1対1で通信する。</td>
		</tr>
		<tr>
			<td>17</td>
			<td>管理用ネットワーク</td>
			<td>物理機器への管理アクセス用ネットワーク。他のネットワーク機器と同一の管理用vlan。</td>
		</tr>
	</table>




	<ul>
	<li>ポートアサイン<br />　上位インターフェース設計を投手した各インターフェースに対するポートアサインは以下の通りとする。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ネットワーク種別</th>
			<th>メディアタイプ</th>
			<th>アクティブ側割り当てインターフェース</th>
			<th>パッシヴ側割り当てインターフェース</th>
			<th>vlan</th>
			<th>備考</th>
		</tr>
		<tr>
			<td>コントロールリンク</td>
			<td>fxp1</td>
			<td>ge-0/0/1</td>
			<td>ge-5/0/1</td>
			<td>-</td>
			<td>Chassis Cluster有効後にシステムによって自動的にマッピングされる。</td>
		</tr>
		<tr>
			<td>ファブリックリンク</td>
			<td>fab0(アクティブ),fab1(バックアップ)</td>
			<td>ge-0/0/2,ge-0/0/3</td>
			<td>ge-5/0/2,ge-5/0/3</td>
			<td>-</td>
			<td>任意の2ポートを手動でマッピングする。</td>
		</tr>
		<tr>
			<td>管理用ポート</td>
			<td>fxp0</td>
			<td>ge-0/0/0</td>
			<td>ge-5/0/0</td>
			<td>-</td>
			<td>Chassis Cluster有効後にシステムによってマッピングされる。IPアドレスを手動で設定することでそれぞれのデヴァイスへ接続可能となる。</td>
		</tr>
		<tr>
			<td>クラウドサービス用インターネット</td>
			<td>reth0</td>
			<td>ge-0/0/4</td>
			<td>ge-5/0/4</td>
			<td>20</td>
			<td></td>
		</tr>
		<tr>
			<td>ユーザ用インターネット</td>
			<td>reth1</td>
			<td>ge-0/0/5</td>
			<td>ge-5/0/5</td>
			<td>21</td>
			<td></td>
		</tr>
		<tr>
			<td>ISBクラウド設備PPPoE接続用ネットワーク</td>
			<td>reth2</td>
			<td>ge-0/0/15</td>
			<td>ge-5/0/15</td>
			<td>2051</td>
			<td></td>
		</tr>
		<tr>
			<td>ISBクラウド設備PPPoE接続用ネットワーク</td>
			<td>reth3</td>
			<td>ge-0/0/14</td>
			<td>ge-5/0/14</td>
			<td>2048</td>
			<td></td>
		</tr>
		<tr>
			<td>ユーザ用インターネット</td>
			<td>reth4</td>
			<td>ge-0/0/6</td>
			<td>ge-5/0/6</td>
			<td>93</td>
			<td></td>
		</tr>
	</table>




<hr />
	<ul>
	<li>その他<br />IPアドレス、対向インターフェース等のインターフェースに関わるその他情報については、「【共通】ネットワークアドレス管理台帳.xlsx」参照。</li>
	</ul>


<hr />


	<a name="Internet_Firewall詳細設計_ゾーン設計"></a>
<h2 >ゾーン設計<a href="#Internet_Firewall詳細設計_ゾーン設計" class="wiki-anchor">&para;</a></h2>


	<p>本デヴァイスの主機能であるインターネット⇔LAN間の通信を制御する「セキュリティポリシー」に関わる各設計を定義する。</p>


	<ul>
	<li>セキュリティゾーン<br />SRXシリーズでは仮想的なインターフェースグループであるセキュリティゾーンを作成し、rethインターフェースへゾーンをマッピングする。本デヴァイスで作成するセキュリティゾーン一覧を以下に示す。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>セキュリティゾーン名</th>
			<th>ゾーン種別</th>
			<th>割り当てインターフェース</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>pppoe_channel-4</td>
			<td>Untrust</td>
			<td>reth2.0</td>
			<td>ISBクラウド設備および一部ユーザクラウド環境PPPoE接続に使用するWAN側のUntrustゾーン</td>
		</tr>
		<tr>
			<td>pppoe_channel-5</td>
			<td>Untrust</td>
			<td>reth3.0</td>
			<td>ユーザクラウド環境PPPoE接続に使用するWAN側のUntrustゾーン</td>
		</tr>
		<tr>
			<td>cloud_service_internet</td>
			<td>Trust</td>
			<td>reth0.0</td>
			<td>クラウドサービス用インターネットと繋がるTrustゾーン</td>
		</tr>
		<tr>
			<td>user_internet</td>
			<td>Trust</td>
			<td>reth1.0</td>
			<td>ユーザ用インターネットと繋がるTrustゾーン</td>
		</tr>
		<tr>
			<td>user_internet_202.215.185</td>
			<td>Trust</td>
			<td>reth4.0</td>
			<td>202.215.185のグローバルIPアドレスを持つユーザ用インターネットと繋がるTrustゾーン</td>
		</tr>
	</table>




<hr />


	<a name="Internet_Firewall詳細設計_仮想ルータ設計"></a>
<h2 >仮想ルータ設計<a href="#Internet_Firewall詳細設計_仮想ルータ設計" class="wiki-anchor">&para;</a></h2>


	<p>本デヴァイスでは仮想ルータを１つ作成する。仮想ルータにはrethインターフェースを割り当て、インターネット⇔LAN間の中継経路とし、通信は仮想ルータ個別に適用されたルーティングテーブルに従いルーティングを行う。なお、RI_pppoe_channel-4に紐付られるネットワークは仮想ルータ化せず、物理機器上での管理とする。</p>


	<table>
		<tr style="background:#d3eaf3;">
			<th>仮想ルータ名</th>
			<th>割り当てインターフェース</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>RI_pppoe_channel-5</td>
			<td>reth3,reth4</td>
			<td>ユーザクラウド環境PPPoEセッション用の仮想ルータ。</td>
		</tr>
	</table>




	<p><img src="/attachments/download/1371/%E4%BB%AE%E6%83%B3%E3%83%AB%E3%83%BC%E3%82%BF.png" alt="" /></p>


<hr />


	<a name="Internet_Firewall詳細設計_セキュリティポリシー設計"></a>
<h2 >セキュリティポリシー設計<a href="#Internet_Firewall詳細設計_セキュリティポリシー設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>セキュリティポリシー種別<br />本デヴァイスにおけるセキュリティポリシーは、前述の通り「セキュリティゾーン間」で作成する。セキュリティポリシー一覧を以下に示す。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>セキュリティポリシー一覧</th>
			<th>送信元ゾーン</th>
			<th>宛先ゾーン</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>pppoe_channel-4_to_cloud_service_internet</td>
			<td>pppoe_channel-4</td>
			<td>cloud_service_internet</td>
			<td>インターネットからISBクラウド設備の公開サーバへアクセスする際のポリシー</td>
		</tr>
		<tr>
			<td>pppoe_channel-4_to_user_internet</td>
			<td>pppoe_channel-4</td>
			<td>user_internet</td>
			<td>インターネットからユーザクラウド環境の公開サーバへアクセスする際のポリシー</td>
		</tr>
		<tr>
			<td>pppoe_channel-5_to_user_internet_202.215.185</td>
			<td>pppoe_channel-5</td>
			<td>user_internet</td>
			<td>インターネットからユーザクラウド環境の公開サーバへアクセスする際のポリシー</td>
		</tr>
		<tr>
			<td>cloud_service_internet_to_pppoe_channel-4</td>
			<td>cloud_service_internet</td>
			<td>pppoe_channel-4</td>
			<td>ISBクラウド設備からインターネットへアクセスする際のポリシー</td>
		</tr>
		<tr>
			<td>user_internet_to_pppoe_channel-4</td>
			<td>user_internet</td>
			<td>pppoe_channel-4</td>
			<td>ユーザクラウド環境からインターネットへアクセスする際のポリシー</td>
		</tr>
		<tr>
			<td>user_internet_202.215.185_to_pppoe_channel-5</td>
			<td>user_internet</td>
			<td>pppoe_channel-5</td>
			<td>ユーザクラウド環境からインターネットへアクセスする際のポリシー</td>
		</tr>
		<tr>
			<td>user_internet_to_cloud_service_internet</td>
			<td>user_internet</td>
			<td>cloud_service_internet</td>
			<td>ユーザクラウド環境からISBクラウド設備へのDNS,NTP,DHCPサーバへアクセスする際のポリシー。このポリシーはpppoe_channel-4/5共通で使用する。</td>
		</tr>
	</table>




	<ul>
	<li>セキュリティポリシー一覧<br />上記種別毎のセキュリティポリシーは、別紙「Firewallセキュリティポリシー一覧.xlsx」を参照。</li>
	</ul>


<hr />


	<a name="Internet_Firewall詳細設計_スタティックルート設計"></a>
<h2 >スタティックルート設計<a href="#Internet_Firewall詳細設計_スタティックルート設計" class="wiki-anchor">&para;</a></h2>


	<p>本環境では、仮想ルータ毎に個別のスタティックルートを定義する。基本的にはデフォルトルートおよび下位ネットワークの仮想ルータへのスタティックルート定義となるが、一部仮想ルータを跨いだ通信（DNS,NTP,DHCP）が必要となる。以下にスタティックルート一覧を示す。</p>


	<table>
		<tr style="background:#d3eaf3;">
			<th>適用箇所</th>
			<th>宛先</th>
			<th>nex-hop</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>デフォルトテーブル</td>
			<td>0.0.0.0/0</td>
			<td>pp0.0</td>
			<td>ISBクラウド設備用のデフォルトルート</td>
		</tr>
		<tr>
			<td>デフォルトテーブル</td>
			<td>10.16.35.0/24</td>
			<td>10.16.36.1</td>
			<td>DNS,NTP,DHCPサーバへのルート</td>
		</tr>
		<tr>
			<td>デフォルトテーブル</td>
			<td>122.103.89.74/32</td>
			<td>10.16.37.14</td>
			<td>事業者14の仮想ルータへのルート</td>
		</tr>
		<tr>
			<td>デフォルトテーブル</td>
			<td>122.103.89.75/32</td>
			<td>10.16.37.14</td>
			<td>事業者14の仮想ルータへのルート</td>
		</tr>
		<tr>
			<td>デフォルトテーブル</td>
			<td>122.103.89.76/32</td>
			<td>10.16.37.15</td>
			<td>事業者15の仮想ルータへのルート</td>
		</tr>
		<tr>
			<td>デフォルトテーブル</td>
			<td>122.103.89.77/32</td>
			<td>10.16.37.16</td>
			<td>事業者16の仮想ルータへのルート</td>
		</tr>
		<tr>
			<td>デフォルトテーブル</td>
			<td>122.103.89.78/32</td>
			<td>10.16.37.16</td>
			<td>事業者16の仮想ルータへのルート</td>
		</tr>
		<tr>
			<td>RI_pppoe_channel-5</td>
			<td>0.0.0.0/0</td>
			<td>pp0.1</td>
			<td>ユーザインターネット環境用のデフォルトルート</td>
		</tr>
		<tr>
			<td>RI_pppoe_channel-5</td>
			<td>202.215.185.234/32</td>
			<td>10.16.37.17</td>
			<td>事業者17の仮想ルータへのルート</td>
		</tr>
		<tr>
			<td>RI_pppoe_channel-5</td>
			<td>202.215.185.235/32</td>
			<td>10.16.37.17</td>
			<td>事業者17の仮想ルータへのルート</td>
		</tr>
		<tr>
			<td>RI_pppoe_channel-5</td>
			<td>202.215.185.237/32</td>
			<td>10.16.37.18</td>
			<td>事業者18の仮想ルータへのルート</td>
		</tr>
		<tr>
			<td>RI_pppoe_channel-5</td>
			<td>202.215.185.238/32</td>
			<td>10.16.37.18</td>
			<td>事業者18の仮想ルータへのルート</td>
		</tr>
	</table>




<hr />


	<a name="Internet_Firewall詳細設計_冗長化設計"></a>
<h2 >冗長化設計<a href="#Internet_Firewall詳細設計_冗長化設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>Chassis Cluster<br />　Juniper SRXシリーズのハードウェア冗長化技術であるChassis Clusterを使用する。この機能により、２台のデヴァイスは論理的な１台のHA構成となる。</li>
		<li>クラスタタイプ<br />　Chassis Clusterにて実装するクラスタのタイプは「アクティブ/パッシヴ」型クラスタを採用する。このクラスタタイプでは、クラスタ内の単一のデヴァイスが常に全てのトラフィックの処理を行い、その他のデヴァイスは「障害時」に限定して使用される。障害が発生した場合は、その他のバックアップ機器がマスターデヴァイスとなり全ての処理を引き継ぐ。</li>
		<li>Chassis Clusterとrethの組み合わせ<br />　Chassis Clusteとrethグループを組み合わせて使用することで、上位/下位ネットワークへ接続されるrethインターフェースのどちらか一方で障害が発生した場合でもChassis Cluster機能によりバックアップ機器がマスタデヴァイスへ昇格するよう耐障害性を確保する。</li>
		<li>インターフェース番号<br />　インターフェース番号は、0/*/*表記はアクティブデヴァイス、5/*/*表記はバックアップデヴァイスとなる。</li>
	</ul>


<hr />


	<a name="Internet_Firewall詳細設計_Chassis-Cluster構成図"></a>
<h2 >Chassis Cluster構成図<a href="#Internet_Firewall詳細設計_Chassis-Cluster構成図" class="wiki-anchor">&para;</a></h2>


	<p>上記設計を踏襲したChassis Cluster構成は以下の通りとなる。</p>


	<p><img src="/attachments/download/1372/chassiscluster%E6%A7%8B%E6%88%90%E5%9B%B3.png" alt="" /></p>


	<a name="Internet_Firewall詳細設計_障害時動作シナリオ"></a>
<h2 >障害時動作シナリオ<a href="#Internet_Firewall詳細設計_障害時動作シナリオ" class="wiki-anchor">&para;</a></h2>


	<a name="Internet_Firewall詳細設計_障害時動作"></a>
<h3 >障害時動作<a href="#Internet_Firewall詳細設計_障害時動作" class="wiki-anchor">&para;</a></h3>


	<p>障害発生時の動作は以下の通りとなる。なお、本シナリオは「アクティブ/パッシヴ」構成の場合の動作となり、ファイアウォール障害時、上位ネットワーク機器障害時それぞれのパターンを記述する。</p>


	<ul>
	<li>通常時<br />通常時は青線の経路でアクティブデヴァイスを経由しトラフィックが転送される。<br /><img src="/attachments/download/1373/%E9%80%9A%E5%B8%B8%E6%99%82.png" alt="" /></li>
	</ul>


	<ul>
	<li>ファイアウォール障害時<br />ファイアウォール筐体の障害時は赤線の経路でバックアップデヴァイスがアクティブデヴァイスへフェイルオーバーする。フェイルオーバー時はrethグループに指定されているネットワークインターフェースも全て切り替わる為、reth2.0/3.0に接続されている上位ネットワーク機器の通信経路もバックアップデヴァイス側からの通信となる。</li>
	</ul>


	<p><img src="/attachments/download/1374/%E9%9A%9C%E5%AE%B3%E6%99%82.png" alt="" /></p>


	<ul>
	<li>上位ネットワーク機器障害時<br />上位ネットワーク機器に障害が発生した場合は、機器に繋がるインターネット回線経路の冗長化はされていない為、ONUから伸びるLANケーブルをバックアップデヴァイスに手動で差し替える必要がある。なお、ファイアウォール側はreth2.0/3.0に異常が発生したことを検知する為、バックアップデヴァイスがアクティブデヴァイスへフェイルオーバーする。</li>
	</ul>


	<p><img src="/attachments/download/1375/%E4%B8%8A%E4%BD%8D%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E6%A9%9F%E5%99%A8%E9%9A%9C%E5%AE%B3%E6%99%82.png" alt="" /></p>
<hr />
<a name="Internet_Gateway詳細設計" />
<a name="Internet_Gateway詳細設計_Internet-Gateway詳細設計"></a>
<h1 >Internet Gateway詳細設計<a href="#Internet_Gateway詳細設計_Internet-Gateway詳細設計" class="wiki-anchor">&para;</a></h1>


	<a name="Internet_Gateway詳細設計_基本設定"></a>
<h2 >基本設定<a href="#Internet_Gateway詳細設計_基本設定" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>NTPサーバ<br />　NTPサーバはクラウド基盤NTPサーバを指定する。</li>
		<li>シスログ管理<br />　シスログ機能を使用し、クラウド基盤のシスログサーバへログを転送する。ログレベルは、ファシリティは以下の通りとする。<br />*<strong><b></strong>*</b>**<strong>***検討中</strong>*********************</li>
		<li>管理用IPアドレス<br />　Virtual Chassis構成の物理スイッチそれぞれに対し、一つの管理用IPアドレスを付与する。</li>
	</ul>


<hr />


	<a name="Internet_Gateway詳細設計_ルーティング設計"></a>
<h2 >ルーティング設計<a href="#Internet_Gateway詳細設計_ルーティング設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>インターネット接続<br />インターネット環境からコンダクト等の既存環境へのゲートウェイとして機能し、LANネットワークへルーティングする。<br />インターネット接続にはPPPoE接続を使用し、それぞれのPPPoE接続を本デヴァイスにて一元管理する。なお、インターネット回線（フレッツ）の使用上、PPpoE接続経路は<br />物理的に1対1のインターフェースで接続する必要がある為、ONUとゲートウェイルータの間にL2スイッチをはさみ、セッション毎のルーテッドポートとL2スイッチポートを1対1で結線する。<br />以下にインターネット接続一覧を示す。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>セッション</th>
			<th>利用ユーザ</th>
			<th> 説明</th>
		</tr>
		<tr>
			<td>コンダクト用インターネット</td>
			<td>コンダクト,ファーマ</td>
			<td>旧/新コンダクトサーバ用のインターネットセッション。コンダクトの他、ファーマ用サーバへもインターネット接続を提供する。</td>
		</tr>
		<tr>
			<td>ミニマム環境用インターネット</td>
			<td>ミニマム環境、既存ISBクラウド設備、TMMC</td>
			<td>ミニマム環境用のインターネットセッション。ミニマムの他、既存ISBクラウド設備（Zabbix、NTP等）や、TMMC α版サーバへもインターネット接続を提供する。</td>
		</tr>
	</table>




	<ul>
	<li>グローバルIPアドレスの管理とアドレス変換(NAT)<br />既存環境に付与されるグローバルIPアドレスを本デヴァイスにて管理する。対象となるグローバルIPアドレスは、LAN内のローカルサーバと紐付けられ<br />本デヴァイスにてグローバルIPアドレスからローカルアドレスにNATし、下位ネットワーク機器へルーティングする。また、内部から外部への通信に対しても同様に、ローカルIPアドレスから<br />グローバルIPアドレスにNATしインターネット環境へルーティングする。<br />グローバルIPアドレスおよびローカルIPアドレス一覧については、「【共通】ネットワークアドレス管理台帳.xlsx」を参照。</li>
	</ul>


	<ul>
	<li>インターネット環境からのアクセス制限<br />ACL機能によりインターネット環境からの不要な通信を遮断し、各公開システムに対し必要に応じて許可する送信元/ポートを絞り、不要な送信元からの意図しない通信を制限する。</li>
	</ul>


	<p><img src="/attachments/download/1077/%E3%82%A4%E3%83%B3%E3%82%BF%E3%83%BC%E3%83%8D%E3%83%83%E3%83%88%E3%83%AB%E3%83%BC%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0.png" alt="" /></p>


<hr />


	<a name="Internet_Gateway詳細設計_VRF設計"></a>
<h2 >VRF設計<a href="#Internet_Gateway詳細設計_VRF設計" class="wiki-anchor">&para;</a></h2>


	<p>本デヴァイスではルーティングテーブルの仮想化技術である「VRF」を実装し、一部例外を除き「PPPoEセッション」毎に異なるVRFを作成することでルーティングテーブルの分割管理を行う。<br />実質的にはVRF毎に「仮想ルータ」が作成される形となる。</p>


	<ul>
	<li>VRF一覧<br />「PPPoEセッション」毎に作成されるVRFの定義を以下に示す。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>VRF名</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>VRF_Dialer2</td>
			<td>ミニマム環境、既存ISBクラウド設備、TMMC用のルーティングテーブル。コンダクト用インターネットに紐付く。</td>
		</tr>
		<tr>
			<td>VRF_Dialer3</td>
			<td>新/旧コンダクト、ファーマ用のルーティングテーブル。ミニマム環境用インターネットに紐付く。</td>
		</tr>
		<tr>
			<td>VRF_management</td>
			<td>管理者がIPsecで各エリアにアクセスする為のルーティングテーブル。特定のインターネットセッションとは紐付かず、特殊なVRFとなる。</td>
		</tr>
	</table>




	<ul>
	<li>VRFインターフェース<br />それぞれのVRFはルーティングテーブルの他に仮想インターフェースを持ち、そのインターフェースを通じて通信を行う。以下にVRFのインターフェース一覧を示す。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>インターフェース</th>
			<th>VRF</th>
			<th>インターフェース種別</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>Loopback2</td>
			<td>VRF_Dialer2</td>
			<td>loopback</td>
			<td>グローバルIPアドレスが設定されるVRF_Dialer2から見たWAN側インターフェース。</td>
		</tr>
		<tr>
			<td>Vlan2058</td>
			<td>VRF_Dialer2</td>
			<td>vlan</td>
			<td>LANネットワークとつながるvlanインターフェース。下位ネットワーク機器であるL3スイッチとの通信経路。</td>
		</tr>
		<tr>
			<td>Dialer2</td>
			<td>VRF_Dialer2</td>
			<td>dialer</td>
			<td>PPPoE接続用インターフェース。loopback2インターフェースに紐付けられたグローバルIPアドレスがマッピングされる。</td>
		</tr>
		<tr>
			<td>Loopback3</td>
			<td>VRF_Dialer3</td>
			<td>loopback</td>
			<td>グローバルIPアドレスが設定されるVRF_Dialer3から見たWAN側インターフェース。</td>
		</tr>
		<tr>
			<td>vlan2059</td>
			<td>VRF_DIaler3</td>
			<td>vlan</td>
			<td>LANネットワークとつながるvlanインターフェース。下位ネットワーク機器であるL3スイッチとの通信経路。</td>
		</tr>
		<tr>
			<td>Dialer3</td>
			<td>VRF_Dialer3</td>
			<td>dialer</td>
			<td>PPPoE接続用インターフェース。loopback3インターフェースに紐付けられたグローバルIPアドレスがマッピングされる。</td>
		</tr>
		<tr>
			<td>Virtual-Template1</td>
			<td>VRF_management</td>
			<td>virtual templete</td>
			<td>IPsec通信用のインターフェース。Ipsec接続後の外部との通信はこのインターフェースを経由して行われる。</td>
		</tr>
		<tr>
			<td>vlan2056</td>
			<td>VRF_management</td>
			<td>vlan</td>
			<td>Virtual-Templeteにマッピングされるvlanインターフェース。</td>
		</tr>
	</table>




	<p><img src="/attachments/download/1078/vrf.png" alt="" /></p>


<hr />


	<a name="Internet_Gateway詳細設計_冗長化設計"></a>
<h2 >冗長化設計<a href="#Internet_Gateway詳細設計_冗長化設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>筐体冗長化設計<br />本デヴァイスは2台の冗長構成とする。但し、スタッキング等の筐体レベルでの論理冗長化技術は用いず、単純な2台の物理冗長構成とする。</li>
	</ul>


	<ul>
	<li>LANインターフェース冗長化（HSRP）<br />本デヴァイスが持つインターフェースを冗長化し、障害時に経路の自動切り替えが可能な設計とする。冗長化にはCisco固有のイーサネット冗長化技術である「HSRP」を使用する。<br />HSRPはアクティブ側、スタンバイ側それぞれのインターフェースに設定し、インターフェース毎に物理IPアドレスの他、グループ内で１つのネットワーク上の仮想的なゲートウェイとなる「VIP」を持つ。<br />また、アクティブ、スタンバイそれぞれのインターフェースに優先度（プライオリティ）を設定、通常時のアクティブインターフェースを定義し、障害時はスタンバイ側へVIPをフェイルオーバーさせる。<br />なお、HSRP設定は下位ネットワークへ接続されるLAN側インターフェースに適用し、WAN側インターフェースには適用しない。つまり、LANネットワークの冗長化設計となる。</li>
	</ul>


	<ul>
	<li>WAN側インターフェース障害の監視（オブジェクトトラッキング）<br />HSRPはLANインターフェースの障害を検知しスタンバイ側へ自動切り替えを行うが、WAN側インターフェース単体の自動切り替えは考慮していない。この場合、WANインターフェースに障害が発生した場合でも<br />同筐体の「LAN側インターフェース」は生存している為、フェイルオーバーが発動せずアクティブ→スタンバイといった無駄な経路で通信が行われしまう。この問題を回避する為、WAN側インターフェース障害を<br />トリガーとし、同一筐体のLAN側インターフェースがスタンバイ側へ切り替わるよう「オブジェクトトラッキング」を実装する。</li>
	</ul>


	<ul>
	<li>WAN側回線障害発生時<br />WAN側インターフェースで障害が発生した場合は、HSRP機能によりLAN側インターフェースの自動切り替えは発動する。しかしながら、WAN側インターフェースそのものの冗長化は実装されていない<br />為、対向機器と繋がるLANケーブルを手動で差し替える必要がある。具体的には、アクティブ側のケーブルを抜き、スタンバイ側に差し替える必要がある。</li>
	</ul>


	<ul>
	<li>仮想ルータにおけるHSRP<br />本デヴァイスではインターネットセッション毎にVRF（仮想ルータ）を作成しているが、HSRPはLAN側インターフェースで実装し、そのLAN側インターフェースは物理ルータおよび各VRFに紐付られている。<br />よって、本デヴァイスにおけるHSRPは「仮想ルータに紐付くインターフェースへのHSRP実装」を考慮した設計とする必要がある。本デヴァイスにおいては、VRFに紐づく「vlanインターフェース」に対しHSRPを実装する。</li>
	</ul>


	<ul>
	<li>インターフェーストラッキング<br />HSRPはLAN側インターフェースの冗長化設定の為、例えばWAN側インターフェースに障害が発生した場合でも、LAN側インターフェースは障害を検知していない為、LAN側は正常、WAN側は障害といった<br />現象が発生してしまう。この問題を回避する為、HSRP設定に「インターフェーストラッキング」を実装する。この機能は、HSRPによりLAN側インターフェースの他に、WAN側インターフェースとなる物理ルーテッドポートや<br />vlanインターフェースが紐付けられる物理スイッチングポートを監視し、対象インターフェースに障害が発生すると、LAN側インターフェースのVIPをフェイルオーバーさせる。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>インターフェース名</th>
			<th>HSRPインターフェース名</th>
			<th>HSRPグループ</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>vlan2056</td>
			<td>2056</td>
			<td>HSRP_GR2056</td>
			<td>IPsec用仮想ルータのLAN側インターフェース</td>
		</tr>
		<tr>
			<td>vlan2057</td>
			<td>2057</td>
			<td>HSRP_GR2057</td>
			<td>物理ルータのLAN側インターフェース</td>
		</tr>
		<tr>
			<td>vlan2058</td>
			<td>2058</td>
			<td>HSRP_GR2058</td>
			<td>Dialer2仮想ルータのLAN側インターフェース</td>
		</tr>
		<tr>
			<td>vlan2059</td>
			<td>2059</td>
			<td>HSRP_GR2059</td>
			<td>Dialer3仮想ルータのLAN側インターフェース</td>
		</tr>
	</table>




	<p><img src="/attachments/download/1041/hsrp.png" alt="" /></p>


	<ul>
	<li>Event Manager<br />カスタム動作を定義する為の機能である「Event Manager」を実装する。これは、HSRPのステータスが変更されたことをトリガーとし、状態に応じてアクティブ、スタンバイ機器のDialerインターフェースを<br />「no shutdown」「shutdown」する為に使用する。具体的な動作を以下に記述する。<br />Event ManagerにてHSRPのステータスを監視。「%HSRP-5-STATECHANG」をトリガーとしてスクリプトが発動される。<br />↓<br />show standby briefを実行。<br />↓<br />出力結果から、「VI***」の部分と「state」の部分を変数「id」「hsrp_state」に格納する。<br />↓<br />show ip vrf interfaces | include Vl　****を実行。****は変数「id」の値を一つ一つ入れる。<br />↓<br />出力結果から、「VRF_(<strong><b>)」かっこでくくられた部分を変数「dialer」に格納する。<br />↓<br />show interfaces *</b></strong>  | include ^Dialer[0-9]+　を実行。****は変数「dialer」の値を入れる。<br />↓<br />出力結果から、「Dialer is (**), 」かっこでくくられた部分を変数「dialer_state」に格納する。<br />↓<br />変数「hsrp_state」に格納されている値が「Active」且つ、変数「dialer_state」に格納されている値が「up」以外の場合、変数「Dialer」に格納されたDialerインターフェースを「no shutdown」する。　★スタンバイ機がアクティブ機になる為の処理<br />↓<br />そうでなければ<br />↓<br />変数「dialer_state」に格納されている値が「up」の場合、変数「Dialer」に格納されたDialerインターフェースを「shutdown」する。　★アクティブ機がスタンバイ機になる為の処理。</li>
	</ul>


	<p>HSRPが「Down状態」でDialerが「Up状態」である場合、Dialerインターフェースを「shutdown」する。</p>


<hr />
<hr />
<a name="ISCSIストレージボリュームの拡張〜Linux上での認識まで〜" />
<a name="ISCSIストレージボリュームの拡張〜Linux上での認識まで〜_ISCSIストレージボリュームの拡張Linux上での認識まで"></a>
<h1 >ISCSIストレージボリュームの拡張〜Linux上での認識まで〜<a href="#ISCSIストレージボリュームの拡張〜Linux上での認識まで〜_ISCSIストレージボリュームの拡張Linux上での認識まで" class="wiki-anchor">&para;</a></h1>


	<a name="ISCSIストレージボリュームの拡張〜Linux上での認識まで〜_事前準備"></a>
<h2 >事前準備<a href="#ISCSIストレージボリュームの拡張〜Linux上での認識まで〜_事前準備" class="wiki-anchor">&para;</a></h2>


	<p>・EquallogicのWeb管理画面で拡張対象ボリュームの現状サイズを確認しておく。<br />・対象ボリュームのLinux上で認識されているデヴァイス名と現状サイズを確認しておく（fdisk -l）</p>


	<a name="ISCSIストレージボリュームの拡張〜Linux上での認識まで〜_iSCSIストレージのボリューム領域拡張"></a>
<h2 >iSCSIストレージのボリューム領域拡張<a href="#ISCSIストレージボリュームの拡張〜Linux上での認識まで〜_iSCSIストレージのボリューム領域拡張" class="wiki-anchor">&para;</a></h2>


	<p>※EquallogicのWeb管理画面から作業を行う。</p>


	<p>1. 画面左側の「Volumes」をクリック。<br />2. 「Volumes」ツリーを展開し、対象ボリューム名を選択。<br />3. 「Modify settings」をクリックします。<br />4. 「Space」タブをクリックして、「Volume size」フィールドの値を変更し、「OK」をクリック。<br />5. スナップショットを作成のダイアログについて「Cancel」をクリック。<br />6. ボリューム管理画面の「Volume space - Reported size」が拡張後のサイズとなっていることを確認。</p>


	<a name="ISCSIストレージボリュームの拡張〜Linux上での認識まで〜_PV拡張"></a>
<h2 >PV拡張<a href="#ISCSIストレージボリュームの拡張〜Linux上での認識まで〜_PV拡張" class="wiki-anchor">&para;</a></h2>


	<p>拡張したボリュームと接続しているLinux上でサイズ確認<br /><pre>
[root@daesohyp01 ~]# fdisk -l /dev/eql/daes02

ディスク /dev/eql/daes02: 338.2 GB, 338228674560 バイト　←サイズが増えたか確認。
ヘッド 255, セクタ 63, シリンダ 41120
Units = シリンダ数 of 16065 * 512 = 8225280 バイト
セクタサイズ (論理 / 物理): 512 バイト / 512 バイト
I/O size (minimum/optimal): 512 bytes / 512 bytes
ディスク識別子: 0x00000000

[root@daesohyp01 ~]# 
</pre></p>


	<p>pvscanで現状のフリー領域が0Gとなっていることを確認。</p>


	<p>pvresizeでpvの認識サイズを拡張<br /><pre>
pvresize /dev/eql/daes02
</pre></p>


	<p>pvscanでフリー領域が増えたことを確認。<br /><pre>
[root@daesohyp01 ~]# pvscan
  PV /dev/eql/daes02   VG Voldaes02    lvm2 [315.00 GiB / 15.00 GiB free]
</pre></p>


	<p>vgdisplayでVGも拡張されたことを確認<br /><pre>
[root@daesohyp01 ~]# vgdisplay
  --- Volume group ---
  VG Name               Voldaes02
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  4
  VG Access             read/write
  VG Status             resizable
  Clustered             yes
  Shared                no
  MAX LV                0
  Cur LV                2
  Open LV               2
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               315.00 GiB　←サイズが拡張されたことを確認
  PE Size               4.00 MiB
  Total PE              80639
  Alloc PE / Size       76799 / 300.00 GiB
  Free  PE / Size       3840 / 15.00 GiB
  VG UUID               rdFcmM-azj6-ZLhT-J3n0-tRW2-MvMU-IXK1SE
</pre></p>
<hr />
<a name="ISCSIボリュームの認識〜接続まで" />
<a name="ISCSIボリュームの認識〜接続まで_ISCSIターゲットの認識接続まで"></a>
<h1 >ISCSIターゲットの認識〜接続まで<a href="#ISCSIボリュームの認識〜接続まで_ISCSIターゲットの認識接続まで" class="wiki-anchor">&para;</a></h1>


	<h2>iSCSIターゲットの検索<br /><pre>
iscsiadm -m discovery -t sendtargets -p 10.16.24.129
</pre></h2>


	<h2>サーバが認識しているiSCSIターゲット一覧の表示<br /><pre>
iscsiadm -m node
</pre></h2>


	<a name="ISCSIボリュームの認識〜接続まで_iSCSIターゲットへログイン"></a>
<h2 >iSCSIターゲットへログイン<a href="#ISCSIボリュームの認識〜接続まで_iSCSIターゲットへログイン" class="wiki-anchor">&para;</a></h2>


	<p>※HIT for linux（Equallogic接続管理ソフト）のコマンドを使用する。iscsiadmコマンドは非推奨。<br /><pre>
ehcmcli login -T [ターゲット名]
</pre></p>


	<a name="ISCSIボリュームの認識〜接続まで_ログイン確認"></a>
<h3 >ログイン確認<a href="#ISCSIボリュームの認識〜接続まで_ログイン確認" class="wiki-anchor">&para;</a></h3>


	<p>※iSCSI接続はマルチパスの為、設定したインターフェースの数分の経路でログインできていることを確認する。<br /><pre>
ehcmcli status
</pre></p>
<hr />
<a name="ISCSI設計" />
<a name="ISCSI設計_ISCSI設計"></a>
<h1 >ISCSI設計<a href="#ISCSI設計_ISCSI設計" class="wiki-anchor">&para;</a></h1>


	<a name="ISCSI設計_概要"></a>
<h2 >概要<a href="#ISCSI設計_概要" class="wiki-anchor">&para;</a></h2>


	<p>各KVMハイパーバイザから一つの共通ボリュームに対してiSCSI接続する<br />ディスク容量を節約するためシンプロビジョニングを利用する<br />本システムのストレージは二台のストレージで構成されているが、クラスタ構成になっていないためストレージの筐体ごと停止した場合、<br />そのストレージに保存されているデータは利用できなくなる。そのケースを想定してSyncRepを利用し二台のストレージにデータをミラーリングする。<br />ACLで各KVMハイパーバイザからのみアクセスをさせるようにアクセス制限をかける</p>


	<a name="ISCSI設計_ボリューム設定"></a>
<h2 >ボリューム設定<a href="#ISCSI設計_ボリューム設定" class="wiki-anchor">&para;</a></h2>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ボリューム名</th>
			<th>エイリアス名</th>
			<th>場所</th>
			<th>プール</th>
			<th>容量</th>
			<th>セクタサイズ</th>
			<th>RAIDパフォーマンス</th>
			<th>用途</th>
		</tr>
		<tr>
			<td>idc-01</td>
			<td>idc-01</td>
			<td>Volumes\idc-01</td>
			<td>side-A，side-B</td>
			<td>5T</td>
			<td>512</td>
			<td>Automatic</td>
			<td>kvm用ストレージ</td>
		</tr>
		<tr>
			<td>idc-02</td>
			<td>idc-02</td>
			<td>Volumes\idc-02</td>
			<td>side-A，side-B</td>
			<td>1G</td>
			<td>512</td>
			<td>Automatic</td>
			<td>kvmクラスタ用quorum disk</td>
		</tr>
	</table>




	<a name="ISCSI設計_シンプロビジョニング設定"></a>
<h2 >シンプロビジョニング設定<a href="#ISCSI設計_シンプロビジョニング設定" class="wiki-anchor">&para;</a></h2>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ボリューム名</th>
			<th>Minimum volume reserved</th>
			<th>In-use warning limit</th>
			<th>Maximum in-use space</th>
		</tr>
		<tr>
			<td>idc-01</td>
			<td>10％</td>
			<td>80％</td>
			<td>100％</td>
		</tr>
		<tr>
			<td>idc-02</td>
			<td>10％</td>
			<td>80％</td>
			<td>100％</td>
		</tr>
	</table>




	<a name="ISCSI設計_スナップショット設定"></a>
<h2 >スナップショット設定<a href="#ISCSI設計_スナップショット設定" class="wiki-anchor">&para;</a></h2>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ボリューム名</th>
			<th>Space</th>
			<th>Warning limit</th>
		</tr>
		<tr>
			<td>idc-01</td>
			<td>5%</td>
			<td>80%</td>
		</tr>
		<tr>
			<td>idc-02</td>
			<td>5%</td>
			<td>80%</td>
		</tr>
	</table>




	<a name="ISCSI設計_SyncRep設定"></a>
<h2 >SyncRep設定<a href="#ISCSI設計_SyncRep設定" class="wiki-anchor">&para;</a></h2>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ボリューム名</th>
			<th>SyncActive </th>
			<th>SyncAlternate</th>
		</tr>
		<tr>
			<td>idc-01</td>
			<td>side-A</td>
			<td>side-B</td>
		</tr>
		<tr>
			<td>idc-02</td>
			<td>side-A</td>
			<td>side-B</td>
		</tr>
	</table>




	<a name="ISCSI設計_アクセス情報設定"></a>
<h2 >アクセス情報設定<a href="#ISCSI設計_アクセス情報設定" class="wiki-anchor">&para;</a></h2>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ボリューム名</th>
			<th>IQN</th>
			<th>異なるイニシエータからの同時接続</th>
		</tr>
		<tr>
			<td>idc-01</td>
			<td>iqn.2001-05.com.equallogic:8-661fc6-8b3afabbe-5ff0059b5a855028-idc-01</td>
			<td>TRUE</td>
		</tr>
		<tr>
			<td>idc-02</td>
			<td>iqn.2001-05.com.equallogic:8-661fc6-f79afabbe-82a0059bded556bd-idc-02</td>
			<td>TRUE</td>
		</tr>
	</table>




	<a name="ISCSI設計_ACL設定"></a>
<h2 >ACL設定<a href="#ISCSI設計_ACL設定" class="wiki-anchor">&para;</a></h2>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ボリューム名</th>
			<th>CHAPユーザ名</th>
			<th>CHAPパスワード</th>
			<th>IP制限</th>
		</tr>
		<tr>
			<td>idc-01</td>
			<td>idc-ohyp0a</td>
			<td>m33FeawddKHH</td>
			<td>10.16.41.*,10.16.42.*</td>
		</tr>
		<tr>
			<td>idc-02</td>
			<td>idc-ohyp0a</td>
			<td>m33FeawddKHH</td>
			<td>10.16.41.*,10.16.42.*</td>
		</tr>
	</table>




	<a name="ISCSI設計_LVM設定"></a>
<h2 >LVM設定<a href="#ISCSI設計_LVM設定" class="wiki-anchor">&para;</a></h2>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ボリュームグループ名称</th>
			<th>ロジカルグループ名称</th>
		</tr>
		<tr>
			<td>vg_idc-01</td>
			<td>LVはインスタンス名称(事業者名称)を付与する</td>
		</tr>
	</table>




	<a name="ISCSI設計_マルチパス設定"></a>
<h2 ><a href="#マルチパス設定" class="wiki-page">マルチパス設定</a><a href="#ISCSI設計_マルチパス設定" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="ISCSI設計_eqltune"></a>
<h2 ><a href="#Eqltune" class="wiki-page">eqltune</a><a href="#ISCSI設計_eqltune" class="wiki-anchor">&para;</a></h2>


<hr />
<hr />
<a name="Keepalivedとlvs" />
<a name="Keepalivedとlvs_Keepalivedとlvs"></a>
<h1 >Keepalivedとlvs<a href="#Keepalivedとlvs_Keepalivedとlvs" class="wiki-anchor">&para;</a></h1>


<pre>
# yum install ipvsadm keepalived
# cp /etc/keepalived/keepalived.conf{,.org}
# vim /etc/sysctl.conf
### net.ipv4.ip_forward = 1に変更する
# sysctl -p
# cat /etc/keepalived/keepalived.conf
! Configuration File for keepalived

global_defs {
   router_id LVS_DEVEL
}

virtual_server 10.16.227.132 8080 {
    delay_loop 2
    lb_algo lc
    lb_kind NAT
    protocol TCP

    real_server 10.16.228.50 8080 {
        weight 1
        inhibit_on_failure
        TCP_CHECK {
            connect_port 8080
            connect_timeout 2
            nb_get_retry 3
            delay_before_retry 2
        }
    }
    real_server 10.16.228.51 8080 {
        weight 1
        inhibit_on_failure
        TCP_CHECK {
            connect_port 8080
            connect_timeout 2
            nb_get_retry 3
            delay_before_retry 2
        }
    }
    real_server 10.16.228.52 8080 {
        weight 1
        inhibit_on_failure
        TCP_CHECK {
            connect_port 8080
            connect_timeout 2
            nb_get_retry 3
            delay_before_retry 2
        }
    }
}
</pre>

	<p>lb_algoにsh(source hash)を指定することで特定のサーバに分散するように設定できる。<br />lb_kindにDR(Direct Return)を指定することでDSRを利用できる。<br />DSRの場合はリアルサーバに以下の設定を追加して、<br />リアルサーバの処理ということをわからせる必要がある。<br /><pre>
iptables -t nat -A PREROUTING -d [VIP] -j REDIRECT
service iptables save
</pre></p>


<hr />


	<p>参考までに静的ページを返すだけのnginxを用意してLBを経由しない場合とした場合の速度測定を行った。<br />測定方法は以下</p>


	<pre><code>for i in `seq 100`;do curl -s -o /dev/null -w '%{time_total}\n' 192.168.3.131;done</code></pre>


	<p>最終行は平均値。単位は[sec]</p>


	<table>
		<tr>
			<td>test1 direct</td>
			<td>test2 direct</td>
			<td>lb</td>
		</tr>
		<tr>
			<td>0.003</td>
			<td>0.003</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.003</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.002</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.002</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.001</td>
			<td>0.001</td>
			<td>0.002</td>
		</tr>
		<tr>
			<td>0.00113</td>
			<td>0.00157</td>
			<td>0.00247</td>
		</tr>
	</table>
<hr />
<a name="Kickstart" />
<a name="Kickstart_Kickstart"></a>
<h1 >Kickstart<a href="#Kickstart_Kickstart" class="wiki-anchor">&para;</a></h1>


<hr />


	<a name="Kickstart_インストール"></a>
<h2 >インストール<a href="#Kickstart_インストール" class="wiki-anchor">&para;</a></h2>


	<p>pxeブートにはNICにIPアドレスを割り振るDHCPが必要となる。<br />クラコンにはもともとDHCPを用意するため、<br />それを利用する。</p>


	<p>まずはDHCPをインストールする。<br />ポイントは以下となる。</p>


	<ul>
	<li>TFTPサーバをnext-serverで指定</li>
		<li>NBPのパスをfilenameで指定</li>
		<li>PXEブートを指定する場合はIPアドレスを割り当てるMACアドレスを制限する</li>
	</ul>


<pre>
# yum -y update
# yum -y install dhcp
# egrep -v "(^$|^#| ?#)" dhcpd.conf
option domain-name-servers 192.168.76.254;
default-lease-time 600;
max-lease-time 7200;
log-facility local7;
subnet 192.168.76.0 netmask 255.255.255.0 {
  range 192.168.76.2 192.168.76.100;
  option routers 192.168.76.254;
  option broadcast-address 192.168.76.255;
  next-server 192.168.76.1;
  filename "/pxelinux.0";
}
# /etc/init.d/dhcpd start
dhcpd を起動中:                                            [  OK  ]
</pre>

	<p>※dhcpでlistenするIPを指定したい場合、以下のようにインターフェイスを指定する。</p>


<pre>
# egrep -v "(^#|^$)" /etc/sysconfig/dhcpd
DHCPDARGS=bond0
</pre>

	<p>TFTPサーバをインストールする。</p>


<pre>
# yum install -y tftp-server
# egrep -v "(^#|^$)" /etc/xinetd.d/tftp 
service tftp
{
    socket_type        = dgram
    protocol        = udp
    wait            = yes
    user            = root
    server            = /usr/sbin/in.tftpd
    server_args        = -s /var/lib/tftpboot
    disable            = no
    per_source        = 11
    cps            = 100 2
    flags            = IPv4
}
# /etc/init.d/xinetd start
xinetd を起動中:                                           [  OK  ]

</pre>

	<p>pxeブートパッケージの入ったsyslinuxをインストール及び必要なファイルを集める。</p>


<pre>
# yum -y install syslinux
# cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/
# mount CentOS-6.6-x86_64-bin-DVD1.iso /mnt -o loop
# mkdir /var/lib/tftpboot/CentOS-6.6-x86_64
# cp /mnt/images/pxeboot/{initrd.img,vmlinuz} /var/lib/tftpboot/CentOS-6.6-x86_64/
# umount /mnt
# mkdir /var/lib/tftpboot/pxelinux.cfg
# vim /var/lib/tftpboot/pxelinux.cfg/default
# egrep -v "(^#|^$)" /var/lib/tftpboot/pxelinux.cfg/default 
default CentOS-6.6-x86_64
prompt 1
timeout 10
label CentOS-6.6-x86_64
kernel /CentOS-6.6-x86_64/vmlinuz
append ksdevice=eth0 ks=http://192.168.76.1/ks.cfg initrd=/CentOS-6.6-x86_64/initrd.img
</pre>

	<p>nginxをインストールする。<br />isoはマウントして中身が見える状態にしておく。</p>


<pre>
# yum install -y http://nginx.org/packages/centos/6/noarch/RPMS/nginx-release-centos-6-0.el6.ngx.noarch.rpm
# yum -y install nginx
# cp anaconda-ks.cfg /usr/share/nginx/html/ks.cfg
# mount CentOS-6.6-x86_64-bin-DVD1.iso /usr/share/nginx/html/iso -o loop
# /etc/init.d/nginx start
</pre>

	<p>ks.cfgの内容を参考までに記載する。</p>


<pre>
# egrep -v "(^$|^#)" ks.cfg
install
url --url http://192.168.76.1/iso
lang ja_JP.UTF-8
keyboard jp106
network --onboot no --device eth0 --bootproto dhcp --noipv6
rootpw  --iscrypted $6$Pd.3uUWtlhrh7WNV$poYzKEvBRXdBlbIMLv.cXnSn.j61/VQ2uwBOPmLtXJHfnR3BqQpmzlqP8ELBuyMjN6veHxj.c3KE8JLkWAWZa0
firewall --service=ssh
authconfig --enableshadow --passalgo=sha512
selinux --enforcing
timezone --utc Asia/Tokyo
bootloader --location=mbr --driveorder=sda --append="crashkernel=auto rhgb quiet" 
clearpart --none --drives=sda
part /boot --fstype=ext4 --size=500
part pv.008002 --grow --size=200
volgroup vgtest --pesize=4096 pv.008002
logvol / --fstype=ext4 --name=root --vgname=vgtest --size=4096
logvol swap --name=swap --vgname=vgtest --size=1024
logvol /var --fstype=ext4 --name=var --vgname=vgtest --size=4096
logvol /var/log --fstype=ext4 --name=varlog --vgname=vgtest --size=4096
%packages
@base
@core
@development
@japanese-support
@server-policy
@workstation-policy
sgpio
device-mapper-persistent-data
%end
</pre>

<hr />


	<a name="Kickstart_注意点"></a>
<h2 >注意点<a href="#Kickstart_注意点" class="wiki-anchor">&para;</a></h2>


	<a name="Kickstart_tftp通信ができない場合"></a>
<h3 >tftp通信ができない場合<a href="#Kickstart_tftp通信ができない場合" class="wiki-anchor">&para;</a></h3>


	<p>iptablesの設定をする。<br />今回はストップ</p>


<pre>
# /etc/init.d/iptables stop
</pre>

	<a name="Kickstart_diskが見つからない場合"></a>
<h3 > diskが見つからない場合<a href="#Kickstart_diskが見つからない場合" class="wiki-anchor">&para;</a></h3>


	<p>書き込みを選択する際にチェックを外す。</p>
<hr />
<a name="Kickstart設計" />
<a name="Kickstart設計_Kickstart設計"></a>
<h1 >Kickstart設計<a href="#Kickstart設計_Kickstart設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#Kickstart設計_Kickstart設計">Kickstart設計</a></li><li><a href="#Kickstart設計_Introduction">●Introduction</a><ul><li><a href="#Kickstart設計_目的">目的</a></li><li><a href="#Kickstart設計_定義範囲">定義範囲</a></li></ul>
</li><li><a href="#Kickstart設計_構成">●構成</a><ul><li><a href="#Kickstart設計_サーバ構成">サーバ構成</a></li><li><a href="#Kickstart設計_パーティション構成">パーティション構成</a></li><li><a href="#Kickstart設計_導入パッケージ">導入パッケージ</a></li><li><a href="#Kickstart設計_システムサービス一覧">システムサービス一覧</a></li><li><a href="#Kickstart設計_ログ設定">ログ設定</a></li><li><a href="#Kickstart設計_ログローテート設定">ログローテート設定</a></li></ul>
</li><li><a href="#Kickstart設計_システム設計">●システム設計</a><ul><li><a href="#Kickstart設計_dhcpd設定">dhcpd設定</a><ul><li><a href="#Kickstart設計_dhcpdconf">dhcpd.conf</a></li></ul>
</li><li><a href="#Kickstart設計_tftpdxined">tftpd(xined)</a></li><li><a href="#Kickstart設計_mrepo">mrepo</a></li><li><a href="#Kickstart設計_nginx">nginx</a></li></ul>
</li><li><a href="#Kickstart設計_冗長化">●冗長化</a></li><li><a href="#Kickstart設計_運用手順">●運用手順</a><ul><li><a href="#Kickstart設計_リポジトリの更新">リポジトリの更新</a></li></ul>
</li><li><a href="#Kickstart設計_バックアップ">●バックアップ</a></li></ul>


<hr />


	<a name="Kickstart設計_Introduction"></a>
<h1 >●Introduction<a href="#Kickstart設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="Kickstart設計_目的"></a>
<h2 >目的<a href="#Kickstart設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>本システムは多くの機能を提供するに当たり、<br />複数台の物理サーバが稼働している。<br />初期状態ではOSがないためOSインストールから行う必要があり、<br />OSインストールは一般的に対話型の作業となる。<br />多くの台数がある場合、OSインストール作業だけでも多くの時間がかかる。<br />この作業を解消するためOS自動インストール機能を提供するkickstartを用いる。<br />本書ではkickstartでのOS自動インストール機能を定義する。<br />また、関連のあるミラーリポジトリの設計についても記載する。</p>


	<a name="Kickstart設計_定義範囲"></a>
<h2 >定義範囲<a href="#Kickstart設計_定義範囲" class="wiki-anchor">&para;</a></h2>


	<p>kickstartはOS自動インストール機能の名称であり、<br />kickstart自体にデーモン等は存在しない。<br />kickstartではdhcp,tftp,http,pxeブートを用いて、<br />ネットワーク上にあるOSのイメージファイルを読み込み、その上で機能を提供する。<br />定義範囲を以下に示す。</p>


	<p><img src="/attachments/download/1044/kickstart_system_define_range.png" alt="" /></p>


	<p>kickstartではOSがインストールされたサーバが最低1台必要となる。<br />サーバのそれぞれの役割をシーケンスとともに以下に示す。</p>


	<ul>
	<li>① dhcp：サーバに電源投入後pxeブートを起動する。pxeブートでは始めにdhcpによりIPアドレスの取得を行う。また次にアクセスするtftpサーバのIPアドレスをdhcpの機能により通知する。</li>
		<li>② tftp：dhcpによってIPアドレスを取得した端末がインストールイメージおよびkickstartの定義ファイルURLを取得するサーバ。</li>
		<li>③ http：OSのイメージおよびkickstart定義ファイルを格納したサーバ。それらの情報が取得可能であればftpサーバでも良い。</li>
	</ul>


	<p>これらシーケンスの後に、OSインストールが起動する。<br />対話型のOSインストールで入力する情報がkickstart定義ファイルに書かれている。</p>


<hr />


	<a name="Kickstart設計_構成"></a>
<h1 >●構成<a href="#Kickstart設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="Kickstart設計_サーバ構成"></a>
<h2 >サーバ構成<a href="#Kickstart設計_サーバ構成" class="wiki-anchor">&para;</a></h2>


	<p>本システムではkickstartサーバをクラコンで提供する。<br />ただし冗長化を行わず単に2台でdhcp,tftp,httpサービスを提供する。<br />以下にサーバ構成を示す。</p>


	<p><img src="/attachments/download/1045/kickstart_system_structure.png" alt="" /></p>


	<p>dhcpは2台構成で運用するが、設定内容は同一であるため問題とならない。</p>


	<a name="Kickstart設計_パーティション構成"></a>
<h2 >パーティション構成<a href="#Kickstart設計_パーティション構成" class="wiki-anchor">&para;</a></h2>


	<p>パーティション構成を以下に示す。<br />isoイメージやミラーリポジトリも格納するため、<br />/var領域を拡張する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>デバイスファイル</th>
			<th>マウントポイント</th>
			<th>LVM</th>
			<th>FSタイプ</th>
			<th>サイズ</th>
			<th>備考</th>
		</tr>
		<tr>
			<td>/dev/mapper/vg_system-lv_var</td>
			<td>/var</td>
			<td>○</td>
			<td>ext4</td>
			<td>50GB</td>
			<td>ミラーリポジトリのためのパッケージ格納領域に20GB程度利用するため余裕を持って50GBとする</td>
		</tr>
	</table>




	<a name="Kickstart設計_導入パッケージ"></a>
<h2 >導入パッケージ<a href="#Kickstart設計_導入パッケージ" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>パッケージ名</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>dhcp</td>
			<td>dhcpサーバ</td>
		</tr>
		<tr>
			<td>portreserve</td>
			<td>ポート調査のユーティリティ</td>
		</tr>
		<tr>
			<td>dhclient</td>
			<td>dhcpクライアント</td>
		</tr>
		<tr>
			<td>dhcp-common</td>
			<td>サーバとクライアント共通ファイル</td>
		</tr>
		<tr>
			<td>tftp-server</td>
			<td>tftpサーバ</td>
		</tr>
		<tr>
			<td>xinetd</td>
			<td>tftpの代わりにtftpの利用ポートで待ち受けを行うサーバ</td>
		</tr>
		<tr>
			<td>syslinux</td>
			<td>カーネルローダ</td>
		</tr>
		<tr>
			<td>mtools</td>
			<td>MSDOSにアクセスするためのツール。syslinuxに必要</td>
		</tr>
		<tr>
			<td>syslinux-nonlinux</td>
			<td>syslinux関連のパッケージ</td>
		</tr>
		<tr>
			<td>nginx</td>
			<td>httpサーバ</td>
		</tr>
	</table>




	<a name="Kickstart設計_システムサービス一覧"></a>
<h2 >システムサービス一覧<a href="#Kickstart設計_システムサービス一覧" class="wiki-anchor">&para;</a></h2>


	<p>自動起動設定を以下の状態になるように設定する。</p>


<pre>
# chkconfig 
dhcpd              0:off    1:off    2:off    3:off    4:off    5:off    6:off
dhcpd6             0:off    1:off    2:off    3:off    4:off    5:off    6:off
dhcrelay           0:off    1:off    2:off    3:off    4:off    5:off    6:off
dhcrelay6          0:off    1:off    2:off    3:off    4:off    5:off    6:off
nginx              0:off    1:off    2:on    3:on    4:on    5:on    6:off
portreserve        0:off    1:off    2:on    3:on    4:on    5:on    6:off
xinetd             0:off    1:off    2:off    3:on    4:on    5:on    6:off

xinetd ベースのサービス:
    chargen-dgram:     off
    chargen-stream:    off
    daytime-dgram:     off
    daytime-stream:    off
    discard-dgram:     off
    discard-stream:    off
    echo-dgram:        off
    echo-stream:       off
    rsync:             off
    tcpmux-server:     off
    tftp:              on
    time-dgram:        off
    time-stream:       off
</pre>

	<a name="Kickstart設計_ログ設定"></a>
<h2 >ログ設定<a href="#Kickstart設計_ログ設定" class="wiki-anchor">&para;</a></h2>


	<p>dhcpはlocal1でsyslogに出力する。<br />xinetdはdaemonでsyslogに出力する。<br />上記設定を加えたsyslogの設定ファイルを以下に示す。</p>


	<ul>
	<li>/etc/rsyslog.conf</li>
	</ul>


<pre>
$ModLoad imuxsock # provides support for local system logging (e.g. via logger command)
$ModLoad imklog   # provides kernel logging support (previously done by rklogd)
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat
$IncludeConfig /etc/rsyslog.d/*.conf
*.info;mail.none;authpriv.none;cron.none;local1.none;daemon.none                /var/log/messages/messages
authpriv.*                                              /var/log/secure/secure
mail.*                                                  -/var/log/mail/maillog
cron.*                                                  /var/log/cron/cron
*.emerg                                                 *
uucp,news.crit                                          /var/log/spooler/spooler
local7.*                                                /var/log/boot/boot.log
local1.* /var/log/dhcp/dhcpd.log
daemon.* /var/log/xinetd/tftp.log
</pre>

	<p>nginxのログ出力先を以下に示す。</p>


<pre>
/var/log/nginx/access.log
/var/log/nginx/error.log
</pre>

	<a name="Kickstart設計_ログローテート設定"></a>
<h2 >ログローテート設定<a href="#Kickstart設計_ログローテート設定" class="wiki-anchor">&para;</a></h2>


	<p>ログは400日間(1年と1ヶ月程度)保存するものとする。</p>


	<ul>
	<li>/etc/logrotate.d/dhcpd</li>
	</ul>


<pre>
/var/log/dhcp/dhcpd.log {
    daily
    create 600 root root
    rotate 400
    ifempty
    missingok
    dateext
    compress
}
</pre>

	<ul>
	<li>/etc/logrotate.d/xinetd</li>
	</ul>


<pre>
/var/log/xinetd/tftp.log {
    daily
    create 600 root root
    rotate 400
    ifempty
    missingok
    dateext
    compress
}
</pre>

	<ul>
	<li>/etc/logrotate.d/nginx</li>
	</ul>


<pre>
/var/log/nginx/*.log {
        daily
        missingok
        rotate 400
        compress
        notifempty
        create 640 nginx adm
        sharedscripts
        postrotate
                [ -f /var/run/nginx.pid ] &#38;&#38; kill -USR1 `cat /var/run/nginx.pid`
        endscript
}
</pre>

<hr />


	<a name="Kickstart設計_システム設計"></a>
<h1 >●システム設計<a href="#Kickstart設計_システム設計" class="wiki-anchor">&para;</a></h1>


	<a name="Kickstart設計_dhcpd設定"></a>
<h2 >dhcpd設定<a href="#Kickstart設計_dhcpd設定" class="wiki-anchor">&para;</a></h2>


	<a name="Kickstart設計_dhcpdconf"></a>
<h3 >dhcpd.conf<a href="#Kickstart設計_dhcpdconf" class="wiki-anchor">&para;</a></h3>


	<p>本システムでは各サーバのIPアドレスが予め決定しているため、<br />MACアドレスによるIPアドレスの固定化を設定する。<br />IPアドレスの割り当て範囲において、<br />zabbix proxyの253, gatewayの254を除いたIPアドレスを設定する。</p>


	<ul>
	<li>/etc/dhcp/dhcpd.conf</li>
	</ul>


	<p><a class="collapsible collapsed" href="#" id="collapse-ea181dc0-show" onclick="$(&#x27;#collapse-ea181dc0-show, #collapse-ea181dc0-hide&#x27;).toggle(); $(&#x27;#collapse-ea181dc0&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-ea181dc0-hide" onclick="$(&#x27;#collapse-ea181dc0-show, #collapse-ea181dc0-hide&#x27;).toggle(); $(&#x27;#collapse-ea181dc0&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-ea181dc0" style="display:none;"><pre>
default-lease-time 600;
max-lease-time 7200;
log-facility local1;
subnet 10.16.33.0 netmask 255.255.255.0 {
  range 10.16.33.1 10.16.33.252;
  option routers 10.16.33.254;
  option broadcast-address 10.16.33.255;
  next-server 10.16.33.82; # クラコン#2の場合は10.16.33.83
  filename "pxelinux.0";
  group {
    host idc-occr01 {
      hardware ethernet 00:0A:F7:7D:33:38;
      fixed-address 10.16.33.82;
    }
    host idc-occr02 {
      hardware ethernet 00:0A:F7:7D:23:DC;
      fixed-address 10.16.33.83;
    }
    host idc-omon01 {
      hardware ethernet B8:2A:72:DF:32:BA;
      fixed-address 10.16.33.146;
    }
    host idc-omon02 {
      hardware ethernet B8:2A:72:DF:39:02;
      fixed-address 10.16.33.147;
    }
    host idc-onas01 {
      hardware ethernet 00:10:18:F5:95:EC;
      fixed-address 10.16.33.194;
    }
    host idc-ohyp01 {
      hardware ethernet 00:0A:F7:7D:33:24;
      fixed-address 10.16.33.98;
    }
    host idc-ohyp02 {
      hardware ethernet 00:0A:F7:7D:33:30;
      fixed-address 10.16.33.99;
    }
    host idc-ohyp03 {
      hardware ethernet 00:0A:F7:7D:34:04;
      fixed-address 10.16.33.100;
    }
    host idc-ohyp04 
      hardware ethernet 00:0A:F7:7D:23:54;
      fixed-address 10.16.33.101;
    }
    host idc-ohyp05 {
      hardware ethernet 00:0A:F7:7D:23:84;
      fixed-address 10.16.33.102;
    }
    host idc-ohyp06 {
      hardware ethernet 00:0A:F7:7D:24:CC;
      fixed-address 10.16.33.103;
    }
    host idc-ohyp07 {
      hardware ethernet 00:0A:F7:7D:34:00;
      fixed-address 10.16.33.104;
    }
  }
}
</pre></div></p>


	<p>上記の設定はクラウド管理NWの設定になるため、<br />dhcpサービスを提供するインターフェイスを制限する。</p>


	<ul>
	<li>/etc/sysconfig/dhcpd</li>
	</ul>


<pre>
DHCPDARGS=bond0
</pre>

	<p><strong>note: lacpを持ったbondingの設定ではpxeブートができない</strong><br /><strong>その場合、L2SW側は通常のVLAN設定のみとしOS側ではbonding mode=1のactive standby方式を選択すること。</strong></p>


	<a name="Kickstart設計_tftpdxined"></a>
<h2 >tftpd(xined)<a href="#Kickstart設計_tftpdxined" class="wiki-anchor">&para;</a></h2>


	<p>tftpサービスを有効化するための設定を行う。</p>


	<ul>
	<li>/etc/xinetd.d/tftp</li>
	</ul>


<pre>
service tftp
{
    socket_type        = dgram
    protocol        = udp
    wait            = yes
    user            = root
    server            = /usr/sbin/in.tftpd
    server_args        = -s /var/lib/tftpboot
    disable            = no
    per_source        = 11
    cps            = 100 2
    flags            = IPv4
}
</pre>

	<p>/var/lib/tftpbootフォルダ配下に以下のファイルを置く。</p>


	<ul>
	<li>pxelinux.0 pxeboot用のイメージ</li>
		<li>CentOS-6.6-x86_64/initrd.img centos関連のイメージ</li>
		<li>CentOS-6.6-x86_64/vmlinuz centos関連のイメージ</li>
		<li>pxelinux.cfg/default 各種イメージパスを記載したファイル</li>
	</ul>


	<p>pxelinux.0はsyslinuxのパッケージから、<br />initrd.img,vmlinuxはcentosのisoイメージからコピーを行う。</p>


	<p>dhcpからtftpサーバのIPアドレスを指定するとまずはpxelinu.0を読み込む。<br />その後pxelinux.cfg配下のMACアドレスやIPアドレスと一致するファイルを取得する。<br />存在しない場合はdefaultファイルを取得する。<br />以下にdefaultファイルを示す。</p>


	<ul>
	<li>/var/lib/tftpboot/pxelinux.cfg/default</li>
	</ul>


<pre>
default CentOS-6.6-x86_64
prompt 1
timeout 10
label CentOS-6.6-x86_64
kernel /CentOS-6.6-x86_64/vmlinuz
append ksdevice=eth0 ks=http://10.16.33.82:8888/pxeboot/ks.cfg initrd=/CentOS-6.6-x86_64/initrd.img
</pre>

	<p>上記ファイルではlabel,kernel,append項目が1セットとなっており、<br />このセットを複数用いてその他のディストリビューション等のインストールも可能となる。</p>


	<a name="Kickstart設計_mrepo"></a>
<h2 >mrepo<a href="#Kickstart設計_mrepo" class="wiki-anchor">&para;</a></h2>


	<p>mrepoはyumパッケージを管理するミラーサイトから各種パッケージをダウンロードし、<br />ローカル環境にミラーリポジトリを作成できるツールである。<br />64bitアーキテクチャを指定する。<br />centosのisoを指定することで自動的にマウントを行う。</p>


	<ul>
	<li>/etc/mrepo.conf</li>
	</ul>


<pre>
[main]
srcdir = /var/mrepo
wwwdir = /var/www/html/mrepo
confdir = /etc/mrepo.conf.d
arch = x86_64
mailto = root@localhost
smtp-server = localhost
</pre>

	<ul>
	<li>/etc/mrepo.conf.d/centos6.6.conf</li>
	</ul>


<pre>
[centos6]
name = CentOS $release ($arch)
release = 6
arch = x86_64
metadata = repomd repoview
iso = CentOS-6.6-x86_64-bin-DVD?.iso
updates    = rsync://ftp.riken.jp/centos/$release/updates/$arch/Packages/
fasttrack  = rsync://ftp.riken.jp/centos/$release/fasttrack/$arch/Packages/
centosplus = rsync://ftp.riken.jp/centos/$release/centosplus/$arch/Packages/
extras     = rsync://ftp.riken.jp/centos/$release/extras/$arch/Packages/
contrib    = rsync://ftp.riken.jp/centos/$release/contrib/$arch/Packages/
epel = rsync://ftp.jaist.ac.jp/pub/Linux/Fedora/epel/$release/$arch/
</pre>

	<a name="Kickstart設計_nginx"></a>
<h2 >nginx<a href="#Kickstart設計_nginx" class="wiki-anchor">&para;</a></h2>


	<p>クラコンでは通常のwebサービスの他にプロキシ等も動作しているため、<br />kickstartで用いるwebサーバはポート8888を利用する。</p>


	<ul>
	<li>/etc/nginx/conf.d/web_etc.conf</li>
	</ul>


<pre>
server {
    listen       8888;
    server_name  10.16.33.82; # クラコン#2の場合は10.16.33.83
    location / {
        root   /var/www/html;
        index  index.html index.htm;
    }
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}
</pre>

	<p>/var/www/html配下の構成を以下に示す。</p>


	<ul>
	<li>additional_package 基本リポジトリには存在しないパッケージや独自rpmファイルを格納するフォルダ</li>
		<li>mrepo mrepoによって自動生成されるyumパッケージを格納したフォルダ（フォルダ配下は省略）</li>
		<li>pxeboot pxeブートで利用するファイル等を格納するフォルダ</li>
		<li>pxeboot/CentOS-6.6-x86_64 isoイメージをマウントしたフォルダ(mrepoにより既にマウントしているた、そこへのシンボリックリンクとなる。)</li>
		<li>pxeboot/ks.cfg kickstart設定ファイル</li>
	</ul>


	<p>kickstartは基本的にOSインストールの役割で利用する。<br />postセクションを用いて多種の設定を行うことも可能であるが、<br />それらは構成管理ツールansibleで管理する。<br />sdaという名前でのOSインストールを行うため、<br />sdaがない場合は失敗する。<br />kickstart設定ファイルの内容を以下に示す。</p>


	<ul>
	<li>/var/www/html/pxeboot/ks.cfg</li>
	</ul>


<pre>
### テキストモードでのインストール
text
### 終了時に再起動を行う
reboot
### 新規インストールを行う 直下のurlとセットでOSイメージのURLを指定する
install
url --url http://10.16.33.1:8888/pxeboot/CentOS-6.6-x86_64
### 言語を日本語に設定
lang ja_JP.UTF-8
### キーボードを日本語に設定
keyboard jp106
### ルートパスワードを設定
rootpw  --iscrypted $6$Pd.3uUWtlhrh7WNV$poYzKEvBRXdBlbIMLv.cXnSn.j61/VQ2uwBOPmLtXJHfnR3BqQpmzlqP8ELBuyMjN6veHxj.c3KE8JLkWAWZa0
### iptablesをオフに設定
firewall --disabled
### 認証の方法を決定
authconfig --enableshadow --passalgo=sha512
### selinuxを無効にする
selinux --disabled
### タイムゾーンを東京に設定
timezone --utc Asia/Tokyo
### ブートローダの設定 デフォルト設定を利用する
bootloader --location=mbr --driveorder=sda --append="crashkernel=auto rhgb quiet" 
### 自動起動の設定
services --enabled ntpd,ntpdate --disabled auditd,haldaemon,ip6tables,iptables,mdmonitor,messagebus,postfix
### サポートされないハードウェアを検知した場合に自動インストールが止まってしまう問題への対策
unsupported_hardware
### sdaのパーティションを削除する
clearpart --drives=sda
### パーティションテーブルの初期化
zerombr
### 以降パーティションの設定 詳細は共通設計を参照のこと
part /boot --fstype=ext4 --size=500
part pv.008002 --grow --size=200
volgroup vg_system --pesize=4096 pv.008002
logvol swap --name=lv_swap --vgname=vg_system --size=12288
logvol / --fstype=ext4 --name=lv_root --vgname=vg_system --size=10240
logvol /home --fstype=ext4 --name=lv_home --vgname=vg_system --size=10240
logvol /tmp --fstype=ext4 --name=lv_tmp --vgname=vg_system --size=10240
logvol /usr --fstype=ext4 --name=lv_usr --vgname=vg_system --size=10240
logvol /var --fstype=ext4 --name=lv_var --vgname=vg_system --size=10240
logvol /var/log --fstype=ext4 --name=lv_var_log --vgname=vg_system --size=51200
### インストールするパッケージグループを指定
%packages
@base
@core
@development
@japanese-support
@server-policy
@workstation-policy
sgpio
device-mapper-persistent-data
telnet
%end
### postセクションではコマンドを実行することができる
### ただし本来はansibleコマンド化するべき内容である
%post
### グループおよびユーザの追加とパスワードの設定
groupadd -g 1001 -p isbadmin isb
useradd -g 1001 -u 1001 ta1
useradd -g 1001 -u 1002 suga
useradd -g 1001 -u 1003 kajiro
echo "ta1:ta1" | chpasswd
echo "suga:suga" | chpasswd
echo "kajiro:kajiro" | chpasswd
### 作成したグループのsudo設定
echo "%isb ALL=(ALL) ALL" &gt;&gt; /etc/sudoers
### sysstatの設定変更
sed -i "s/HISTORY=28/HISTORY=400/g" /etc/sysconfig/sysstat
### ネームサーバの設定
echo "nameserver 10.16.33.83" &gt; /etc/resolv.conf
### sshの秘密鍵の作成
ssh-keygen -q -t rsa -b 2048 -f ~/.ssh/id_rsa -N "" 
%end
</pre>

<hr />


	<a name="Kickstart設計_冗長化"></a>
<h1 >●冗長化<a href="#Kickstart設計_冗長化" class="wiki-anchor">&para;</a></h1>


	<p>kickstartのサービスは2台運用での冗長化を図る。<br />dhcpにより最初にIPアドレスを取得したサーバで以降の処理も進行する。<br />そのためdhcp以降で利用されるサービス(tftp,http)が停止していた場合PXEブートが失敗することになる。<br />ただしOSインストール自体の頻度が多くないことと、<br />内部のサポート的なサービスのため完全な冗長化を図るコストと吊り合わない。<br />そのため、単にサービスを起動しておくだけとする。</p>


<hr />


	<a name="Kickstart設計_運用手順"></a>
<h1 >●運用手順<a href="#Kickstart設計_運用手順" class="wiki-anchor">&para;</a></h1>


	<a name="Kickstart設計_リポジトリの更新"></a>
<h2 >リポジトリの更新<a href="#Kickstart設計_リポジトリの更新" class="wiki-anchor">&para;</a></h2>


	<p>始めにクラコン#1,2のどちらかで現在のバックアップを取得する。<br />ローカルディスクは容量が少ないので、十分注意すること。</p>


<pre>
# cd /var
# tar czf mrepo-`date +%F`.tar.gz mrepo
# mv mrepo-`date +%F`.tar.gz /export/mrepo
</pre>

	<p>次に両系にてリポジトリのデータ更新、およびミラーリポジトリ再構築を行う。<br />その際にディスクの容量について十分注意すること。<br />また、多少時間がかかる(差分にも依るが、一時間内に完了する)。</p>


<pre>
# mrepo -ugv
</pre>

	<p>※現在のリポジトリは2015/06/18 10:30時点のもの。</p>


<hr />


	<a name="Kickstart設計_バックアップ"></a>
<h1 >●バックアップ<a href="#Kickstart設計_バックアップ" class="wiki-anchor">&para;</a></h1>


	<p>本システムではログのバックアップのみ行う。</p>
<hr />
<a name="KVMクラスタ設計" />
<a name="KVMクラスタ設計_KVMクラスタ設計"></a>
<h1 >KVMクラスタ設計<a href="#KVMクラスタ設計_KVMクラスタ設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#KVMクラスタ設計_KVMクラスタ設計">KVMクラスタ設計</a><ul><li><a href="#KVMクラスタ設計_クラスタインフラストラクチャ">クラスタインフラストラクチャ</a><ul><li><a href="#KVMクラスタ設計_クラスタの管理概要cmancluster-manager">クラスタの管理概要　cman(cluster manager)　</a></li><li><a href="#KVMクラスタ設計_定足数ディスク概要">定足数ディスク概要</a></li><li><a href="#KVMクラスタ設計_隔離機能フェンシング概要fenced">隔離機能　フェンシング概要(fenced)</a></li><li><a href="#KVMクラスタ設計_ロックの管理概要">ロックの管理概要</a></li></ul>
</li><li><a href="#KVMクラスタ設計_ISBクラウドにおけるHAクラスタ設計">ISBクラウドにおけるHAクラスタ設計</a><ul><li><a href="#KVMクラスタ設計_クラスタ管理設計">クラスタ管理設計</a></li><li><a href="#KVMクラスタ設計_定足数設計">定足数設計</a></li><li><a href="#KVMクラスタ設計_フェンシング設計">フェンシング設計</a></li><li><a href="#KVMクラスタ設計_定足数ディスク設計">定足数ディスク設計</a></li><li><a href="#KVMクラスタ設計_ヒューリスティック設計">ヒューリスティック設計</a></li><li><a href="#KVMクラスタ設計_障害時動作まとめ">障害時動作まとめ</a></li></ul></li></ul></li></ul>


	<p>本構成ではHAクラスタを使用してKVMホストのクラスタ環境を構築する。本環境でのKVMクラスタは、cman（cluster manager)サービスとclvm（cluster lvm）で構成される。</p>


	<a name="KVMクラスタ設計_クラスタインフラストラクチャ"></a>
<h2 >クラスタインフラストラクチャ<a href="#KVMクラスタ設計_クラスタインフラストラクチャ" class="wiki-anchor">&para;</a></h2>


	<p>HAクラスタでは、サーバ（以下、ノード）の集合体が１つのクラスタとして機能するための基本的な機能（クラスタインフラストラクチャ）を提供する。クラスタインフラストラクチャは以下のコンポーネントで構成される。</p>


	<ul>
	<li>クラスタの管理</li>
		<li>隔離機能</li>
		<li>ロックの管理</li>
		<li>クラスタ設定の管理</li>
	</ul>


	<a name="KVMクラスタ設計_クラスタの管理概要cmancluster-manager"></a>
<h3 >クラスタの管理概要　cman(cluster manager)　<a href="#KVMクラスタ設計_クラスタの管理概要cmancluster-manager" class="wiki-anchor">&para;</a></h3>


	<p>HAクラスタでは、ノードの集合体が1 つのクラスタとして連動して機能するための管理機能（クラスタの管理コンポーネント）として、cman(cluster manager)を使用する。cmanは各ノードをクラスタグループの一員として連動させるためのあくまで「管理機能」となり、クラスターの定足数とクラスタメンバの管理のみを行う。</p>


	<ul>
	<li>クラスタ状態の監視<br />cmanは、他のクラスタノードからのハートビートメッセージによりクラスタメンバを管理する。クラスタメンバに変化が生じると、cmanによって他のコンポーネントに対して「通知」が送られ、他のコンポーネントは変化に応じた動作をとる。一定時間内にメッセージを送信しないクラスタノードがあると、cmanはクラスタからそのノードを取り除き、他のコンポーネントにノードがメンバーではないことを「通知」する。他のコンポーネントはノードがクラスタのメンバではなくなったという通知を受けると、それに応じて実行すべき動作をとる。クラスタノードの中で「マスターノード」と呼ばれる代表ノードが１台選出され、基本的にはマスターノードが実行すべき動作をとる。仮にマスターノードに異常が発生した場合、他のノードがマスターノードに切り替わる。</li>
	</ul>


	<ul>
	<li>クラスタ定足数<br />定足数とはHAクラスタで使用する投票アルゴリズムであり、クラスタ内で稼働するノードのハートビートメッセージを監視することで「定足数」を判断する。クラスタ内で稼働する全ノードの半数以上が稼働している状態を、「定足数を獲得している」と呼び、クラスタ稼働は継続される。半数以下の場合は「定足数に満たない状態」となり、例え正常稼働しているノードが残っていても、全てのクラスタ動作が停止される。定足数は、各クラスタノードがクラスタとして動作を継続するかどうかの判断基準となる。</li>
	</ul>


	<ul>
	<li>定足数の算出<br />クラスタノードにおける定足数の算出は、ハートビートメッセージを確認できるノードの数により決定される。１台のノードにつき「１」づつ「定足数投票数」を持ち、自身を含む定足数投票数の合計値が定足数を満たせていれば正常と判断される。<br />例１<br /><img src="/attachments/download/1279/cman1.png" alt="" /><br />例２<br /><img src="/attachments/download/1280/cman2.png" alt="" /></li>
	</ul>


	<ul>
	<li>スプリットブレインと定足数の関係<br />もし定足数を実施していなかった場合、ノード間で通信エラーが発生したような状況下では、異常があるノード群、ないノード群でクラスタが分断されてしまい、どちらも共有リソースにアクセスしてしまう可能性がある。定足数を実施していれば、定足数を獲得しているノード群が共有リソースにアクセスするため、リソースの整合性は確保される。<br /><img src="/attachments/download/1281/splitbrain.png" alt="" /></li>
	</ul>


	<a name="KVMクラスタ設計_定足数ディスク概要"></a>
<h3 >定足数ディスク概要<a href="#KVMクラスタ設計_定足数ディスク概要" class="wiki-anchor">&para;</a></h3>


	<p>定足数ディスクは、クラスタノードの稼働状況を監視する上でのオプションの機能となる。ノード１がノード２のハートビート通信を受信できなかった場合、自身に問題があるのか、ノード２に問題があるのか認識することはないため、場合によってはお互いがお互いを隔離しようとしてしまい、いわゆる「相打ち」で多数のノードが停止してしまう可能性がある。定足数ディスクとは、それぞれのクラスタノードがハートビートとは別に専用ディスクへ生存情報を書き込み、その書き込みによってクラスタノードの稼働状況を判断する仕組みとなる。ハートビート監視とは別に定足数ディスク用の投票数を設定し、クラスタノードは合計値によって定足数に達しているか判断を行う。<br /><img src="/attachments/download/1290/quorumdisk.png" alt="" /></p>


	<a name="KVMクラスタ設計_隔離機能フェンシング概要fenced"></a>
<h3 >隔離機能　フェンシング概要(fenced)<a href="#KVMクラスタ設計_隔離機能フェンシング概要fenced" class="wiki-anchor">&para;</a></h3>


	<p>cmanによるクラスタ監視において、いずれかのノード通信に異常が発生した場合、cmanはクラスタインフラストラクチャのコンポーネントである「フェンスデーモン fenced 」と通信を行う。cmanから通知を受け取ったfencedは、異常が発生しているノードに対し、定義された方法でノードの切断を行う。なお、ノードの切断は、クラスタグループにおけるマスターノード上のfencedが行う。cmanはあくまでクラスタの監視と異常の通知を行い、fencedが実際に問題のあるノードの切断を行う。<br /><img src="/attachments/download/1284/fenced.png" alt="" /></p>


	<a name="KVMクラスタ設計_ロックの管理概要"></a>
<h3 >ロックの管理概要<a href="#KVMクラスタ設計_ロックの管理概要" class="wiki-anchor">&para;</a></h3>


	<p>ロック管理（ロックマネージャ）は、クラスタインフラストラクチャの他のコンポーネントが共有リソースへのアクセスを同期するための仕組みを提供する機能となる。ロック管理は、dlmデーモンによって行われ、クラスタ内の共有リソースへのアクセスを制御する。ロック管理は単体で使用されるのではなく、他のコンポーネント（clvm)と連動して動作する。clvm(後述）は、ロックマネージャーのロックを使用して (共有ストレージ上の) LVM ボリュームおよびボリュームグループへの更新を同期する。</p>


	<ul>
	<li>clvm<br />clvmはlvmのクラスタリング拡張機能として動作する。クラスタノードは、clvmを使用することによりlvmを使用した共有ストレージの管理が可能となる。clvmデーモンは、クラスタノード内で動作し、クラスタ内でlvmメタデータを同期、各クラスタノードに同一の論理ボリューム情報を提供する。また、clvmはHAクラスタにおけるロックマネージャと連動し、物理ストレージへのアクセスを「ロック」することによって、クラスタノードが論理ボリュームを設定することを可能になる。なお、lvmclvmはあくまで論理ボリュームのクラスタ管理を提供する機能であり、論理ボリューム上に作成されたデータ領域をクラスタ管理するものではない。<br /><img src="/attachments/download/1283/clvm.png" alt="" /></li>
	</ul>


	<a name="KVMクラスタ設計_ISBクラウドにおけるHAクラスタ設計"></a>
<h2 >ISBクラウドにおけるHAクラスタ設計<a href="#KVMクラスタ設計_ISBクラウドにおけるHAクラスタ設計" class="wiki-anchor">&para;</a></h2>


	<a name="KVMクラスタ設計_クラスタ管理設計"></a>
<h3 >クラスタ管理設計<a href="#KVMクラスタ設計_クラスタ管理設計" class="wiki-anchor">&para;</a></h3>


クラスタノード間の死活監視は、以下の3種類の監視を組み合わせ定足数を算出する。
	<table>
		<tr style="background:#d3eaf3;">
			<th>監視方法</th>
			<th>　概要</th>
		</tr>
		<tr>
			<td>ハートビート通信</td>
			<td>各クラスタノードがハートビート用のマルチキャストパケットを投げ、お互いを監視し合う。なお、ハートビート通信に使用するポートは物理ポート数の問題からクラウド管理ネットワーク用ポートを兼用する。</td>
		</tr>
		<tr>
			<td>定足数ディスクへの書き込み</td>
			<td>各クラスタノードがquorum diskへ生存情報へ書き込むことでお互いの生存を確認する。</td>
		</tr>
		<tr>
			<td>ヒューリスティック</td>
			<td>クラウド管理ネットワークのゲートウェイアドレスへpingを打ち、自身の生存を確認する。</td>
		</tr>
	</table>




	<p><img src="/attachments/download/1288/heartbeat.png" alt="" /></p>


	<a name="KVMクラスタ設計_定足数設計"></a>
<h3 >定足数設計<a href="#KVMクラスタ設計_定足数設計" class="wiki-anchor">&para;</a></h3>


	<p>定足数の定義はハートビート通信、定足数ディスクへの書き込みそれぞれに対しvotesを定義し、合計した値を定足数とする。なお、本環境におけるクラスタノードの最低稼働台数は1台とし、1台のみ稼働状態でも継続稼働可能な投票数を割り当てる為、定足数ディスクにクラスタノード台数分の投票数を持たせる。つまり、本環境におけるvotesの合計値は「14」となり、定足数は過半数である「8」となる。</p>


	<table>
		<tr style="background:#d3eaf3;">
			<th>項目</th>
			<th>値</th>
			<th>概要</th>
		</tr>
		<tr>
			<td>ハートビートタイムアウト値</td>
			<td>20秒</td>
			<td>クラスタノードが死亡判定されるまでの時間</td>
		</tr>
		<tr>
			<td>votes合計</td>
			<td>14</td>
			<td>クラスタノードが持つvotesとquorum diskが持つvotesの合計値。</td>
		</tr>
		<tr>
			<td>クラスタノードvotes</td>
			<td>1</td>
			<td>クラスタノード1台に割り当てられるvotes数。クラスタノード１台とハートビート通信が成功した場合、1が加算される。</td>
		</tr>
		<tr>
			<td>定足数ディスクvotes</td>
			<td>7</td>
			<td>定足数ディスクに割り当てられるvotes数。定足数ディスクへの書き込みが成功した場合、7が加算される。</td>
		</tr>
		<tr>
			<td>定足数</td>
			<td>8</td>
			<td>全votes数の過半数。クラスタノードはこの値を持っていれば継続稼働することができる。</td>
		</tr>
		<tr>
			<td>最低稼働台数</td>
			<td>1</td>
			<td>クラスタノードvotes 1 + quorum votes 7 = 8となり、稼働台数1台でも定足数に達する為、継続稼働可能となる。</td>
		</tr>
	</table>




	<a name="KVMクラスタ設計_フェンシング設計"></a>
<h3 >フェンシング設計<a href="#KVMクラスタ設計_フェンシング設計" class="wiki-anchor">&para;</a></h3>


前述のハートビート通信において、いずれかのノードで監視が一定時間失敗した場合、定義された設定に基づき、クラスタノード内のマスターノードが異常ノードに対してクラスタからの切断処理を実施する。これを「フェンシング」と呼ぶ。本環境では、fencing動作の定義(fenceデバイス)としてfence_drac5を使用する。fence_drac5は、クラスタノード障害発生時、fence_drac5エージェントが障害ノードのIPMI用アドレスにssh接続し、シャットダウンコマンドを実行することで障害発生ノードを物理的に停止させる。
	<table>
		<tr style="background:#d3eaf3;">
			<th>項目</th>
			<th>値</th>
			<td>概要</td>
		</tr>
		<tr>
			<td>action</td>
			<td>off</td>
			<td>フェンシングの動作。ノードをシャットダウンする。</td>
		</tr>
		<tr>
			<td>cmd_prompt</td>
			<td>admin1-&gt;</td>
			<td>fence_drac5のお約束。</td>
		</tr>
		<tr>
			<td>debug</td>
			<td>/var/log/cluster/fence_drac5.log</td>
			<td>フェンシング実行時のデバッグログ保存先。</td>
		</tr>
		<tr>
			<td>ipaddr</td>
			<td>10.16.34.**</td>
			<td>IPMIにssh接続する際のIPアドレス。kvm#1~7それぞれのアドレスを個別に設定する。</td>
		</tr>
		<tr>
			<td>login</td>
			<td>root</td>
			<td>iDRACログインユーザ</td>
		</tr>
		<tr>
			<td>passwd</td>
			<td>calvin</td>
			<td>iDRACログインユーザパス</td>
		</tr>
		<tr>
			<td>name</td>
			<td>ipmifence*</td>
			<td></td>
		</tr>
		<tr>
			<td>power_wait</td>
			<td>60</td>
			<td></td>
		</tr>
		<tr>
			<td>secure</td>
			<td>on</td>
			<td></td>
		</tr>
		<tr>
			<td>verbose</td>
			<td>on</td>
			<td></td>
		</tr>
	</table>




	<p><img src="/attachments/download/1289/fencing.png" alt="" /></p>


	<a name="KVMクラスタ設計_定足数ディスク設計"></a>
<h3 >定足数ディスク設計<a href="#KVMクラスタ設計_定足数ディスク設計" class="wiki-anchor">&para;</a></h3>


各クラスタノードは、ハートビートによる死活監視の他、予め決められた設定内容に従い定足数ディスクへの生存情報書き込みを行い死活監視を行う。生存情報の書き込みが一定期間失敗した場合、失敗したノードに対してフェンシングを実行または、書き込めないことに自分で気づいた場合は自発的に再起動する。定足数ディスクはiSCSIストレージ上に存在する為、この監視はストレージネットワークを監視する側面も持つ。なお、この監視はハートビート通信よりも信頼度が高い為、ハートビートの死亡判定タイムアウト値よりも短い値を設定する。定足数ディスクの各種設定を以下に示す。
	<table>
		<tr style="background:#d3eaf3;">
			<th>項目</th>
			<th>値</th>
			<td>概要</td>
		</tr>
		<tr>
			<td>label</td>
			<td>idc-02</td>
			<td>quorum diskの名前</td>
		</tr>
		<tr>
			<td>diskサイズ</td>
			<td>1GB</td>
			<td>quorum disk（ボリューム領域）のサイズ。値の大小は関係ないが一応1GBで設定。</td>
		</tr>
		<tr>
			<td>interval</td>
			<td>1秒</td>
			<td>クラスタノードが生存情報を書き込む間隔</td>
		</tr>
		<tr>
			<td>tko</td>
			<td>10</td>
			<td>何回連続して生存情報が確認できない場合に死亡判定するかの閾値</td>
		</tr>
	</table>




	<p><img src="/attachments/download/1290/quorumdisk.png" alt="" /></p>


	<a name="KVMクラスタ設計_ヒューリスティック設計"></a>
<h3 >ヒューリスティック設計<a href="#KVMクラスタ設計_ヒューリスティック設計" class="wiki-anchor">&para;</a></h3>


前述の通り、ハートビート監視の他に定足数ディスクへの書き込みで二重に監視を実施しているが、定足数ディスクへの書き込みに対する付加的な監視方法として、クラウド管理ネットワークへのping確認を行う「ヒューリスティック設定」を入れ、一定期間ping疎通が失敗した場合は、自発的に再起動させる監視も加える。これは他のどの監視よりも優先されるべきものとなり、障害検知時、まず最初に実行されるアクションとなる。
	<table>
		<tr style="background:#d3eaf3;">
			<th>項目</th>
			<th>値</th>
			<th>概要</th>
		</tr>
		<tr>
			<td>interval</td>
			<td>2秒</td>
			<td>ヒューリスティック監視の間隔</td>
		</tr>
		<tr>
			<td>program</td>
			<td>ping -c1 -w1 10.16.33.254</td>
			<td>ヒューリスティック監視の内容。クラウド管理セグメントのGWに対しpingを実行する。</td>
		</tr>
		<tr>
			<td>tko</td>
			<td>3</td>
			<td>死亡判定までの失敗回数</td>
		</tr>
	</table>




	<p><img src="/attachments/download/1292/heuristic.png" alt="" /></p>


	<a name="KVMクラスタ設計_障害時動作まとめ"></a>
<h3 >障害時動作まとめ<a href="#KVMクラスタ設計_障害時動作まとめ" class="wiki-anchor">&para;</a></h3>


HAクラスタ障害時の動作は、それぞれ監視パターンにより異なる。パターン毎の障害動作を以下に示す。
	<table>
		<tr style="background:#d3eaf3;">
			<th>トリガー</th>
			<th>タイムアウト値</th>
			<th>動作</th>
			<th>概要</th>
		</tr>
		<tr>
			<td>ハートビート通信が20秒間途絶えた場合</td>
			<td>20秒</td>
			<td>フェンシング</td>
			<td>分断されたグループのうち、votes数の合計値が最大のグループに属するノードが生き残り、その他のノードはfence設定に従いfencingする。qdiskタイムアウトの2倍がredhat推奨値</td>
		</tr>
		<tr>
			<td>1秒ごとの定足数ディスクへの書き込みが10回連続で失敗（10秒間）失敗した場合</td>
			<td>10秒</td>
			<td>フェンシング,自爆</td>
			<td>fence設定に従いfencingする。または10回連続で書き込めないことに自分で気づいた場合は自発的に再起動する。</td>
		</tr>
		<tr>
			<td>クラウド管理NWのGW(10.16.33.254)へ2秒間隔でpingを飛ばし、3回連続(6秒間）失敗した場合</td>
			<td>6秒</td>
			<td>自爆</td>
			<td>管理ネットワークとハートビートポートを兼用している場合の対応策。自発的に再起動する。</td>
		</tr>
	</table>
<hr />
<a name="KVMホスト運用手順" />
<a name="KVMホスト運用手順_KVMホスト運用手順"></a>
<h1 >KVMホスト運用手順<a href="#KVMホスト運用手順_KVMホスト運用手順" class="wiki-anchor">&para;</a></h1>


	<a name="KVMホスト運用手順_サービス起動停止順序"></a>
<h2 >サービス起動・停止順序<a href="#KVMホスト運用手順_サービス起動停止順序" class="wiki-anchor">&para;</a></h2>


	<p>クラスタ関連のサービスは以下の順序で起動・停止する。順序を間違えるとサービスがハングするので注意。</p>


	<ul>
	<li>起動</li>
	<ol>
	<li>iscsi関連およびehcmd　※OS起動時に自動起動・</li>
		<li>cman　※クラスタノード全体で同時に起動する。タイミングがずれると最後に立ち上がったノードがフェンスされるので注意。</li>
		<li>clvmd ※cmanが起動していないと起動しない。</li>
		<li>ricci,luci</li>
		<li>libvirtd</li>
	</ol></li>
	</ul>


	<ul>
	<li>停止</li>
	<ol>
	<li>ricci,luci</li>
		<li>libvirtd　</li>
		<li>clvmd</li>
		<li>cman　※clvmを停止しないと停止できない。</li>
	</ol></li>
	</ul>


<hr />


	<a name="KVMホスト運用手順_KVMホスト追加手順"></a>
<h2 >KVMホスト追加手順<a href="#KVMホスト運用手順_KVMホスト追加手順" class="wiki-anchor">&para;</a></h2>


	<a name="KVMホスト運用手順_1既存kvmホスト側作業"></a>
<h3 >1.既存kvmホスト側作業<a href="#KVMホスト運用手順_1既存kvmホスト側作業" class="wiki-anchor">&para;</a></h3>


	<a name="KVMホスト運用手順_etcclusterclusterconfの編集"></a>
<h3 >/etc/cluster/cluster.confの編集<a href="#KVMホスト運用手順_etcclusterclusterconfの編集" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>クラスタのノードIDを確認。<br /><pre>
[root@idc-ohyp03 suga]# cman_tool nodes
Node  Sts   Inc   Joined               Name
   0   M      0   2015-06-19 17:26:42  /dev/block/253:8
   3   M   4064   2015-06-19 17:26:36  idc-ohyp03
   4   M   4068   2015-06-19 17:26:37  idc-ohyp04
   5   M   4112   2015-06-19 17:26:38  idc-ohyp05
   6   M   4120   2015-06-19 17:26:40  idc-ohyp06
   7   M   4128   2015-06-25 09:57:32  idc-ohyp07

</pre></li>
	</ul>


	<ul>
	<li>マスターノードを確認。<br /><pre>
[root@idc-ohyp03 suga]# cman_tool services
fence domain
member count  5
victim count  0
victim now    0
master nodeid 3　　★マスターノードはノードID:3=idc-ohyp03
wait state    none
members       3 4 5 6 7 

dlm lockspaces
name          clvmd
id            0x4104eefa
flags         0x00000000 
change        member 5 joined 1 remove 0 failed 0 seq 7,7
members       3 4 5 6 7 
</pre></li>
	</ul>


	<ul>
	<li>マスターノードの/etc/cluster/cluster.confに新規追加するkvmホスト情報を追記、編集する。追加するkvmホストの分だけ数値を増やす。<br /><pre>
&lt;cluster config_version="4" name="idc-ohyp_clust"&gt;　★[config_version]の数値を増やす。
        &lt;cman expected_votes="7" quorum_dev_poll="21000"/&gt;　★[expected_votes]の数値を増やす。
</pre></li>
	</ul>


	<ul>
	<li>追加するkvmホストの分だけclusternode記述を追記。以下で１ホスト分の記述となる。<br />[clusternode]の追加。[clusternode name]にホスト名、[nodeid]にホストの通し番号、[device name]にipmifence+ホストの通し番号。<br />いずれの値も他のclusternodeとかぶらないようにする。<br /><pre>
                &lt;clusternode name="kvmホスト名" nodeid="ノードID" votes="1"&gt;
                        &lt;fence&gt;
                                &lt;method name="1"&gt;
                                        &lt;device name="ipmifence+ノードID"/&gt;
                                &lt;/method&gt;
                        &lt;/fence&gt;
                &lt;/clusternode&gt;

※記述例
                &lt;clusternode name="idc-ohyp01" nodeid="1" votes="1"&gt;
                        &lt;fence&gt;
                                &lt;method name="1"&gt;
                                        &lt;device name="ipmifence1"/&gt;
                                &lt;/method&gt;
                        &lt;/fence&gt;
                &lt;/clusternode&gt;
</pre></li>
	</ul>


	<ul>
	<li>追加するkvmホストの分だけfencedevice記述を追記。以下で１ホスト分の記述となる。<br />[fencedevices]の追加。[ipaddr]にkvmホストのipmi用アドレス、[name]にclusternodeの[device name]で指定した名前。いずれの値も他のfencedeviceとかぶらないようにする。<br /><pre>
 &lt;fencedevice action="off" agent="fence_drac5" cmd_prompt="admin1-&amp;gt;" debug="/var/log/cluster/fence_drac5.log" ipaddr="ipmi用アドレス" login="root" name="ipmifence3" passwd="calvin" power_wait="60" secure="on" verbose="on"/&gt;

※記述例
&lt;fencedevice action="off" agent="fence_drac5" cmd_prompt="admin1-&amp;gt;" debug="/var/log/cluster/fence_drac5.log" ipaddr="10.16.34.98" login="root" name="ipmifence1" passwd="calvin" power_wait="60" secure="on" verbose="on"/&gt;

</pre></li>
	</ul>


	<ul>
	<li>クラスタノードへ変更内容を反映。cluster.confを編集したkvmホスト上で実行する。cman/clvmdの再起動は必要ない（はず）。<br />※全てのクラスタノードでricciが立ち上がっていること。<br /><pre>
cman_tool version -r
</pre></li>
	</ul>


	<a name="KVMホスト運用手順_2追加するkvmホスト側作業"></a>
<h3 >2.追加するkvmホスト側作業<a href="#KVMホスト運用手順_2追加するkvmホスト側作業" class="wiki-anchor">&para;</a></h3>


	<a name="KVMホスト運用手順_clusterconfを配置"></a>
<h3 >cluster.confを配置<a href="#KVMホスト運用手順_clusterconfを配置" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>マスターノードのcluster.confを追加するkvmホストへコピーする。<br /><pre>
scp /etc/cluster/cluster.conf kvmmanager@idc-ohyp*:/home/kvmmanager/

cp /home/kvmmanager/cluster.conf /etc/cluster/cluster.conf
</pre></li>
	</ul>


	<a name="KVMホスト運用手順_etclvmlvmconfの編集"></a>
<h3 >/etc/lvm/lvm.confの編集<a href="#KVMホスト運用手順_etclvmlvmconfの編集" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>編集前の状態を確認。<br /><pre>
[root@idc-ohyp07 suga]# lvscan
  ACTIVE            '/dev/vg_system/lv_swap' [12.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_root' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_home' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_tmp' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_usr' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_var' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_var_log' [50.00 GiB] inherit
[root@idc-ohyp07 suga]# 

</pre></li>
	</ul>


	<ul>
	<li>LVMボリュームのロックタイプをクラスタ用に変更<br /><pre>
変更前　locking_type = 1
変更後　locking_type = 3
</pre></li>
	</ul>


	<ul>
	<li>編集後の状態を確認。警告メッセージが出力されていることを確認。<br /><pre>
[root@idc-ohyp07 suga]# lvscan
  connect() failed on local socket: そのようなファイルやディレクトリはありません
  Internal cluster locking initialisation failed.
  WARNING: Falling back to local file-based locking.
  Volume Groups with the clustered attribute will be inaccessible.
  ACTIVE            '/dev/vg_system/lv_swap' [12.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_root' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_home' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_tmp' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_usr' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_var' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_var_log' [50.00 GiB] inherit
[root@idc-ohyp07 suga]# service cman start

</pre></li>
	</ul>


	<a name="KVMホスト運用手順_iSCSIターゲットへのログイン"></a>
<h3 >iSCSIターゲットへのログイン<a href="#KVMホスト運用手順_iSCSIターゲットへのログイン" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>現在の状態を確認。どこにもログインしていないことを確認。<br /><pre>
[root@idc-ohyp07 suga]# ehcmcli status
Generating diagnostic data, please wait...

========================================================
Adapter List
========================================================
  Name: eth7
    IP Address: 10.16.42.104
    HW addr: 00:0A:F7:7B:4E:67

  Name: eth3
    IP Address: 10.16.41.104
    HW addr: 00:0A:F7:7D:34:03

========================================================
Volume list
========================================================
========================================================
Summary
========================================================
  Adapters:         2
  Managed Volumes:  0
  iSCSI Sessions:   0
  Errors:           0
  Warnings:         0
  Suggestions:      0

</pre></li>
	</ul>


	<ul>
	<li>ターゲットの検索。<br /><pre>
iscsiadm -m discovery -t sendtargets -p 10.16.24.129
</pre></li>
	</ul>


	<ul>
	<li>不要なターゲットが検索されてしまった場合は、以下で削除。<br /><pre>
iscsiadm -m node -o delete -T [ターゲット名]
</pre></li>
	</ul>


	<ul>
	<li>ターゲットへログイン。kvm用ボリュームとquorum diskの２つのターゲットへログインする。<br /><pre>
ehcmcli login -T [ターゲット名] --login-at-boot
</pre></li>
	</ul>


	<ul>
	<li>ログイン確認。eth3とeth7の２つのインターフェース(パルチパス構成)で接続していることを確認。一つのターゲットに対し２つの接続となるので<br />本環境では合計４つの接続が確認できる。マルチパス接続が完了するまでは少し時間がかかる(１０分程度)。<br /><pre>
[root@idc-ohyp03 suga]# ehcmcli status
Generating diagnostic data, please wait...

========================================================
Adapter List
========================================================
  Name: eth7
    IP Address: 10.16.42.100
    HW addr: 00:0A:F7:7B:4C:B7

  Name: eth3
    IP Address: 10.16.41.100
    HW addr: 00:0A:F7:7D:34:07

========================================================
Volume list
========================================================
  Volume: kvm-test
    Target name: iqn.2001-05.com.equallogic:8-661fc6-44bafabbe-8170059be765583a-kvm-test
    Device to mount: /dev/eql/kvm-test
    Status: Normal: Per-member session count reduced due to number of available host and array NICs
    Session: 10    /dev/sdc   10.16.42.100   -&gt;   10.16.24.130   5d 19:23:52
    Session: 13    /dev/sdb   10.16.41.100   -&gt;   10.16.25.130   5d 19:19:47

  Volume: test3-qdisk
    Target name: iqn.2001-05.com.equallogic:8-661fc6-38cafabbe-f220059be6b5580d-test3-qdisk
    Device to mount: /dev/eql/test3-qdisk
    Status: Normal: Per-member session count reduced due to number of available host and array NICs
    Session: 11    /dev/sdd   10.16.42.100   -&gt;   10.16.25.130   5d 19:20:26
    Session: 12    /dev/sde   10.16.41.100   -&gt;   10.16.24.130   5d 19:20:26

========================================================
Summary
========================================================
  Adapters:         2
  Managed Volumes:  2
  iSCSI Sessions:   4
  Errors:           0
  Warnings:         0
  Suggestions:      0

</pre></li>
	</ul>


	<a name="KVMホスト運用手順_cmanclvmd-起動"></a>
<h3 >cman/clvmd 起動<a href="#KVMホスト運用手順_cmanclvmd-起動" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>cman→clvmdの順で起動する。<br /><pre>
[root@idc-ohyp01 suga]# service cman start
Starting cluster: 
   Checking if cluster has been disabled at boot...        [  OK  ]
   Checking Network Manager...                             [  OK  ]
   Global setup...                                         [  OK  ]
   Loading kernel modules...                               [  OK  ]
   Mounting configfs...                                    [  OK  ]
   Starting cman...                                        [  OK  ]
   Starting qdiskd...                                      [  OK  ]
   Waiting for quorum...                                   [  OK  ]
   Starting fenced...                                      [  OK  ]
   Starting dlm_controld...                                [  OK  ]
   Tuning DLM kernel config...                             [  OK  ]
   Starting gfs_controld...                                [  OK  ]
   Unfencing self...                                       [  OK  ]
   Joining fence domain...                                 [  OK  ]

[root@idc-ohyp01 suga]# service clvmd start
Starting clvmd: 
Activating VG(s):   4 logical volume(s) in volume group "vg_idc-01" now active
  7 logical volume(s) in volume group "vg_system" now active
                                                           [  OK  ]
</pre></li>
	</ul>


	<a name="KVMホスト運用手順_稼働確認"></a>
<h3 >稼働確認<a href="#KVMホスト運用手順_稼働確認" class="wiki-anchor">&para;</a></h3>


<pre>
cman_tool nodes

cman_tool status

ccs_tool lsnode

ccs_tool lsfence

pvscan

vgscan

lvscan

</pre>
<hr />
<a name="KVM仮想マシンテンプレートからのリストア" />
<a name="KVM仮想マシンテンプレートからのリストア_KVM仮想マシンテンプレートからのリストア"></a>
<h1 >KVM仮想マシンテンプレートからのリストア<a href="#KVM仮想マシンテンプレートからのリストア_KVM仮想マシンテンプレートからのリストア" class="wiki-anchor">&para;</a></h1>


	<a name="KVM仮想マシンテンプレートからのリストア_KVM仮想マシンテンプレート作成"></a>
<h3 >KVM仮想マシンテンプレート作成<a href="#KVM仮想マシンテンプレートからのリストア_KVM仮想マシンテンプレート作成" class="wiki-anchor">&para;</a></h3>


	<p>テンプレートとして保存する為に必要なファイルは以下の2種類<br />#仮想マシンディスクファイル<br />#仮想マシン構成ファイル</p>


	<p>virsh cloneコマンドにてクローンを作成する。クローンとして作成する仮想マシンは基本的にimg形式で出力する。<br />※クローン元がLVMブロックデバイスであっても、img形式で出力することが可能。<br /><pre>
virt-clone --original &lt;仮想マシン名&gt; --name &lt;クローン仮想マシン名&gt; --file /***/***/&lt;出力ファイル名.img&gt;
</pre></p>


	<p>クローン作成後、出力先ディレクトリにimgファイルが作成されていることを確認。<br />その後、以下のディレクトリ内のxmlファイルをimgファイルの出力先にコピーする。<br /><pre>
cp -p /etc/libvirt/qemu/&lt;クローン仮想マシン名.xml&gt; /***/***/&lt;クローン仮想マシン名.xml&gt;
</pre></p>


	<p>クローン作成後、仮想マシン一覧にクローン仮想マシンも登録されてしまう為、邪魔なようであれば<br />以下コマンドで仮想マシン登録を削除する。<br />※以下のコマンド実行後にxmlファイルが削除される為、必ず実行前にファイルをコピーしておく。<br /><pre>
[root@kvm-01 bkp]# virsh list --all
 Id    名前                         状態
----------------------------------------------------
 -     test1                          シャットオフ
 -     test1_clone                    シャットオフ　★邪魔
 -     test3                          シャットオフ
 -     vrtest1                        シャットオフ
 -     vrtest1                        シャットオフ

[root@kvm-01 bkp]# virsh undefine test1_clone
ドメイン test1_clone の定義が削除されました

[root@kvm-01 bkp]# virsh list --all
 Id    名前                         状態
----------------------------------------------------
 -     test1                          シャットオフ
 -     test3                          シャットオフ
 -     vrtest1                        シャットオフ
 -     vrtest1                        シャットオフ

</pre></p>


	<a name="KVM仮想マシンテンプレートからのリストア_仮想マシンをテンプレートからリストア"></a>
<h3 >仮想マシンをテンプレートからリストア<a href="#KVM仮想マシンテンプレートからのリストア_仮想マシンをテンプレートからリストア" class="wiki-anchor">&para;</a></h3>


	<p>仮想マシンテンプレートを使ってddコマンドで戻す。テンプレートがimg形式であったとしても<br />LVMブロックデバイスに出力可能！</p>


	<p>クローン仮想マシンを格納するLVMボリューム領域を作成する。<br /><pre>
lvcreate -L 30G /dev/ISCSIVolGroup00 -n test1_clone　※PV,VG等は例
</pre></p>


	<p>ddコマンドでLVMボリューム領域に仮想マシンデータを復元。<br /><pre>
dd if=/**/**/&lt;クローン仮想マシン名.img&gt; of=/dev/ISCSIVolGroup00/test1_clone obs=1024k count=1000000  ※PV,VG等は例
</pre></p>


	<p>仮想マシン定義ファイルを/etc/libvirtd/qemu/にコピー<br /><pre>
cp -p /***/***/&lt;クローン仮想マシン名.xml&gt; /etc/libvirt/qemu/&lt;クローン仮想マシン名.xml&gt;
</pre></p>


	<p>仮想マシン定義ファイルを編集。<br /><pre>
編集前
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='raw' cache='none' io='native'/&gt;
      &lt;source file='/**/**/&lt;クローン仮想マシン名.img&gt;'/&gt;　　★クローンしたディレクトリ内のimgファイルが指定されている。
      &lt;target dev='hda' bus='ide'/&gt;
編集後
    &lt;disk type='block' device='disk'&gt;　★ディスクタイプをblockに指定。
      &lt;driver name='qemu' type='raw' cache='none' io='native'/&gt;
      &lt;source dev='/dev/ISCSIVolGroup00/clone1'/&gt;　　★source devに変更し、LVMボリューム領域を指定。
      &lt;target dev='hda' bus='ide'/&gt;
</pre><br />仮想マシン定義ファイルを登録<br /><pre>
virsh define &lt;クローン仮想マシン名&gt;
</pre></p>


	<p>仮想マシン起動<br /><pre>
virsh start &lt;クローン仮想マシン名&gt;
</pre></p>


	<p>OS起動後、/etc/sysconfig/network-scripts/ifcfg-eth*　のMACアドレスを、仮想マシン定義ファイルに書かれているものに書き換える。<br />この作業をしないとインターフェースがupしない。<br /><pre>
&lt;クローン仮想マシン名.xml&gt;からインターフェース項目を抜粋

    &lt;interface type='bridge'&gt;
      &lt;mac address='52:54:00:3d:80:13'/&gt;　★このMACアドレスが最新。
      &lt;source bridge='br0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&gt;
    &lt;/interface&gt;
    &lt;interface type='bridge'&gt;
      &lt;mac address='52:54:00:c1:6a:33'/&gt;　★このMACアドレスが最新。
      &lt;source bridge='br1'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/&gt;
    &lt;/interface&gt;</p></pre>
<hr />
<a name="KVM設計" />
<a name="KVM設計_KVM設計"></a>
<h1 >KVM設計<a href="#KVM設計_KVM設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#KVM設計_KVM設計">KVM設計</a></li><li><a href="#KVM設計_Introduction">●Introduction</a><ul><li><a href="#KVM設計_目的">目的</a></li><li><a href="#KVM設計_定義範囲">定義範囲</a></li></ul>
</li><li><a href="#KVM設計_構成">●構成</a><ul><li><a href="#KVM設計_ハードウェア概要">ハードウェア概要</a></li></ul>
</li><li><a href="#KVM設計_設計">●設計</a><ul><li><a href="#KVM設計_KVMホスト設計">KVMホスト設計</a><ul><li><a href="#KVM設計_導入ソフトウェア">導入ソフトウェア</a></li><li><a href="#KVM設計_libvirt設定">libvirt設定</a></li><li><a href="#KVM設計_qemuconf設定">qemu.conf設定</a></li><li><a href="#KVM設計_lvmconf設定etclvmlvmconf">lvm.conf設定(/etc/lvm/lvm.conf)</a></li><li><a href="#KVM設計_ricci設定">ricci設定</a></li><li><a href="#KVM設計_luci設定">luci設定</a></li><li><a href="#KVM設計_サービス自動起動">サービス自動起動</a></li></ul>
</li><li><a href="#KVM設計_共用ストレージ設計">共用ストレージ設計</a><ul><li><a href="#KVM設計_equallogicへの接続">equallogicへの接続</a></li><li><a href="#KVM設計_KVM用ボリューム">KVM用ボリューム</a></li><li><a href="#KVM設計_iscsiイニシエータ設定">iscsiイニシエータ設定</a></li><li><a href="#KVM設計_Host-Integration-Tools-for-Linux設定">Host Integration Tools for Linux設定</a></li></ul></li></ul></li></ul>


	<a name="KVM設計_Introduction"></a>
<h1 >●Introduction<a href="#KVM設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="KVM設計_目的"></a>
<h2 >目的<a href="#KVM設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>本書は、クラウドサービス向けIaaS基盤における事業者クラウド領域を提供する為の仮想化ハイパーバイザについて、その仕様および設計を定義するものである。</p>


	<a name="KVM設計_定義範囲"></a>
<h2 >定義範囲<a href="#KVM設計_定義範囲" class="wiki-anchor">&para;</a></h2>


	<p>本書における定義範囲は、事業者へクラウド環境を提供する為の仮想化基盤、クラウド環境を格納する為のストレージ領域、およびそれらを構成する為のネットワーク構成とする。</p>


	<a name="KVM設計_構成"></a>
<h1 >●構成<a href="#KVM設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="KVM設計_ハードウェア概要"></a>
<h2 >ハードウェア概要<a href="#KVM設計_ハードウェア概要" class="wiki-anchor">&para;</a></h2>


	<p>仮想化基盤は以下のハードウェアで構成される。</p>


	<table>
		<tr style="background:#d3eaf3;">
			<th>メーカ</th>
			<th>モデル名</th>
			<th>数量</th>
			<th>用途</th>
		</tr>
		<tr>
			<td>DELL</td>
			<td>PowerEdge R420</td>
			<td>7</td>
			<td>仮想化ハイパーバイザー用サーバ</td>
		</tr>
		<tr>
			<td>DELL</td>
			<td>Equallogic PS6210X</td>
			<td>2</td>
			<td>仮想マシン領域用共用ストレージ</td>
		</tr>
	</table>




	<a name="KVM設計_設計"></a>
<h1 >●設計<a href="#KVM設計_設計" class="wiki-anchor">&para;</a></h1>


	<a name="KVM設計_KVMホスト設計"></a>
<h2 >KVMホスト設計<a href="#KVM設計_KVMホスト設計" class="wiki-anchor">&para;</a></h2>


	<a name="KVM設計_導入ソフトウェア"></a>
<h3 >導入ソフトウェア<a href="#KVM設計_導入ソフトウェア" class="wiki-anchor">&para;</a></h3>


	<p>KVMホストとなるサーバへ導入するソフトを以下に示す。</p>


	<table>
		<tr style="background:#d3eaf3;">
			<th>名前</th>
			<th>用途</th>
			<th>　概要</th>
		</tr>
		<tr>
			<td>virtualization</td>
			<td>kvm</td>
			<td>仮想化マシンのホスティング環境を提供</td>
		</tr>
		<tr>
			<td>virtualization-client</td>
			<td>kvm</td>
			<td>仮想化インスタンスのインストールおよび管理クライアント</td>
		</tr>
		<tr>
			<td>virtualization-platform</td>
			<td>kvm</td>
			<td>仮想マシンおよびコンテナーのアクセスおよび制御インターフェースを提供</td>
		</tr>
		<tr>
			<td>virtualization-tools</td>
			<td>kvm</td>
			<td>オフラインでの仮想イメージ管理のツール</td>
		</tr>
		<tr>
			<td>cman</td>
			<td>linuxクラスタ</td>
			<td>サーバの集合体が１つのクラスタとして連動して機能するための基本機能を提供する。この上でlvmクラスタ機能やフェンス機能等のコンポーネントを動かす。</td>
		</tr>
		<tr>
			<td>clvm（Cluster Logical Volume Manager）</td>
			<td>linuxクラスタ</td>
			<td>cman上で動作するオプションのコンポーネント。クラスタノードが使用するストレージのボリュームを管理する。</td>
		</tr>
		<tr>
			<td>lvm2-cluster</td>
			<td>linuxクラスタ</td>
			<td>クラスタ環境での論理ボリューム管理（LVM）</td>
		</tr>
		<tr>
			<td>ccs</td>
			<td>linuxクラスタ</td>
			<td>クラスタノード管理ソフト</td>
		</tr>
		<tr>
			<td>ricci</td>
			<td>linuxクラスタ</td>
			<td>クラスタ設定管理ソフト</td>
		</tr>
		<tr>
			<td>luci</td>
			<td>linuxクラスタ</td>
			<td>クラスタノードのGUI管理</td>
		</tr>
		<tr>
			<td>iscsi-initiator-utils</td>
			<td>ストレージ接続</td>
			<td>iscsiイニシエータ</td>
		</tr>
		<tr>
			<td>Host Integration Tools for Linux</td>
			<td>ストレージ接続</td>
			<td>Equallogic接続管理ソフト。マルチパス接続する際は必須となり、isoイメージファイルから各種パッケージを導入する。</td>
		</tr>
		<tr>
			<td>expect</td>
			<td></td>
		</tr>
	</table>




	<a name="KVM設計_libvirt設定"></a>
<h3 >libvirt設定<a href="#KVM設計_libvirt設定" class="wiki-anchor">&para;</a></h3>


	<p>/etc/libvirt/libvirt.confの設定を以下に定義する。<br /><pre>
unix_sock_group = "libvirt" 
unix_sock_rw_perms = "0770" 
</pre></p>


	<a name="KVM設計_qemuconf設定"></a>
<h3 >qemu.conf設定<a href="#KVM設計_qemuconf設定" class="wiki-anchor">&para;</a></h3>


	<p>/etc/libvirt/qemu.confの設定を以下に定義する。全ての送信元からのvnc接続を許可する。<br /><pre>
vnc_listen = "0.0.0.0" 
</pre></p>


	<a name="KVM設計_lvmconf設定etclvmlvmconf"></a>
<h3 >lvm.conf設定(/etc/lvm/lvm.conf)<a href="#KVM設計_lvmconf設定etclvmlvmconf" class="wiki-anchor">&para;</a></h3>


	<p>lvmwをクラスタ管理する際の設定を以下に示す。なお、type3はlvmをクラスタ管理する際に選択するtypeとなり、kvmホストからのlvmアクセスの排他制御はclvmを使用する。<br /><pre>
locking_type = 3
</pre></p>


	<a name="KVM設計_ricci設定"></a>
<h3 >ricci設定<a href="#KVM設計_ricci設定" class="wiki-anchor">&para;</a></h3>


	<p>パス：isbadmin</p>


<hr />


	<a name="KVM設計_luci設定"></a>
<h3 >luci設定<a href="#KVM設計_luci設定" class="wiki-anchor">&para;</a></h3>


	<p>luciはidc-ohyp03のみに導入する。<br />GUI管理画面はrootユーザにてログインする。</p>


<hr />


	<a name="KVM設計_サービス自動起動"></a>
<h3 >サービス自動起動<a href="#KVM設計_サービス自動起動" class="wiki-anchor">&para;</a></h3>


KVMクラスタ関連のサービス自動起動設定を以下に示す。
	<table>
		<tr style="background:#d3eaf3;">
			<th>サービス名</th>
			<th>　設定</th>
		</tr>
		<tr>
			<td>cman</td>
			<td>off</td>
		</tr>
		<tr>
			<td>clvmd</td>
			<td>off</td>
		</tr>
		<tr>
			<td>libvirtd</td>
			<td>off</td>
		</tr>
		<tr>
			<td>libvirt-guests</td>
			<td>off</td>
		</tr>
		<tr>
			<td>lvm2-monitor</td>
			<td>on</td>
		</tr>
		<tr>
			<td>iscsi</td>
			<td>on</td>
		</tr>
		<tr>
			<td>iscsid</td>
			<td>on</td>
		</tr>
		<tr>
			<td>ehcmd</td>
			<td>on</td>
		</tr>
		<tr>
			<td>ricci</td>
			<td>off</td>
		</tr>
		<tr>
			<td>luci</td>
			<td>off</td>
		</tr>
		<tr>
			<td>scsi_reserve_eql</td>
			<td>on</td>
		</tr>
	</table>




<hr />


	<a name="KVM設計_共用ストレージ設計"></a>
<h2 >共用ストレージ設計<a href="#KVM設計_共用ストレージ設計" class="wiki-anchor">&para;</a></h2>


	<a name="KVM設計_equallogicへの接続"></a>
<h3 >equallogicへの接続<a href="#KVM設計_equallogicへの接続" class="wiki-anchor">&para;</a></h3>


	<p>equallogicへの接続は、iscsiイニシエータとHost Integration Tools for Linuxの２つのソフトウェアにて管理する。</p>


	<a name="KVM設計_KVM用ボリューム"></a>
<h3 >KVM用ボリューム<a href="#KVM設計_KVM用ボリューム" class="wiki-anchor">&para;</a></h3>


	<p>仮想インスタンスの実データ格納領域として、iscsiストレージに専用ボリュームを２つ作成する。１つは実データ格納領域、もう一つはquorum diskとして使用する。ボリューム領域の詳細についてはiscsi設計参照。</p>


	<a name="KVM設計_iscsiイニシエータ設定"></a>
<h3 >iscsiイニシエータ設定<a href="#KVM設計_iscsiイニシエータ設定" class="wiki-anchor">&para;</a></h3>


	<ol>
	<li> </li>
		<li></li>
		<li></li>
		<li>CHAP認証<br />iscsiターゲットへの接続はCHAP認証を使用する。/etc/iscsi/iscsi.confの設定を以下に示す。<br /><pre>
node.session.auth.authmethod = CHAP
node.session.auth.username = idc-ohyp0a
node.session.auth.password = m33FeawddKHH
discovery.sendtargets.auth.authmethod = CHAP
discovery.sendtargets.auth.username = idc-ohyp0a
discovery.sendtargets.auth.password = m33FeawddKHH
</pre></li>
	</ol>


	<a name="KVM設計_Host-Integration-Tools-for-Linux設定"></a>
<h3 >Host Integration Tools for Linux設定<a href="#KVM設計_Host-Integration-Tools-for-Linux設定" class="wiki-anchor">&para;</a></h3>


	<p>※以下の設定はソフトインストール時に対話形式で入力した項目であり、コンフィグファイルを手動で編集したものではない。</p>


	<ol>
	<li>adapter設定（/etc/equallogic/eql.conf)<br />equallogicへの接続はマルチパスを使用する。マルチパスとして使用するインターフェース設定を以下に示す。<br /><pre>
[MPIO-Include]
Adapter = eth3
Adapter = eth7
</pre></li>
		<li>グループアクセス設定（/etc/equallogic/eql.conf）<br />接続するequallogicグループの設定をを以下に示す。<br /><pre>
[idc-oist0a-EQLGROUP]
Group Name = idc-oist0a
Group IP = 10.16.24.129
</pre></li>
		<li>asm（auto snapshot manager）設定1（/etc/equallogic/asm-group-access)<br />asmを実行する為の設定を以下に示す。基本使用しない？が一応設定しておく。<br />[idc-oist0a-EQLGROUP]<br />Group Name = idc-oist0a<br />Group IP = 10.16.17.129</li>
		<li>eqltune<br />equallogi最適化の為の各種設定を以下に示す。</li>
	</ol>


<hr />
<hr />
<a name="L2_Switch詳細設計" />
<a name="L2_Switch詳細設計_L2-Switch詳細設計"></a>
<h1 >L2 Switch詳細設計<a href="#L2_Switch詳細設計_L2-Switch詳細設計" class="wiki-anchor">&para;</a></h1>


	<a name="L2_Switch詳細設計_構成概要"></a>
<h2 >構成概要<a href="#L2_Switch詳細設計_構成概要" class="wiki-anchor">&para;</a></h2>


	<p>L２スイッチの構成概要図を以下に示す。</p>


	<p><img src="/attachments/download/914/L2%E3%82%B9%E3%82%A4%E3%83%83%E3%83%81%E6%A7%8B%E6%88%90%E6%A6%82%E8%A6%81.png" alt="" /></p>


	<a name="L2_Switch詳細設計_冗長化設計"></a>
<h2 >冗長化設計<a href="#L2_Switch詳細設計_冗長化設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>Virtual Chassis<br />　Juniper EXシリーズのハードウェア冗長化技術である、Virtual Chassisを使用する。この機能により、２台の物理スイッチは<br />　いわゆるスタック構成となり論理的な１台の構成となる。
	<ul>
	<li>管理用IPアドレス<br />　Virtual Chassis構成の論理スイッチに対し、一つの管理用IPアドレスを付与する。</li>
		<li>スイッチのWeb管理<br />　Webブラウザからの設定および管理を可能とする為、http接続設定を有効化する。</li>
		<li>NTPサーバ<br />　NTPサーバはクラウド基盤NTPサーバを指定する。</li>
		<li>シスログ機能を使用し、クラウド基盤のシスログサーバへログを転送する。ログレベルは、ファシリティは以下の通りとする。</li>
	</ul>
	</li>
		<li>インターフェースの冗長化<br />　サーバ機器へ接続される（iSCSIストレージ除く）1GEポートおよび、上位スイッチへの接続に使用される10GEポートは<br />　物理マシンの冗長化と併せて、インターフェースレベルでの冗長化技術であるリンクアグリゲーションを使用する。<br />　リンクアグリゲーション技術にはIEEE802.3adで標準化されているLACPを使用する。物理スイッチ２台のそれぞれの１ポートを<br />　リンクアグリゲーションの１グループとして登録し、アクティブ側のインターフェースに障害が発生し使用不能となった場合でも<br />　スタンバイ側へ自動切り替わりを実施する。よって、本スイッチに接続するサーバ機器はLACPを有効化し、ネットワーク種別毎に<br />　アクティブ/スタンバイスイッチそれぞれのポートへ接続させる必要がある。
	<ul>
	<li>LACPモード<br />　L2スイッチ側でのLACPモードは以下の通りとする。LACPは接続される双方のポートのどちらかがアクティブとなっている必要がある為<br />　サーバ側インターフェース（サーバから見るとアップリンク）はactiveとなっている必要がある。</li>
	</ul></li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ポート種別</th>
			<th>LACPモード</th>
		</tr>
		<tr>
			<td>アップリンク</td>
			<td>active</td>
		</tr>
		<tr>
			<td>ダウンリンク</td>
			<td>passive</td>
		</tr>
	</table>




	<ul>
	<li>LACPグループ<br />　本機器におけるLACPグループの最大数は111となり、本構成では「クラウド管理ネットワーク」以外のネットワークに繋がるポートに対し設定する。<br />　また、物理ポートのグループはスロット番号が同じポートをLACPの１グループとして作成する。例．0/0/1,1/0/1</li>
		<li>マルチパス構成<br />　iSCSIネットワークへの接続用に使用するポートは、冗長化構成に２つの物理ポートを使用したマルチパス機能を使用する。<br />　よってL2スイッチ側とサーバ側でインターフェースレベルでの特別な設定は行わない。サーバ側のアプリケーションレベルで対応する。</li>
		<li>クラウド管理用ネットワークポート<br />　サーバ側のクラウド管理ネットワークに繋がるポートは、ボンディング構成にlinuxでいう「mode=1」(active/backup)で構成する為<br />　L2スイッチ側はアグリゲーションインターフェースを作成せず、通常のスイッチポートとして構成する。<br />　</li>
	</ul>


	<a name="L2_Switch詳細設計_インターフェース設計"></a>
<h2 >インターフェース設計<a href="#L2_Switch詳細設計_インターフェース設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>インターフェース種別<br />　インターフェース種別を以下の通り定義する。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ポート種別</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>ダウンリンク</td>
			<td>サーバ機器に接続されるポート。</td>
		</tr>
		<tr>
			<td>アップリンク</td>
			<td>上位スイッチへ接続されるポート。</td>
		</tr>
		<tr>
			<td>VCポート</td>
			<td>Virtusl Chassis用にL2スイッチ間で相互接続されるポート。</td>
		</tr>
	</table>




	<ul>
	<li>ポートアサイン<br />　ポートアサインルールは以下の通りとする。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ポート種別</th>
			<th>アサインルール。</th>
		</tr>
		<tr>
			<td>1GEポート</td>
			<td>ラックの下段に設置されているサーバから若番を順番に使用する。</td>
		</tr>
		<tr>
			<td>10GEポート</td>
			<td>若番２つをヴァーチャルシャーシ相互接続用リンクとし、再老番を上位スイッチへのアップリンクポートとする。</td>
		</tr>
	</table>




	<ul>
	<li>ポートNo<br />　インターフェース構成は、0/*/*表記は１号機、1/*/*表記を２号機とする。</li>
		<li>上位スイッチへのアップリンク経路 <br />　上位スイッチであるL3スイッチも、L2スイッチと同様にスタック構成となっている為、L2スイッチ間とのケーブル結線をたすき掛け構造にする必要はない。<br />　上位L2スイッチと接続するアップリンクポートは、10GE接続とし、物理接続は図の通りそれぞれ１経路のみとする。</li>
		<li>iSCSIネットワーク接続ポートにおけるMTU設計<br />　iSCSIストレージ接続用ネットワークポートに接続されるポートは、ジャンボフレームに対応するためMTUを9014に設定する。</li>
	</ul>


	<a name="L2_Switch詳細設計_vlan設計"></a>
<h2 >vlan設計<a href="#L2_Switch詳細設計_vlan設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>vlan種別<br />各ポートへ割り当てるネットワーク種別、vlan種別は以下の通りとする。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ネットワーク種別</th>
			<th>vlan種別</th>
			<th>タグ</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>クラウド管理ネットワーク</td>
			<td>ポートベース</td>
			<td>無</td>
			<td>管理、監視系ネットワーク</td>
		</tr>
		<tr>
			<td>デフォルトネットワーク </td>
			<td>ポートベース</td>
			<td>無</td>
			<td>クラウド事業者に割り当てるられるインターネット接続用ネットワーク</td>
		</tr>
		<tr>
			<td>カスタムネットワーク</td>
			<td>ポートベース</td>
			<td>無</td>
			<td>クラウド事業者に割り当てられるオプションのLANネットワーク</td>
			<td></td>
		</tr>
		<tr>
			<td>サービスネットワーク</td>
			<td>トランク</td>
			<td>有</td>
		</tr>
		<tr>
			<td>仮想マシン系ネットワーク</td>
			<td>トランク</td>
			<td>有</td>
			<td>クラウド事業者が使用する各ネットワーク</td>
		</tr>
		<tr>
			<td>VC-Port</td>
			<td>無</td>
			<td>無</td>
			<td>Virtual Chassis用スイッチ間相互接続用ポート</td>
		</tr>
		<tr>
			<td>アップリンク</td>
			<td>トランク</td>
			<td>有</td>
			<td>上位スイッチへ接続する10GEアップリンクポート</td>
		</tr>
	</table>




	<a name="L2_Switch詳細設計_ポートセキュリティ"></a>
<h2 >ポートセキュリティ<a href="#L2_Switch詳細設計_ポートセキュリティ" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>ファイアウォールフィルタ<br />　アップリンク/ダウンリンクポートは、ポート単位でのファイアウォールフィルタは適用しない。</li>
		<li>フロー制御<br />　以下のポートは、パケットバッファの制御用としてIEEE802.3xに準拠したフロー制御機能を有効にする。<br />　　iSCSIストレージ接続用ネットワークポート<br />　　上位スイッチへ接続されるアップリンクポート</li>
	</ul>


	<a name="L2_Switch詳細設計_QoS設計"></a>
<h2 >QoS設計<a href="#L2_Switch詳細設計_QoS設計" class="wiki-anchor">&para;</a></h2>


	<a name="L2_Switch詳細設計_コンフィグ"></a>
<h2 >コンフィグ<a href="#L2_Switch詳細設計_コンフィグ" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>L２スイッチのコンフィグについては、別紙【idc-ol2s06 - 07 パラメータシート.xlsx】に記載する。</li>
	</ul>
<hr />
<a name="L3_Switch詳細設計" />
<a name="L3_Switch詳細設計_L3-Switchネットワーク詳細設計"></a>
<h1 >L3 Switchネットワーク詳細設計<a href="#L3_Switch詳細設計_L3-Switchネットワーク詳細設計" class="wiki-anchor">&para;</a></h1>


	<a name="L3_Switch詳細設計_共通設計"></a>
<h2 >■共通設計<a href="#L3_Switch詳細設計_共通設計" class="wiki-anchor">&para;</a></h2>


	<a name="L3_Switch詳細設計_冗長化設計"></a>
<h3 >冗長化設計<a href="#L3_Switch詳細設計_冗長化設計" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>Virtual Chassis<br />　Juniper EXシリーズのハードウェア冗長化技術である、Virtual Chassisを使用する。この機能により、２台の物理スイッチは<br />　いわゆるスタック構成となり論理的な１台の構成となる。
	<ul>
	<li>管理用IPアドレス<br />　Virtual Chassis構成の論理スイッチに対し、一つの管理用IPアドレスを付与する。</li>
		<li>スイッチのWeb管理<br />　Webブラウザからの設定および管理を可能とする為、http接続設定を有効化する。</li>
		<li>NTPサーバ<br />　NTPサーバはクラウド基盤NTPサーバを指定する。</li>
		<li>シスログ機能を使用し、クラウド基盤のシスログサーバへログを転送する。ログレベルは、ファシリティは以下の通りとする。</li>
	</ul>
	</li>
		<li>インターフェースの冗長化<br />　サーバ機器へ接続される（iSCSIストレージ除く）ポートは、物理マシンの冗長化と併せてインターフェースレベルでの冗長化技術であるリンクアグリゲーションを使用する。<br />　リンクアグリゲーション技術にはIEEE802.3adで標準化されているLACPを使用する。物理スイッチ２台のそれぞれの１ポートをリンクアグリゲーションの１グループとして登録し<br />　アクティブ側のインターフェースに障害が発生し使用不能となった場合でもスタンバイ側へ自動切り替わりを実施する。よって、本スイッチに接続する下位ネットワーク機器はLACPを有効化し<br />　ネットワーク種別毎にアクティブ/スタンバイスイッチそれぞれのポートへ接続させる必要がある。
	<ul>
	<li>LACPモード<br />　L3スイッチ側でのLACPモードは以下の通りとする。LACPは接続される双方のポートのどちらかがアクティブとなっている必要がある為<br />　下位ネットワーク機器側のインターフェース（下位から見るとアップリンク）はactiveとなっている必要がある。</li>
	</ul></li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ポート種別</th>
			<th>LACPモード</th>
		</tr>
		<tr>
			<td>アップリンク</td>
			<td>active</td>
		</tr>
		<tr>
			<td>ダウンリンク</td>
			<td>passive</td>
		</tr>
	</table>




	<ul>
	<li>LACPグループ<br />　本機器におけるLACPグループの最大数は111となり、本構成では「iSCSI共用ストレージ」以外の下位ネットワーク機器に繋がるポートに対し設定する。<br />　また、物理ポートのグループはスロット番号が同じポートをLACPの１グループとして作成する。例．0/0/1,1/0/1</li>
		<li>マルチパス構成<br />　iSCSIネットワークへの接続用に使用するポートは、冗長化構成に２つの物理ポートを使用したマルチパス機能を使用する。<br />　よってL3 Swtich側とストレージ装置側でインターフェースレベルでの特別な冗長化設定は行わない。サーバ側のアプリケーションレベルで対応する。</li>
	</ul>


<hr />


	<a name="L3_Switch詳細設計_インターフェース設計"></a>
<h2 >インターフェース設計<a href="#L3_Switch詳細設計_インターフェース設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>インターフェース種別<br />　インターフェース種別を以下の通り定義する。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ポート種別</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>ダウンリンク</td>
			<td>下位ネットワーク機器およびiSCSI共用ストレージ装置に接続されるポート。</td>
		</tr>
		<tr>
			<td>アップリンク</td>
			<td>上位ネットワーク機器へ接続されるポート。</td>
		</tr>
		<tr>
			<td>VCポート</td>
			<td>Virtusl Chassis用にL3スイッチ間で相互接続されるポート。</td>
		</tr>
	</table>




	<ul>
	<li>ポートアサイン<br />　ポートアサインルールは以下の通りとする。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ポート種別</th>
			<th>アサインルール。</th>
		</tr>
		<tr>
			<td>L2スイッチとの接続</td>
			<td>0/0/1から順番にL2スイッチの若い機器番号から使用する。</td>
		</tr>
		<tr>
			<td>iSCSI共用ストレージ装置との接続</td>
			<td>0/0/18から順番にストレージ装置の若い機器番号から使用する。今後の拡張性を考慮し、0/0/29まではストレージ用として予約する。</td>
		</tr>
		<tr>
			<td>VCポート</td>
			<td>0/0/30,0/0/31を使用する。</td>
		</tr>
	</table>




	<ul>
	<li>ポートNo<br />　インターフェース構成は、0/*/*表記は１号機、1/*/*表記を２号機とする。</li>
		<li>上位ネットワーク機器へのアップリンク経路<br />　本デヴァイスの上位ネットワーク機器は環境により異なる。以下に環境毎の上位ネットワーク機器を示す。なお、以下は論理的な上位機器を示し<br />　L3スイッチのインターフェースが全てSFPインターフェースである関係上物理的にはいずれも上位機器の間にL2スイッチが介在する。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>環境</th>
			<th>上位ネットワーク機器</th>
		</tr>
		<tr>
			<td>コンダクト、大東実業等の既存ネットワーク</td>
			<td>インターネットゲートウェイルータ</td>
		</tr>
		<tr>
			<td>ISBクラウド設備、新ユーザ等の新ネットワーク</td>
			<td>インターネットファイアウォール</td>
		</tr>
	</table>




	<ul>
	<li>iSCSIネットワーク接続ポートにおけるMTU設計<br />　iSCSIストレージ接続用ネットワークポートに接続されるポートは、ジャンボフレームに対応するためMTUを9014に設定する。</li>
	</ul>


<hr />


	<a name="L3_Switch詳細設計_既存ネットワーク設計"></a>
<h2 >■既存ネットワーク設計<a href="#L3_Switch詳細設計_既存ネットワーク設計" class="wiki-anchor">&para;</a></h2>


	<p>本デヴァイスは、既に稼働中のコンダクト/大東実業等の環境と今回作成するISBクラウド設備環境で、異なるネットワーク設計を採用する。本項では、既存ネットワーク設計について説明する。</p>


	<a name="L3_Switch詳細設計_仮想ルータ設計"></a>
<h3 >仮想ルータ設計<a href="#L3_Switch詳細設計_仮想ルータ設計" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>仮想ルータ種別</li>
	</ul>


	<p>仮想ルータは以下の２つに大別される。</p>


	<table>
		<tr style="background:#d3eaf3;">
			<th>仮想ルータ種別</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>ユーザ用仮想ルータ</td>
			<td>インターネット環境の出入り口となる仮想ルータ。ユーザ毎に作成され、公開サーバ等、インターネット環境へ接続される必要のあるサーバは<br />この仮想ルータに紐付けられる。なお、ユーザ用仮想ルータ間は、一部例外を除きルーティングを許容しない。</td>
		</tr>
		<tr>
			<td>インターネットゲートウェイ接続用仮想ルータ</td>
			<td>L3 Switchの上位ネットワーク機器はPPPOE接続を管理するインターネットゲートウェイルータとなり、上位デヴァイス側でPPPOEセッション毎に<br />仮想インターネットゲートウェイを作成している。セッションを跨ぐルーティングは基本的に許容しない為、L3 Switch側でも上位の仮想インターネットゲートウェイ接続用ルータを<br />作成し、インターネット経路をセッション毎に仮想的に分割する。</td>
		</tr>
	</table>




	<ul>
	<li>仮想ルータ一覧</li>
	</ul>


	<p>既存ネットワーク環境に作成される仮想ルータ一覧を以下に示す。</p>


	<table>
		<tr style="background:#d3eaf3;">
			<th>仮想ルータ名</th>
			<td>_ 種別</td>
			<th>ユーザ</th>
			<th>PPPOE接続用仮想ルータ</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>RI_Dialer2</td>
			<td>GW接続用</td>
			<td>-</td>
			<td>-</td>
			<td>RI_pharma,RI_tmmc,RI_m2mを配下に持つ、インターネットゲートウェイ接続用仮想ルータ。</td>
		</tr>
		<tr>
			<td>RI_Dialer3</td>
			<td>GW接続用</td>
			<td>-</td>
			<td>-</td>
			<td>RI_pharma,RI_caretive-test,RI_conductを配下に持つ、インターネットゲートウェイ接続用仮想ルータ。</td>
		</tr>
		<tr>
			<td>RI_pharma</td>
			<td>ユーザ用</td>
			<td>pharma</td>
			<td>RI_Dialer3</td>
			<td>pharmaサーバ用仮想ルータ。</td>
		</tr>
		<tr>
			<td>RI_caretive-test</td>
			<td>ユーザ用</td>
			<td>コンダクト</td>
			<td>RI_Dialer3</td>
			<td>caretiveテスト環境用仮想ルータ。</td>
		</tr>
		<tr>
			<td>RI_conduct</td>
			<td>ユーザ用</td>
			<td>コンダクト</td>
			<td>RI_Dialer3</td>
			<td>コンダクト本番環境用仮想ルータ。</td>
		</tr>
		<tr>
			<td>RI_tmmc</td>
			<td>ユーザ用</td>
			<td>tmmc</td>
			<td>RI_Dialer2</td>
			<td>tmmcテスト環境用仮想ルータ。</td>
		</tr>
		<tr>
			<td>RI_m2m</td>
			<td>ユーザ用</td>
			<td>ISB管理</td>
			<td>RI_Dialer2</td>
			<td>m2mミニマム環境へ接続する為の繋ぎとなる仮想ルータ。仮想ルータ自体はミニマム環境用のルータと接続されており仮想ルータ配下に<br />m2mミニマム用サーバが直接接続されているわけではない。</td>
		</tr>
		<tr>
			<td>RI_service</td>
			<td>ユーザ用</td>
			<td>ISB管理</td>
			<td>RI_Dialer2</td>
			<td>Zabbixサーバ、NTPサーバ、シスログサーバといった、複数のサーバへ提供するサービスが稼働するサーバを配下に持つ仮想ルータ。</td>
		</tr>
	</table>




<hr />


	<a name="L3_Switch詳細設計_ファイアウォールフィルタリング設計"></a>
<h3 >ファイアウォールフィルタリング設計<a href="#L3_Switch詳細設計_ファイアウォールフィルタリング設計" class="wiki-anchor">&para;</a></h3>


	<p>仮想ルータおよび仮想ルータに紐付けられるサーバ通信のセキュリティ対策として、仮想ルータが持つvlanインターフェースに対しファイアウォールフィルタリングを実装する。<br />これにより、結果的に仮想ルータおよび各ユーザが持つサーバ間の不要な通信を遮断する。</p>


	<ul>
	<li>フィルタ種別<br />仮想ルータのインターフェースに対し、ファイアウォールフィルタを設定する。フィルタの適用先は以下の通りとなる。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>フィルタ名</th>
			<th>フィルタ適用先</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>FL_Dialer2-to-Tenant</td>
			<td>RI_Dialer2</td>
			<td>WAN環境からの通信を、宛先に応じて各ユーザ仮想ルータへルーティングする。</td>
		</tr>
		<tr>
			<td>FL_Dialer3-to-Tenant</td>
			<td>RI_Dialer3</td>
			<td>仮想ルータが持つインターフェースはインターネット接続用であることから仮想ルータの外側に対してのフィルタリングとなり<br />結果的に仮想ルータ間のルーティングを制限する。</td>
		</tr>
		<tr>
			<td>FL_phama</td>
			<td>RI_phama</td>
			<td></td>
		</tr>
		<tr>
			<td>FL_tmmc</td>
			<td>RI_tmmc</td>
			<td></td>
		</tr>
		<tr>
			<td>FL_m2m</td>
			<td>RI_m2m</td>
			<td></td>
		</tr>
		<tr>
			<td>FL_cloud</td>
			<td>RI_service</td>
			<td></td>
		</tr>
		<tr>
			<td>FL_conduct</td>
			<td>RI_conduct</td>
			<td></td>
		</tr>
		<tr>
			<td>FL_caretive-test</td>
			<td>RI_caretive-test</td>
			<td></td>
		</tr>
		<tr>
			<td>FL_daito</td>
			<td>RI_daito</td>
			<td></td>
		</tr>
	</table>




	<ul>
	<li>フィルタルール一覧<br />フィルタルールは仮想マシン毎に作成され多岐に渡ることから、別紙「ファイアウォールフィルタリング一覧.xlsx」に記載する。</li>
	</ul>


<hr />


	<a name="L3_Switch詳細設計_vlan設計"></a>
<h3 >vlan設計<a href="#L3_Switch詳細設計_vlan設計" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>vlan種別<br />本デヴァイスは仮想ルータ及び物理機器に割り当てられるvlanを管理する。本デヴァイスが持つvlanおよび利用用途を以下に示す。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ネットワーク名</th>
			<th>割り当てデヴァイス</th>
			<th>vlan分割方法</th>
			<th>vlan間ルーティング</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>クラウドサービス用インターネット</td>
			<td>インターネットゲートウェイ接続用仮想ルータ</td>
			<td>仮想ルータ毎</td>
			<td>不可</td>
			<td>インターネットの出入り口となるネットワーク。このネットワークはインターネットゲートウェイ接続用仮想ルータにおけるWAN側インターフェースとなる。<br />仮想ルータの数だけvlanを分割し、vlan間通信は許容しない。</td>
		</tr>
		<tr>
			<td>クラウドサービス用デフォルトネットワーク</td>
			<td>ISBクラウド設備仮想ルータ</td>
			<td>ユーザ毎</td>
			<td>不可</td>
			<td>インターネットの出入り口となるネットワークを作成する。このネットワークはISBクラウド設備仮想ルータにおけるLAN側インターフェースとなり<br />インターネットへ接続する必要のあるサーバに対するゲートウェイとなる。仮想ルータ毎にvlanを作成しvlan間ルーティングは許容しない。</td>
		</tr>
		<tr>
			<td>IPMIネットワーク</td>
			<td>物理サーバ</td>
			<td>ユーザ毎</td>
			<td>可</td>
			<td>物理サーバにIPMI接続する為のハードウェア管理用ネットワーク。全ての物理サーバが本ネットワークを持ち、基本的にはISB管理者のみ使用する。<br />ユーザ毎に異なるvlanを作成し物理L3 Switchが本vlanを管理する。vlan間ルーティングは許容する。</td>
		</tr>
		<tr>
			<td>クラウド管理ネットワーク</td>
			<td>物理サーバ,ネットワーク機器</td>
			<td>ユーザ毎,ISBクラウド機器</td>
			<td>可</td>
			<td>サーバおよびネットワーク機器管理用ネットワーク。ISBが管理する全ての物理サーバおよびネットワーク機器が本ネットワークを持ち<br />ISB管理者のみ使用する。ユーザ毎およびネットワーク機器で異なるvlanを作成し物理L3 Switchが本vlanを管理する。vlan間ルーティングは許容する。</td>
		</tr>
		<tr>
			<td>ストレージネットワーク</td>
			<td>ストレージに接続するサーバ</td>
			<td>ユーザ毎,ストレージ</td>
			<td>可</td>
			<td>仮想化ハイパーバイザ等がiSCSI共用ストレージへ接続する為のネットワーク。iSCSI共有ストレージをディスク領域として使用するサーバおよび<br />iSCSI共有ストレージ本体が本ネットワークを持ちISB管理者のみ使用する。ユーザ毎およびストレージ本体で異なるvlanを作成しストレージネットワーク専用の仮想ルータがvlanを管理する。<br />サーバ⇔ストレージ間はもちろんルーティング可とするが、サーバ間のvlan間ルーティングは許容しない。</td>
		</tr>
		<tr>
			<td>監視ネットワーク</td>
			<td>Zabbix Proxyおよび旧Zabbixサーバ</td>
			<td>共通</td>
			<td>可</td>
			<td>ユーザ環境用監視ネットワーク。ユーザ環境毎に監視中継用のZabbix Proxyを配置し、監視ネットワークを通じてZabbixサーバと通信する。ISB管理者がZabbix Proxyを管理<br />する際の接続経路としても使用する。ユーザに限らずZabbix Proxyは全て同一のvlanとなりvlan間ルーティングは許容する。</td>
		</tr>
		<tr>
			<td>コンダクト管理ネットワーク</td>
			<td>コンダクト物理サーバ</td>
			<td>共通</td>
			<td>可</td>
			<td>コンダクトの物理機器を管理する目的で作成されたネットワーク。コンダクト旧環境の物理サーバが本ネットワークを持つ。<br />全ての機器は同一のvlanとなりvlan間ルーティングは許容する。</td>
		</tr>
		<tr>
			<td>ネットワーク機器用カスタムネットワーク</td>
			<td>物理L3 Switch</td>
			<td>用途毎</td>
			<td>可</td>
			<td>上位ネットワーク機器との接続に使用するvlanや、特殊環境へ接続する為の経路に割り当てられるvlan。</td>
		</tr>
	</table>




	<ul>
	<li>vlanマッピング<br />どのvlanがどこへ割り当てられるのか、別紙「【共通】ネットワークアドレス管理台帳.xlsx」に記載する。</li>
	</ul>


	<a name="L3_Switch詳細設計_論理構成図"></a>
<h3 >論理構成図<a href="#L3_Switch詳細設計_論理構成図" class="wiki-anchor">&para;</a></h3>


	<p>記設計を踏襲した論理構成については、「IDC全体構成図.vsd」参照。</p>


<hr />


	<a name="L3_Switch詳細設計_新ネットワーク設計"></a>
<h2 >■新ネットワーク設計<a href="#L3_Switch詳細設計_新ネットワーク設計" class="wiki-anchor">&para;</a></h2>


	<p>本項では、新規ネットワーク設計について説明する。</p>


	<a name="L3_Switch詳細設計_仮想ルータ設計-2"></a>
<h3 >仮想ルータ設計<a href="#L3_Switch詳細設計_仮想ルータ設計-2" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>仮想ルータ種別</li>
	</ul>


	<p>新ネットワーク設計における仮想ルータ種別は以下の通りとなる。IaaSクラウドを利用するユーザ用の仮想ルータはISBクラウド設備内の仮想化ハイパーバイザにて作成・管理する為、L3 Swtich側では作成しない。</p>


	<table>
		<tr style="background:#d3eaf3;">
			<th>仮想ルータ名</th>
			<th>仮想ルータ種別</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>RI_pppoe-channel-4</td>
			<td>インターネットゲートウェイ接続用仮想ルータ</td>
			<td>ISBクラウド設備用仮想ルータのWAN側ネットワークの位置づけで、PPPOE接続の終端となるインターネットファイアウォールとの接続経路となる。よって、インターネット接続用セグメントのゲートウェイはファイアウォールとなる。</td>
		</tr>
		<tr>
			<td>RI_isb-cloud</td>
			<td>ISBクラウド設備用仮想ルータ</td>
			<td>インターネット環境の出入り口となるISBクラウド設備用の仮想ルータ。公開サーバ等、インターネット環境へ接続される必要のあるサーバはこの仮想ルータに紐付けられ、LAN側ネットワークのゲートウェイの位置づけとなる。<br />なお、既存ネットワークに存在するユーザ用仮想ルータとのルーティングは許容しない。接続する必要がある場合は、仮想ルータが持つネットワークではなく別のネットワークにて通信する。</td>
		</tr>
		<tr>
			<td>RI_isb-cloud-management</td>
			<td>ISBクラウド設備クラウド管理ネットワーク用仮想ルータ</td>
			<td>ISBクラウド設備におけるクラウド管理ネットワークを集約する仮想ルータ。</td>
		</tr>
		<tr>
			<td>RI_isb-cloud-ipmi</td>
			<td>ISBクラウド設備ipmiネットワーク用仮想ルータ</td>
			<td>ISBクラウド設備におけるipmiネットワークを集約する仮想ルータ。</td>
		</tr>
	</table>




<hr />


	<a name="L3_Switch詳細設計_ファイアウォールフィルタリング設計-2"></a>
<h3 >ファイアウォールフィルタリング設計<a href="#L3_Switch詳細設計_ファイアウォールフィルタリング設計-2" class="wiki-anchor">&para;</a></h3>


	<p>仮想ルータおよび仮想ルータに紐付けられるサーバ通信のセキュリティ対策として、仮想ルータが持つvlanインターフェースに対しファイアウォールフィルタリングを実装する。<br />これにより、仮想ルータおよび各ユーザが持つサーバ間の不要な通信を遮断する。</p>


	<ul>
	<li>フィルタ種別<br />仮想ルータのインターフェースに対し、ファイアウォールフィルタを設定する。フィルタの詳細は以下の通りとなる。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>フィルタ名</th>
			<th>フィルタ適用先</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>FL_pppoe_channel-4-to-isb-cloud</td>
			<td>pppoe_channel-4</td>
			<td>WAN環境からISBクラウド設備への宛先となる通信を、配下の仮想ルータへルーティングする。</td>
		</tr>
		<tr>
			<td>FL_isb-cloud</td>
			<td>isb-cloud</td>
			<td>仮想ルータが持つネットワークをルーティングする。また、不要なLANアドレス宛の通信が仮想ルータの外へ出るのを制限する。</td>
		</tr>
	</table>




	<ul>
	<li>フィルタルール一覧<br />フィルタルールは仮想マシン毎に作成され多岐に渡ることから、別紙「ファイアウォールフィルタリング一覧.xlsx」に記載する。</li>
	</ul>


<hr />


	<a name="L3_Switch詳細設計_vlan設計-2"></a>
<h3 >vlan設計<a href="#L3_Switch詳細設計_vlan設計-2" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>vlan種別<br />本デヴァイスは仮想ルータ及び物理機器に割り当てられるvlanを管理する。本デヴァイスが持つvlanおよび利用用途を以下に示す。</li>
	</ul>


	<table>
		<tr style="background:#d3eaf3;">
			<th>ネットワーク名</th>
			<th>割り当て先</th>
			<th>vlan分割方法</th>
			<th>vlan間ルーティング</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>クラウドサービス用インターネット</td>
			<td>RI_pppoe-channel-4</td>
			<td>-</td>
			<td>不可</td>
			<td>インターネットゲートウェイ接続用仮想ルータにおけるWAN側インターフェースとなる。vlan間ルーティングは許容しない。</td>
		</tr>
		<tr>
			<td>クラウドサービス用デフォルトネットワーク</td>
			<td>RI_isb-cloud</td>
			<td>-</td>
			<td>不可</td>
			<td>ISBクラウド設備仮想ルータにおけるLAN側インターフェースとなりインターネットへ接続する必要のあるサーバに対するゲートウェイとなる。vlan間ルーティングは許容しない。</td>
		</tr>
		<tr>
			<td>IPMIネットワーク</td>
			<td>ISBクラウド物理サーバ,ユーザベアメタルサーバ</td>
			<td>ユーザ毎</td>
			<td>可</td>
			<td>物理サーバにIPMI接続する為のハードウェア管理用ネットワーク。全ての物理サーバが本ネットワークを持ち、基本的にはISB管理者のみ使用する。<br />ユーザ毎に異なるvlanを作成し仮想ルータに紐付られる。既存IPMIネットワークを含めたvlan間ルーティングは許容する。</td>
		</tr>
		<tr>
			<td>ZabbixProxy用クラウド管理・監視ネットワーク</td>
			<td>ユーザZabbixProxy</td>
			<td>-</td>
			<td>可</td>
			<td>ISB管理者がzabbixProxyにアクセスする為のネットワーク。併せてZabbix Serverとの通信もこのネットワークを経由して行う。<br />ISB管理者のみ使用し、仮想ルータに紐付られる。他のネットワークとのvlan間ルーティングは許容する。</td>
		</tr>
		<tr>
			<td>ユーザ仮想ルータ用クラウド管理・監視ネットワーク</td>
			<td>ユーザ仮想ルータ</td>
			<td>-</td>
			<td>可</td>
			<td>ISB管理者がユーザ仮想ルータにアクセスする為のネットワーク。併せてZabbix Serverとの通信もこのネットワークを経由して行う。<br />ISB管理者のみ使用し、仮想ルータに紐付られる。他のネットワークとのvlan間ルーティングは許容する。</td>
		</tr>
		<tr>
			<td>ISBクラウド設備用クラウド管理・監視サービス・NASネットワーク</td>
			<td>ISBクラウド設備内のサーバ群</td>
			<td>-</td>
			<td>可</td>
			<td>ISB管理者がISBクラウド設備の各種サーバにアクセスする為のネットワーク。併せてZabbix ServerとユーザZabbixproxy間の監視系通信もこのネットワークを経由して行う。<br />バックアップ用のネットワークとしても使用する。ISB管理者のみ使用し、仮想ルータに紐付られる。他のネットワークとのvlan間ルーティングは許容する。</td>
		</tr>
		<tr>
			<td>ストレージネットワーク</td>
			<td>ストレージに接続するサーバ</td>
			<td>-</td>
			<td>可</td>
			<td>仮想化ハイパーバイザがiSCSI共用ストレージへ接続する為のネットワーク。iSCSI共有ストレージをディスク領域として使用するサーバが本ネットワークを持ち、ISB管理者のみ使用する。<br />ユーザ毎およびストレージ本体で異なるvlanを作成しストレージネットワーク専用の仮想ルータに紐付られる。サーバ⇔ストレージ間はもちろんルーティング可とするが、サーバ間のvlan間ルーティングは許容しない。</td>
		</tr>
	</table>




	<ul>
	<li>vlanマッピング<br />どのvlanがどこへ割り当てられるのか、別紙「【共通】ネットワークアドレス管理台帳.xlsx」に記載する。</li>
	</ul>


	<a name="L3_Switch詳細設計_論理構成図-2"></a>
<h3 >論理構成図<a href="#L3_Switch詳細設計_論理構成図-2" class="wiki-anchor">&para;</a></h3>


	<p>記設計を踏襲した論理構成については、「IDC全体構成図.vsd」参照。</p>


<hr />


	<a name="L3_Switch詳細設計_ポートアサイン"></a>
<h3 >ポートアサイン<a href="#L3_Switch詳細設計_ポートアサイン" class="wiki-anchor">&para;</a></h3>


	<p>ポートアサインについては、「【共通】ネットワークアドレス管理台帳.xlsx」参照。</p>


<hr />


	<a name="L3_Switch詳細設計_コンフィグ"></a>
<h3 >コンフィグ<a href="#L3_Switch詳細設計_コンフィグ" class="wiki-anchor">&para;</a></h3>


	<p>コンフィグついては、「」参照。</p>


<hr />
<hr />
<a name="Libvirt" />
<a name="Libvirt_Libvirt"></a>
<h1 >Libvirt<a href="#Libvirt_Libvirt" class="wiki-anchor">&para;</a></h1>


<hr />


	<a name="Libvirt_リモートのlibvirtdをvirshで操作する方法"></a>
<h2 >リモートのlibvirtdをvirshで操作する方法<a href="#Libvirt_リモートのlibvirtdをvirshで操作する方法" class="wiki-anchor">&para;</a></h2>


	<p>sshの設定ができていれば、<br />リモートのlibvirtdへのアクセスは以下のコマンドで接続可能。</p>


<pre>
$ virsh -c qemu+ssh://kvm-01/system
</pre>

<hr />


	<a name="Libvirt_新規インスタンス作成"></a>
<h2 >新規インスタンス作成<a href="#Libvirt_新規インスタンス作成" class="wiki-anchor">&para;</a></h2>


	<p>新規インスタンスを作成する場合に設定項目毎に<br />インスタンスのテンプレートxmlファイルを作成しておく。<br />テンプレートxmlファイルは以下のように取得可能。</p>


<pre>
$ virsh -c qemu+ssh://kvm-01/system dumpxml test1 &gt; template.xml
</pre>

	<p>こうして取得したxmlにおいて以下の設定を変更することで、<br />新規インスタンスを作成できる。<br />変更する理由はlibvirt内で一意の情報を期待しているため。</p>


	<ul>
	<li>name libvirt内でドメインとして管理される名前</li>
		<li>uuid uuidgenコマンドにて生成したものを設定する</li>
		<li>graphics port属性によっては別のvncポートが衝突する可能性があるため</li>
	</ul>


	<p>上記の情報を変更後、以下のコマンドで新規インスタンスを作成可能。</p>


<pre>
$ virsh -c qemu+ssh://kvm-01/system create template.xml
</pre>

	<p>インスタンスの一覧コマンドで生成されていることを確認可能。</p>


<pre>
$ virsh -c qemu+ssh://kvm-01/system list
 Id    名前                         状態
----------------------------------------------------
 4     test1                          実行中
 6     test2                          実行中
</pre>

<hr />


	<a name="Libvirt_インスタンスの削除"></a>
<h2 >インスタンスの削除<a href="#Libvirt_インスタンスの削除" class="wiki-anchor">&para;</a></h2>


	<p>以下のコマンドでインスタンスを強制削除できる。<br />新規インスタンス作成時でOSインストールが終了していない場合は<br />以下で強制削除してしまう。</p>


<pre>
$ virsh -c qemu+ssh://kvm-01/system destroy test2
</pre>

	<p>以下のコマンドでインスタンスの状態をセーブし、<br />また、内容を書き換えることでリストアすることが可能。<br />書き換える内容はxmlと同様。</p>


<pre>
$ virsh -c qemu+ssh://kvm-01/system save test1 test2.save
~~内容書き換え~~
$ virsh -c qemu+ssh://kvm-01/system restore test2.save
</pre>

<hr />


	<a name="Libvirt_インスタンス複製の問題"></a>
<h2 >インスタンス複製の問題<a href="#Libvirt_インスタンス複製の問題" class="wiki-anchor">&para;</a></h2>


	<p>上記の手順にてインスタンスを複製することができるが、<br />問題点がいくつか存在する。</p>


	<p>1. 固定IP等を降っていた場合はIPが重複してしまう<br />2. dhcpにしている場合はIPがわからないため接続してセッティングできない<br />3. MACアドレスが重複するためその変更が必要<br />4. ホストネームの設定が必要</p>


	<p>また、調整により企業毎のVLANに設定する必要がある。</p>
<hr />
<a name="Linux_ha_pacemaker" />
<a name="Linux_ha_pacemaker_Linux-ha-pacemaker"></a>
<h1 >Linux ha pacemaker<a href="#Linux_ha_pacemaker_Linux-ha-pacemaker" class="wiki-anchor">&para;</a></h1>


<hr />


	<a name="Linux_ha_pacemaker_インストール参考"></a>
<h2 >インストール<br /><a href="http://linux-ha.sourceforge.jp/wp/dl/pminstall_cent5" class="external">参考</a><a href="#Linux_ha_pacemaker_インストール参考" class="wiki-anchor">&para;</a></h2>


<pre>
# curl "http://jaist.dl.sourceforge.jp/linux-ha/61791/pacemaker-1.0.13-2.1.el6.x86_64.repo.tar.gz" | tar -zx -C /tmp
# yum -y -c /tmp/pacemaker-1.0.13-2.1.el6.x86_64.repo/pacemaker.repo install pacemaker-1.0.13-2.el6.x86_64 corosync-1.4.6-1.el6.x86_64 heartbeat-3.0.5-1.1.el6.x86_64
# cp /usr/share/doc/heartbeat-3.0.5/ha.cf /etc/ha.d/
# cp /usr/share/doc/heartbeat-3.0.5/authkeys /etc/ha.d/
# egrep -v "(#|^$)" /etc/ha.d/ha.cf # コメント行と空行を除外
logfacility     local0
auto_failback on
# vim /etc/ha.d/ha.cf
# egrep -v "(#|^$)" /etc/ha.d/ha.cf # コメント行と空行を除外
pacemaker on
logfacility     local1
keepalive 2
deadtime 30
warntime 10
initdead 120
udpport 694
bcast eth1                             &lt;- 環境に合わせる
auto_failback on
watchdog /dev/watchdog
node kajiro1 　　　　　　　　　　　　　&lt;- 環境に合わせる
node kajiro2 　　　　　　　　　　　　　&lt;- 環境に合わせる
# egrep -v "(#|^$)" /etc/ha.d/authkeys # コメント行と空行を除外
# vim /etc/ha.d/authkeys
# egrep -v "(#|^$)" /etc/ha.d/authkeys # コメント行と空行を除外
auth 2
2 sha1 HI!
# chown root:root /etc/ha.d/authkeys
# chmod 600 /etc/ha.d/authkeys
# egrep -v "(#|^$)" /etc/rsyslog.conf
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat
$IncludeConfig /etc/rsyslog.d/*.conf
*.info;mail.none;authpriv.none;cron.none                /var/log/messages
authpriv.*                                              /var/log/secure
mail.*                                                  -/var/log/maillog
cron.*                                                  /var/log/cron
*.emerg                                                 *
uucp,news.crit                                          /var/log/spooler
local7.*                                                /var/log/boot.log
# vim /etc/rsyslog.conf
# egrep -v "(#|^$)" /etc/rsyslog.conf
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat
$IncludeConfig /etc/rsyslog.d/*.conf
*.info;mail.none;authpriv.none;cron.none;local1.none                /var/log/messages
authpriv.*                                              /var/log/secure
mail.*                                                  -/var/log/maillog
cron.*                                                  /var/log/cron
*.emerg                                                 *
uucp,news.crit                                          /var/log/spooler
local7.*                                                /var/log/boot.log
local1.*                                                /var/log/ha-log
# ln -s /usr/lib64/heartbeat/heartbeat /usr/libexec/heartbeat/heartbeat # initスクリプトが別のパスを呼んでしまっている問題を解消
# /etc/init.d/rsyslog restart
</pre>

	<a name="Linux_ha_pacemaker_リソース登録"></a>
<h2 >リソース登録<a href="#Linux_ha_pacemaker_リソース登録" class="wiki-anchor">&para;</a></h2>


	<p><a href="http://linux-ha.sourceforge.jp/wp/dl/pm-crm-setting" class="external">参考</a></p>


<pre>

</pre>
<hr />
<a name="Load_Balancer詳細設計" />
<a name="Load_Balancer詳細設計_Load-Balancer詳細設計"></a>
<h1 >Load Balancer詳細設計<a href="#Load_Balancer詳細設計_Load-Balancer詳細設計" class="wiki-anchor">&para;</a></h1>


	<a name="Load_Balancer詳細設計_インターフェース設計"></a>
<h3 >インターフェース設計<a href="#Load_Balancer詳細設計_インターフェース設計" class="wiki-anchor">&para;</a></h3>


	<table>
		<tr style="background:#d3eaf3;">
			<th>インターフェース名</th>
			<th>インターフェース種別</th>
			<th> 説明</th>
		</tr>
		<tr>
			<td></td>
			<td>NetScalerIPアドレス</td>
			<td>管理アクセス用のIPアドレス。</td>
		</tr>
		<tr>
			<td></td>
			<td>マップされたIPアドレス(MIP)</td>
			<td>サーバ側との接続用IPアドレス。クライアントからのパケットのソースIPアドレスをこのアドレスに置き換えてパケットをサーバへ送信する。</td>
		</tr>
		<tr>
			<td></td>
			<td>仮想サーバIPアドレス(VIP)</td>
			<td>サーバに関連付けられた仮想IPアドレスとなり、クライアントはこのアドレス宛にパケットを送信する。</td>
		</tr>
		<tr>
			<td></td>
			<td>サブネットIPアドレス(SNIP)</td>
			<td>複数のサブネットに接続する場合、それらのサブネットへのアクセス用のMIPとして使用する。</td>
		</tr>
	</table>
<hr />
<a name="Mail設計" />
<a name="Mail設計_Mail設計"></a>
<h1 >Mail設計<a href="#Mail設計_Mail設計" class="wiki-anchor">&para;</a></h1>


	<p>本メールサーバはZabbixから送信されるアラートメールを中継するために利用する。<br />管理監視用ネットワークを中継を許可するネットワークとし、それ以外の中継を許可しないように設定をする。</p>


	<ul>
	<li>メールの中継元となるクライアント

	<p>デフォルトでは、許可されたネットワークのクライアントからはどの転送先へもメールを転送する。<br />  mynetworksにてネットワークの許可を行う。</p></li>
	</ul>


<pre>
mynetworks_style = subnet
mynetworks = 192.168.100.0/24, 127.0.0.0/8
</pre>

	<ul>
	<li>IPv4のみを利用する</li>
	</ul>


<pre>
inet_protocols = ipv4
</pre>

	<ul>
	<li>バージョン非表示</li>
	</ul>


<pre>
smtpd_banner = $myhostname ESMTP
</pre>

	<ul>
	<li>待ち受けネットワークアドレス</li>
	</ul>


<pre>
inet_interfaces = all
</pre>
<hr />
<a name="Mrepo" />
<a name="Mrepo_Mrepo"></a>
<h1 >Mrepo<a href="#Mrepo_Mrepo" class="wiki-anchor">&para;</a></h1>


	<p>mrepoは簡単にyumリポジトリのミラーサイトを構築するツールである。</p>


<pre>
# wget http://pkgs.repoforge.org/mrepo/mrepo-0.8.8-0.pre1.el6.rft.noarch.rpm
# yum -y install mrepo-0.8.8-0.pre1.el6.rft.noarch.rpm
# mkdir -p /var/www/html/mrepo
# cat /etc/mrepo.conf
### Configuration file for mrepo

### The [main] section allows to override mrepo's default settings
### The mrepo-example.conf gives an overview of all the possible settings
[main]
srcdir = /var/mrepo
wwwdir = /var/www/html/mrepo
confdir = /etc/mrepo.conf.d
#arch = i386
arch = x86_64

mailto = root@localhost
smtp-server = localhost

#rhnlogin = username:password

### Any other section is considered a definition for a distribution
### You can put distribution sections in /etc/mrepo.conf.d/
### Examples can be found in the documentation at:
###     /usr/share/doc/mrepo-0.8.8/dists/.

# cat /etc/mrepo.conf.d/centos6.6.conf 
[centos6]
name = CentOS $release ($arch)
release = 6
arch = x86_64
metadata = repomd repoview

### ISO images
iso = CentOS-6.6-x86_64-bin-DVD?.iso

### Additional repositories
updates    = rsync://ftp.riken.jp/centos/$release/updates/$arch/Packages/
fasttrack  = rsync://ftp.riken.jp/centos/$release/fasttrack/$arch/Packages/
centosplus = rsync://ftp.riken.jp/centos/$release/centosplus/$arch/Packages/
extras     = rsync://ftp.riken.jp/centos/$release/extras/$arch/Packages/
contrib    = rsync://ftp.riken.jp/centos/$release/contrib/$arch/Packages/

### EPEL repositories
epel = rsync://ftp.jaist.ac.jp/pub/Linux/Fedora/epel/$release/$arch/

# mkdir /var/mrepo/iso
# mv CentOS-6.6-x86_64-bin-DVD1.iso CentOS-6.6-x86_64-bin-DVD2.iso /var/mrepo/iso
# mrepo -ugvv

# cat CentOS-Base.repo
[base-local]
name=CentOS-$releasever - Base
baseurl=http://192.168.76.1/mrepo/centos$releasever-$basearch/RPMS.os/
gpgcheck=0

[updates-local]
name=CentOS-$releasever - Updates
baseurl=http://192.168.76.1/mrepo/centos$releasever-$basearch/RPMS.updates/
gpgcheck=0

[extras-local]
name=CentOS-$releasever - Extras
baseurl=http://192.168.76.1/mrepo/centos$releasever-$basearch/RPMS.extras/
gpgcheck=0

[centosplus-local]
name=CentOS-$releasever - Plus
baseurl=http://192.168.76.1/mrepo/centos$releasever-$basearch/RPMS.centosplus/
gpgcheck=0
enabled=0

[contrib-local]
name=CentOS-$releasever - Contrib
baseurl=http://192.168.76.1/mrepo/centos$releasever-$basearch/RPMS.contrib/
gpgcheck=0
enabled=0

[fasttrack-local]
name=CentOS-$releasever - Fasttrack
baseurl=http://192.168.76.1/mrepo/centos$releasever-$basearch/RPMS.fasttrack/
gpgcheck=0
enabled=0

</pre>

	<p>ディスク容量が20GBほど必要なので、注意する。</p>
<hr />
<a name="NAS設計" />
<a name="NAS設計_NAS設計"></a>
<h1 >NAS設計<a href="#NAS設計_NAS設計" class="wiki-anchor">&para;</a></h1>


	<a name="NAS設計_TFTPサーバ"></a>
<h3 >TFTPサーバ<a href="#NAS設計_TFTPサーバ" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>xinetd設定</li>
	</ul>


<pre>
service tftp
{
        socket_type             = dgram
        protocol                = udp
        wait                    = yes
        user                    = root
        server                  = /usr/sbin/in.tftpd
        server_args             = -c -s /var/lib/tftpboot　←「-c」を付与し書き込みをできるようにする
        disable                 = no　                     ← noに設定しtftpを有効にする
        per_source              = 11
        cps                     = 100 2
        flags                   = IPv4
}
</pre>

	<ul>
	<li>ディレクトリパーミッション<br />   tftpの公開ディレクトリを利用できるようにするため「nobody」に変更する。</li>
	</ul>


<pre>
# ll /var/lib/
drwxr-xr-x  2 nobody  nobody  4096  9月 23 21:33 2011 tftpboot
</pre>

	<a name="NAS設計_NFSサーバ"></a>
<h3 >NFSサーバ<a href="#NAS設計_NFSサーバ" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>Installするモジュール<br />   NFSサーバはrpcbindを利用するためrpcbindをインストールする必要がある<br />   NFSサーバを起動する場合は、先にrpcbindを起動しておく。</li>
	</ul>


	<ul>
	<li>共有ディレクトリ設定</li>
	</ul>


<pre>
/export      10.16.33.0/255.255.255.0(sync,rw,no_root_squash)
</pre>

	<ul>
	<li>共有ディレクトリは以下のオーナに設定する</li>
	</ul>


<pre>
chown -R nfsnobody:nfsnobody /export
</pre>

	<ul>
	<li>rpcbind設定</li>
	</ul>


<pre>
#
# The network configuration file. This file is currently only used in
# conjunction with the TI-RPC code in the libtirpc library.
#
# Entries consist of:
#
#       &lt;network_id&gt; &lt;semantics&gt; &lt;flags&gt; &lt;protofamily&gt; &lt;protoname&gt; \
#               &lt;device&gt; &lt;nametoaddr_libs&gt;
#
# The &lt;device&gt; and &lt;nametoaddr_libs&gt; fields are always empty in this
# implementation.
#
udp        tpi_clts      v     inet     udp     -       -
tcp        tpi_cots_ord  v     inet     tcp     -       -
#udp6       tpi_clts      v     inet6    udp     -       -
#tcp6       tpi_cots_ord  v     inet6    tcp     -       -
rawip      tpi_raw       -     inet      -      -       -
local      tpi_cots_ord  -     loopback  -      -       -
#unix       tpi_cots_ord  -     loopback  -      -       -

</pre>

	<a name="NAS設計_パーティション分割"></a>
<h3 >パーティション分割<a href="#NAS設計_パーティション分割" class="wiki-anchor">&para;</a></h3>


	<p>上記公開するディレクトリ用のパーティションを設定する。</p>


<pre>
# lvcreate -n lv_tftp -L 10GB vg_system
# lvcreate -n lv_export -L 1TB vg_system
# lvcreate -n lv_remotelog -L 300GB vg_system
</pre>

	<p>/etc/fstabに以下の行を追加する。</p>


<pre>
/dev/mapper/vg_system-lv_tftp /var/lib/tftpboot ext4 defaults 1 2
/dev/mapper/vg_system-lv_export /export ext4 defaults 1 2
/dev/mapper/vg_system-lv_remotelog /var/log/remotelog/ ext4 defaults 1 2
</pre>

	<p>以下のコマンドでマウントを行う。</p>


<pre>
&lt;pre&gt;
&lt;/pre&gt;
# mount -a
</pre>

<hr />


	<p>データの保全性を考慮し、NASのデータを更に別の筐体にバックアップする。</p>


	<p>まずはバックアップ用のディレクトリを作成する。<br /><pre>
# mkdir /backup
</pre></p>


	<p>NFS接続を行う。ここではeth7に直接接続された別筐体のNASサーバが存在することを前提とする。</p>


<pre>
# mount -t nfs 10.16.32.212:/share/CACHEDEV1_DATA/Public /backup
# ls /backup
idc-backup
</pre>

	<p>idc-backupフォルダにバックアップを置く。</p>


	<p>最後にアンマウントしておく。</p>


<pre>
# umount /backup
</pre>
<hr />
<a name="Nginx" />
<a name="Nginx_Nginx"></a>
<h1 >Nginx<a href="#Nginx_Nginx" class="wiki-anchor">&para;</a></h1>


<hr />


	<p>TMMCの設定ファイルをサンプルとして上げておきます。</p>


<pre>
upstream finerezept {
    server 192.168.32.1:80;
}

upstream portalsite {
    server 192.168.32.2:80;
}

server {
    listen       443 ssl;
    server_name  check.finerezept.jp;
    ### ssl setting
    ssl_certificate      /etc/nginx/key/check.finerezept.jp/2015checkfinerezeptserver.crt;
    ssl_certificate_key  /etc/nginx/key/check.finerezept.jp/2015checkfinerezept.key;
    ssl_session_cache shared:SSL:1m;
    ssl_session_timeout  5m;
    ssl_ciphers  HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers   on;
    ### ssl offload
    location / {
        proxy_pass   http://finerezept/;
    }
}

### redirect to https from http on finerezept
server {
    listen 80;
    server_name check.finerezept.jp;
    return 301 https://$host$request_uri;
}

server {
    listen       443 ssl;
    server_name  www.finerezept.jp;
    ### ssl setting
    ssl_certificate      /etc/nginx/key/www.finerezept.jp/2015wwwfinerezeptserver.crt;
    ssl_certificate_key  /etc/nginx/key/www.finerezept.jp/2015wwwfinerezept.key;
    ssl_session_cache shared:SSL:1m;
    ssl_session_timeout  5m;
    ssl_ciphers  HIGH:!aNULL:!MD5;
    ssl_prefer_server_ciphers   on;
    ### ssl offload
    location / {
        proxy_pass   http://portalsite/;
    }
}

server {
    listen 80;
    server_name www.finerezept.jp;
    location / {
        proxy_pass   http://portalsite/;
    }
}
</pre>
<hr />
<a name="Nodejs" />
<a name="Nodejs_Nodejs"></a>
<h1 >Nodejs<a href="#Nodejs_Nodejs" class="wiki-anchor">&para;</a></h1>


<pre>
# curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.26.1/install.sh | bash
# . .bashrc
# nvm --version
# nvm install stable
# nvm alias default stable
### .bashrcに記載されるので、ログアウト/ログインを行う
# exit
# ssh [host]
# node --version
</pre>
<hr />
<a name="Ntpd" />
<a name="Ntpd_Ntpd"></a>
<h1 >Ntpd<a href="#Ntpd_Ntpd" class="wiki-anchor">&para;</a></h1>


<hr />


<pre>
Last login: Fri Mar  6 22:12:33 2015 from pd5fbb6.tokyff01.ap.so-net.ne.jp
[root@db ~]# cp /etc/ntp.conf{,.org}
[root@db ~]# cat &lt;&lt; EOF &gt; /etc/ntp.conf
&gt; driftfile /var/lib/ntp/drift
&gt; restrict default kod nomodify notrap nopeer noquery
&gt; restrict -6 default kod nomodify notrap nopeer noquery
&gt; restrict 127.0.0.1
&gt; restrict -6 ::1
&gt; restrict ntp.nict.jp
&gt; server ntp.nict.jp iburst
&gt; includefile /etc/ntp/crypto/pw
&gt; keys /etc/ntp/keys
&gt; EOF
[root@db ~]# /etc/init.d/ntpd start
ntpd を起動中:                                             [  OK  ]
[root@db ~]# chkconfig ntpd on
[root@db ~]# date
2015年  3月  6日 金曜日 17:10:18 JST
[root@db ~]#
</pre>
<hr />
<a name="NTP設計" />
<a name="NTP設計_NTP設計"></a>
<h1 >NTP設計<a href="#NTP設計_NTP設計" class="wiki-anchor">&para;</a></h1>


	<p>NTPサーバは本システムにおいて外部のNTPサービスと同期し内部向けに時刻同期サービスを提供するためのものである</p>


	<ul>
	<li>同期サーバについて

	<p>同期先はntp.nict.jpとする</p></li>
	</ul>


	<ul>
	<li>設定値</li>
	</ul>


<pre>
# egrep -v "(^$|^#)" /etc/ntp.conf
driftfile /var/lib/ntp/drift
restrict default kod nomodify notrap nopeer noquery
restrict 127.0.0.1
restrict 10.0.0.0 mask 255.0.0.0 nomodify notrap
pool ntp.nict.jp
includefile /etc/ntp/crypto/pw
keys /etc/ntp/keys
</pre>
<hr />
<a name="NW機器サポート問い合わせ先" />
<a name="NW機器サポート問い合わせ先_NW機器サポート問い合わせ先"></a>
<h1 >NW機器サポート問い合わせ先<a href="#NW機器サポート問い合わせ先_NW機器サポート問い合わせ先" class="wiki-anchor">&para;</a></h1>


	<a name="NW機器サポート問い合わせ先_ユーザIDパス"></a>
<h2 >ユーザID・パス<a href="#NW機器サポート問い合わせ先_ユーザIDパス" class="wiki-anchor">&para;</a></h2>


	<p>どの製品ページも以下のIDとパスワードでログイン可能。</p>


	<p>【ユーザＩＤ】22997<br />【パスワード】NqRtpslC</p>


	<a name="NW機器サポート問い合わせ先_Juniper-EXシリーズサポートページL2スイッチ"></a>
<h2 >Juniper EXシリーズサポートページ（L2スイッチ）<a href="#NW機器サポート問い合わせ先_Juniper-EXシリーズサポートページL2スイッチ" class="wiki-anchor">&para;</a></h2>


	<p><a class="external" href="https://www1.macnica.net/SCGI/product/support/contents/support_menu.cgi?mode=input&#38;pro_ctgry_id=47">https://www1.macnica.net/SCGI/product/support/contents/support_menu.cgi?mode=input&#38;pro_ctgry_id=47</a></p>


	<a name="NW機器サポート問い合わせ先_Juniper-SRXサポートページファイアウォール"></a>
<h2 >Juniper SRXサポートページ（ファイアウォール）<a href="#NW機器サポート問い合わせ先_Juniper-SRXサポートページファイアウォール" class="wiki-anchor">&para;</a></h2>


	<p><a class="external" href="https://www1.macnica.net/SCGI/product/support/contents/support_menu.cgi?mode=input&#38;pro_ctgry_id=46">https://www1.macnica.net/SCGI/product/support/contents/support_menu.cgi?mode=input&#38;pro_ctgry_id=46</a></p>


	<a name="NW機器サポート問い合わせ先_Citrix-NetScalerサポートページロードバランサ"></a>
<h2 >Citrix NetScalerサポートページ（ロードバランサ）<a href="#NW機器サポート問い合わせ先_Citrix-NetScalerサポートページロードバランサ" class="wiki-anchor">&para;</a></h2>


	<p><a class="external" href="https://www1.macnica.net/SCGI/product/support/contents/support_menu.cgi?mode=input&#38;pro_ctgry_id=30">https://www1.macnica.net/SCGI/product/support/contents/support_menu.cgi?mode=input&#38;pro_ctgry_id=30</a></p>
<hr />
<a name="NW設計" />
<a name="NW設計_NW設計"></a>
<h1 >NW設計<a href="#NW設計_NW設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#NW設計_NW設計">NW設計</a></li><li><a href="#NW設計_Introduction">●Introduction</a><ul><li><a href="#NW設計_目的">目的</a></li></ul>
</li><li><a href="#NW設計_構成">●構成</a><ul><li><a href="#NW設計_物理構成">物理構成</a></li><li><a href="#NW設計_論理構成">論理構成</a></li><li><a href="#NW設計_ハードウェア概要">ハードウェア概要</a></li></ul>
</li><li><a href="#NW設計_基本設計">●基本設計</a><ul><li><a href="#NW設計_Internet-Gateway基本設計">Internet Gateway基本設計</a></li><li><a href="#NW設計_Internet-Gateway詳細設計">Internet Gateway詳細設計</a></li><li><a href="#NW設計_Internet-Firewall基本設計">Internet Firewall基本設計</a></li><li><a href="#NW設計_Internet-Firewall詳細設計">Internet Firewall詳細設計</a></li><li><a href="#NW設計_L3-Switch基本設計">L3 Switch基本設計</a></li><li><a href="#NW設計_L3-Switch詳細設計">L3 Switch詳細設計</a></li><li><a href="#NW設計_Load-Balancer基本設計">Load Balancer基本設計</a></li><li><a href="#NW設計_Load-Balancer詳細設計">Load Balancer詳細設計</a></li><li><a href="#NW設計_L2-Switch基本設計">L2 Switch基本設計</a></li><li><a href="#NW設計_L2-Switch詳細設計">L2 Switch詳細設計</a></li></ul></li></ul>


	<a name="NW設計_Introduction"></a>
<h1 >●Introduction<a href="#NW設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="NW設計_目的"></a>
<h2 >目的<a href="#NW設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>本書は、クラウドサービス向けIaaS基盤におけるネットワークについて<br />その仕様および設計を定義するものである。なお、ネットワークを構成する<br />物理設備については、一部ISBデータセンタ（IDC）に既に設置されている機器<br />を流用するものとし、既存ネットワークの設計思想を踏襲し、且つセキュリティを<br />考慮した設計とする。</p>


	<a name="NW設計_構成"></a>
<h1 >●構成<a href="#NW設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="NW設計_物理構成"></a>
<h2 >物理構成<a href="#NW設計_物理構成" class="wiki-anchor">&para;</a></h2>


	<p>ネットワーク機器の物理構成および接続構成については、別紙「IDC全体構成図.vsd」参照。</p>


	<a name="NW設計_論理構成"></a>
<h2 >論理構成<a href="#NW設計_論理構成" class="wiki-anchor">&para;</a></h2>


	<p>ネットワーク機器の論理構成については、別紙「IDC全体構成図.vsd」参照。なお、既存事業者の構成は変更を加えない。</p>


	<a name="NW設計_ハードウェア概要"></a>
<h2 >ハードウェア概要<a href="#NW設計_ハードウェア概要" class="wiki-anchor">&para;</a></h2>


	<table>
		<tr style="background:#d3eaf3;">
			<th>メーカ</th>
			<th>モデル名</th>
			<th>数量</th>
			<th>用途</th>
		</tr>
		<tr>
			<td>Cisco</td>
			<td>CISCO 2911/K9</td>
			<td>2</td>
			<td>インターネットゲートウェイルータ</td>
		</tr>
		<tr>
			<td>Juniper</td>
			<td>SRX240H2 SRX240-IDP</td>
			<td>2</td>
			<td>ISBクラウド設備およびクラウド事業者向けインターネットファイアウォール</td>
		</tr>
		<tr>
			<td>Juniper</td>
			<td>SRX240-IDP-3</td>
			<td>2</td>
			<td>SRX240H2向けIDP（侵入検知）ライセンス</td>
		</tr>
		<tr>
			<td>Juniper</td>
			<td>EX4550-32F</td>
			<td>2</td>
			<td>L3スイッチ</td>
		</tr>
		<tr>
			<td>Citrix</td>
			<td>NetScaler MPX 5650 Standard Edition</td>
			<td>2</td>
			<td>dataSamplr向けロードバランサ</td>
		</tr>
		<tr>
			<td>Juniper</td>
			<td>EX3300-48T</td>
			<td>2</td>
			<td>L2スイッチ</td>
		</tr>
	</table>




	<a name="NW設計_基本設計"></a>
<h1 >●基本設計<a href="#NW設計_基本設計" class="wiki-anchor">&para;</a></h1>


	<a name="NW設計_Internet-Gateway基本設計"></a>
<h2 >Internet Gateway基本設計<a href="#NW設計_Internet-Gateway基本設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>機能
	<ul>
	<li>インターネット接続<br />　既存環境のPPPOE接続を管理し、インターネット環境から複数のセッションを集約可能な設計とする。</li>
		<li>グローバルIPアドレス管理<br />　各事業者に付与されるグローバルアドレスを管理し、グローバルからローカルアドレスへのNAT機能を提供する。</li>
		<li>ファイアウォール機能<br />　既存環境における不要な通信の遮断を行う。</li>
		<li>メンテナンス用IPsec接続<br />　IDC管理者によるリモートメンテナンスが可能となるよう、Cisco IPsec機能を提供する。</li>
	</ul></li>
	</ul>


	<ul>
	<li>セキュリティ
	<ul>
	<li>インターネット環境からのアクセス制限<br />　既存環境の各公開システムに対して許可する送信元/ポートを絞り、不要な送信元からの意図しない通信を制限する。</li>
		<li>事業者間通信の制限<br />　PPPOEセッション毎に論理的にルータを分割し、事業者毎のインターネットゲートを持つような構成とする。</li>
	</ul></li>
	</ul>


	<ul>
	<li>冗長化
	<ul>
	<li>冗長化構成<br />　耐障害性を考慮したアクティブ/スタンバイの2台の冗長構成とする。</li>
		<li>WAN側冗長化構成<br />　インターネット回線の特性上、ネットワーク回線の自動切り替えは実施せず、手動切り替えとする。<br />　筐体およびインターフェース障害発生時は、手動でWAN側のLANケーブルを差し替える運用とする。</li>
		<li>LAN側冗長化構成<br />　筐体およびインターフェースレベルでの自動切り替え構成とする。<br />　筐体およびインターフェース障害発生時、LAN側インターフェースが自動でもう一方へ切り替わる構成とする。</li>
	</ul></li>
	</ul>


	<a name="NW設計_Internet-Gateway詳細設計"></a>
<h2 ><a href="#Internet_Gateway詳細設計" class="wiki-page">Internet Gateway詳細設計</a><a href="#NW設計_Internet-Gateway詳細設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="NW設計_Internet-Firewall基本設計"></a>
<h2 >Internet Firewall基本設計<a href="#NW設計_Internet-Firewall基本設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>ファイアウォール機能<br />WAN側、LAN側からの通信を監視し、セキュリティポリシーに従い不要な通信の遮断を行う。<br />なお、セキュリティポリシーは全ての事業者に対する共通ポリシーをFW上で管理し、詳細ポリシーについては配下に設置される事業者毎の仮想ルータにて管理する。<br />また、動作対象はIaaSクラウド基盤の事業者とし、既存のコンダクト、大東実業、大東自動車環境については動作対象外とする。</li>
	</ul>


	<ul>
	<li>侵入検知機能<br />ローカルネットワーク上への不正なアクセスの兆候を検知する。検知した内容をもとに、必要に応じてアクセス対象となる事業者へ通知する。</li>
	</ul>


	<ul>
	<li>冗長化<br />耐障害性を考慮した2台の冗長構成とし、筐体の障害時に自動切り替えが可能な構成とする。</li>
	</ul>


	<a name="NW設計_Internet-Firewall詳細設計"></a>
<h2 ><a href="#Internet_Firewall詳細設計" class="wiki-page">Internet Firewall詳細設計</a><a href="#NW設計_Internet-Firewall詳細設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="NW設計_L3-Switch基本設計"></a>
<h2 >L3 Switch基本設計<a href="#NW設計_L3-Switch基本設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>L3ルーティング機能<br />ISBクラウド設備および既存事業者に付与されるVLANネットワークのゲートウェイとして動作し、ISBクラウド設備においては、異なるVLANを持つ設備間のL3ルーティングを行う。 <br />また、既存事業者においては、インターネット環境へのゲートウェイとなる仮想ルータを事業者毎に提供する。なお、L3スイッチが持つインターフェース速度は10GEとする。</li>
	</ul>


	<ul>
	<li>冗長化<br />耐障害性を考慮した2台の冗長構成とし、筐体の障害時に自動切り替えが可能な構成とする。また、インターフェースの冗長化も併せて実施する。</li>
	</ul>


	<a name="NW設計_L3-Switch詳細設計"></a>
<h2 ><a href="#L3_Switch詳細設計" class="wiki-page">L3 Switch詳細設計</a><a href="#NW設計_L3-Switch詳細設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="NW設計_Load-Balancer基本設計"></a>
<h2 >Load Balancer基本設計<a href="#NW設計_Load-Balancer基本設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>ロードバランシング<br />dataSamplr設備上に設置されたサーバに対するリクエストを複数のサーバへ振り分ける。</li>
	</ul>


	<ul>
	<li>SSLアクセラレータ<br />dataSamplr設備上に設置されたWebサーバにおけるSSL処理を代理で行う。</li>
	</ul>


	<ul>
	<li>httpヘッダリライト<br />Webサーバへのリクエスト処理において、httpヘッダをチェックし、ヘッダに含まれる特定のキーワードを指定された値に書き換えた後に下位のサーバへ渡す。</li>
	</ul>


	<a name="NW設計_Load-Balancer詳細設計"></a>
<h2 ><a href="#Load_Balancer詳細設計" class="wiki-page">Load Balancer詳細設計</a><a href="#NW設計_Load-Balancer詳細設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="NW設計_L2-Switch基本設計"></a>
<h2 >L2 Switch基本設計<a href="#NW設計_L2-Switch基本設計" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>機能
	<ul>
	<li>L2スイッチング<br />　物理サーバ群におけるアクセススイッチとして動作し、サーバ間およびネットワーク機器間の通信中継を行う。</li>
		<li>VLAN<br />　 ネットワークをVLANで分割し、定められたサーバ間の通信を中継する。</li>
	</ul>
	</li>
		<li>冗長化
	<ul>
	<li>冗長化構成<br />各種サービスを提供するサーバ群の通信を中継する重要機器である為、耐障害性を考慮した2台の冗長構成とし、筐体の障害時に自動切り替えが可能な構成とする。<br />また、インターフェースの冗長化も併せて実施しインターフェースレベルでの障害時に自動切り替えが可能な構成とする。</li>
	</ul></li>
	</ul>


	<a name="NW設計_L2-Switch詳細設計"></a>
<h2 ><a href="#L2_Switch詳細設計" class="wiki-page">L2 Switch詳細設計</a><a href="#NW設計_L2-Switch詳細設計" class="wiki-anchor">&para;</a></h2>


<hr />
<hr />
<a name="Openvpn" />
<a name="Openvpn_Openvpn"></a>
<h1 >Openvpn<a href="#Openvpn_Openvpn" class="wiki-anchor">&para;</a></h1>


	<p>インストール手順を以下に示す。</p>


<pre>
# yum install epel-release
# yum install openvpn
# chkconfig openvpn on
</pre>
<hr />
<a name="Postfix" />
<a name="Postfix_Postfix"></a>
<h1 >Postfix<a href="#Postfix_Postfix" class="wiki-anchor">&para;</a></h1>


<pre>
# egrep -v "(^#|^$)" /etc/postfix/main.cf
queue_directory = /var/spool/postfix
command_directory = /usr/sbin
daemon_directory = /usr/libexec/postfix
data_directory = /var/lib/postfix
mail_owner = postfix
myhostname = mail.finerezept.net
mydomain = finerezept.net
myorigin = $mydomain
inet_interfaces = all
inet_protocols = all
mydestination = $myhostname, localhost.$mydomain, localhost
unknown_local_recipient_reject_code = 550
mynetworks_style = subnet
mynetworks = 192.168.3.0/24
alias_maps = hash:/etc/aliases
alias_database = hash:/etc/aliases
home_mailbox = Maildir/

debug_peer_level = 2
debugger_command =
     PATH=/bin:/usr/bin:/usr/local/bin:/usr/X11R6/bin
     ddd $daemon_directory/$process_name $process_id &#38; sleep 5
sendmail_path = /usr/sbin/sendmail.postfix
newaliases_path = /usr/bin/newaliases.postfix
mailq_path = /usr/bin/mailq.postfix
setgid_group = postdrop
html_directory = no
manpage_directory = /usr/share/man
sample_directory = /usr/share/doc/postfix-2.6.6/samples
readme_directory = /usr/share/doc/postfix-2.6.6/README_FILES
</pre>

	<p>iptablesの設定等も忘れずに！</p>
<hr />
<a name="Serverspec" />
<a name="Serverspec_Serverspec"></a>
<h1 >Serverspec<a href="#Serverspec_Serverspec" class="wiki-anchor">&para;</a></h1>


	<a name="Serverspec_検証環境Install手順"></a>
<h2 >検証環境Install手順<a href="#Serverspec_検証環境Install手順" class="wiki-anchor">&para;</a></h2>


	<p><strong>■Installする環境について</strong><br />共通設定でInstallされたサーバに対しServerSpecをInstallする<br />ssh接続に関しては鍵認証にて接続できるように設定しておく<br />~/.ssh/configにservermanagerユーザで鍵認証にて接続できるように設定をしておく</p>


	<p><strong>■まずパッケージをダウンロードする</strong><br />rubyとgemsをダウンロードだけする</p>


<blockquote>

[root@ANSIBLESRV ruby]# yum install --downloadonly --downloaddir=/home/package/ruby ruby<br />読み込んだプラグイン:downloadonly, fastestmirror, security<br />インストール処理の設定をしています<br />Loading mirror speeds from cached hostfile
	<ul>
	<li>base: ftp.riken.jp</li>
		<li>epel: ftp.kddilabs.jp</li>
		<li>extras: ftp.riken.jp</li>
		<li>updates: ftp.riken.jp<br />依存性の解決をしています<br />--> トランザクションの確認を実行しています。<br />---> Package ruby.x86_64 0:1.8.7.374-3.el6_6 will be インストール<br />--> 依存性の処理をしています: ruby-libs = 1.8.7.374-3.el6_6 のパッケージ: ruby-1.8.7.374-3.el6_6.x86_64<br />--> 依存性の処理をしています: libruby.so.1.8()(64bit) のパッケージ: ruby-1.8.7.374-3.el6_6.x86_64<br />--> トランザクションの確認を実行しています。<br />---> Package ruby-libs.x86_64 0:1.8.7.374-3.el6_6 will be インストール<br />--> 依存性の処理をしています: libreadline.so.5()(64bit) のパッケージ: ruby-libs-1.8.7.374-3.el6_6.x86_64<br />--> トランザクションの確認を実行しています。<br />---> Package compat-readline5.x86_64 0:5.2-17.1.el6 will be インストール<br />--> 依存性解決を終了しました。</li>
	</ul>


	<p>依存性を解決しました</p>


	<p>=====================================================================================================<br />パッケージ                  アーキテクチャ    バージョン                   リポジトリー        容量
=====================================================================================================<br />インストールしています:<br />ruby                        x86_64            1.8.7.374-3.el6_6            updates            538 k<br />依存性関連でのインストールをします。:<br />compat-readline5            x86_64            5.2-17.1.el6                 base               130 k<br />ruby-libs                   x86_64            1.8.7.374-3.el6_6            updates            1.7 M</p>


	<p>トランザクションの要約
=====================================================================================================<br />インストール         3 パッケージ</p>


	<p>総ダウンロード容量: 2.3 M<br />インストール済み容量: 7.8 M<br />これでいいですか? [y/N]y<br />パッケージをダウンロードしています:<br />(1/3): compat-readline5-5.2-17.1.el6.x86_64.rpm                               | 130 kB     00:00<br />(2/3): ruby-1.8.7.374-3.el6_6.x86_64.rpm                                      | 538 kB     00:00<br />(3/3): ruby-libs-1.8.7.374-3.el6_6.x86_64.rpm                                 | 1.7 MB     00:00<br />-----------------------------------------------------------------------------------------------------<br />合計                                                                 4.4 MB/s | 2.3 MB     00:00</p>


</blockquote>

<blockquote>

exiting because --downloadonly specified<br />[root@ANSIBLESRV ruby]# yum install --downloadonly --downloaddir=/home/package/ruby rubygems<br />読み込んだプラグイン:downloadonly, fastestmirror, security<br />インストール処理の設定をしています<br />Loading mirror speeds from cached hostfile
	<ul>
	<li>base: ftp.riken.jp</li>
		<li>epel: ftp.kddilabs.jp</li>
		<li>extras: ftp.riken.jp</li>
		<li>updates: ftp.riken.jp<br />依存性の解決をしています<br />--> トランザクションの確認を実行しています。<br />---> Package rubygems.noarch 0:1.3.7-5.el6 will be インストール<br />--> 依存性の処理をしています: ruby(abi) = 1.8 のパッケージ: rubygems-1.3.7-5.el6.noarch<br />--> 依存性の処理をしています: ruby-rdoc のパッケージ: rubygems-1.3.7-5.el6.noarch<br />--> 依存性の処理をしています: /usr/bin/ruby のパッケージ: rubygems-1.3.7-5.el6.noarch<br />--> トランザクションの確認を実行しています。<br />---> Package ruby.x86_64 0:1.8.7.374-3.el6_6 will be インストール<br />---> Package ruby-libs.x86_64 0:1.8.7.374-3.el6_6 will be インストール<br />--> 依存性の処理をしています: libreadline.so.5()(64bit) のパッケージ: ruby-libs-1.8.7.374-3.el6_6.x86_64<br />---> Package ruby-rdoc.x86_64 0:1.8.7.374-3.el6_6 will be インストール<br />--> 依存性の処理をしています: ruby-irb = 1.8.7.374-3.el6_6 のパッケージ: ruby-rdoc-1.8.7.374-3.el6_6.x86_64<br />--> トランザクションの確認を実行しています。<br />---> Package compat-readline5.x86_64 0:5.2-17.1.el6 will be インストール<br />---> Package ruby-irb.x86_64 0:1.8.7.374-3.el6_6 will be インストール<br />--> 依存性解決を終了しました。</li>
	</ul>


	<p>依存性を解決しました</p>


	<p>=====================================================================================================<br />パッケージ                  アーキテクチャ    バージョン                   リポジトリー        容量
=====================================================================================================<br />インストールしています:<br />rubygems                    noarch            1.3.7-5.el6                  base               207 k<br />依存性関連でのインストールをします。:<br />compat-readline5            x86_64            5.2-17.1.el6                 base               130 k<br />ruby                        x86_64            1.8.7.374-3.el6_6            updates            538 k<br />ruby-irb                    x86_64            1.8.7.374-3.el6_6            updates            317 k<br />ruby-libs                   x86_64            1.8.7.374-3.el6_6            updates            1.7 M<br />ruby-rdoc                   x86_64            1.8.7.374-3.el6_6            updates            380 k</p>


	<p>トランザクションの要約
=====================================================================================================<br />インストール         6 パッケージ</p>


	<p>合計容量: 3.2 M<br />総ダウンロード容量: 904 k<br />インストール済み容量: 11 M<br />これでいいですか? [y/N]y<br />パッケージをダウンロードしています:<br />(1/3): ruby-irb-1.8.7.374-3.el6_6.x86_64.rpm                                  | 317 kB     00:00<br />(2/3): ruby-rdoc-1.8.7.374-3.el6_6.x86_64.rpm                                 | 380 kB     00:00<br />(3/3): rubygems-1.3.7-5.el6.noarch.rpm                                        | 207 kB     00:00<br />-----------------------------------------------------------------------------------------------------<br />合計                                                                 2.9 MB/s | 904 kB     00:00</p>


	<p>exiting because --downloadonly specified<br />[root@ANSIBLESRV ruby]# ls<br />compat-readline5-5.2-17.1.el6.x86_64.rpm  ruby-libs-1.8.7.374-3.el6_6.x86_64.rpm<br />ruby-1.8.7.374-3.el6_6.x86_64.rpm         ruby-rdoc-1.8.7.374-3.el6_6.x86_64.rpm<br />ruby-irb-1.8.7.374-3.el6_6.x86_64.rpm     rubygems-1.3.7-5.el6.noarch.rpm</p>


</blockquote>

	<p><strong>■ダウンロードしたRPMをInstallする</strong></p>


<blockquote>

	<p>[root@ANSIBLESRV ruby]# cd /home/package/ruby/<br />[root@ANSIBLESRV ruby]# ls<br />compat-readline5-5.2-17.1.el6.x86_64.rpm  ruby-libs-1.8.7.374-3.el6_6.x86_64.rpm<br />ruby-1.8.7.374-3.el6_6.x86_64.rpm         ruby-rdoc-1.8.7.374-3.el6_6.x86_64.rpm<br />ruby-irb-1.8.7.374-3.el6_6.x86_64.rpm     rubygems-1.3.7-5.el6.noarch.rpm<br />[root@ANSIBLESRV ruby]# rpm -ivh ./*<br />準備中...                ########################################### [100%]<br />1:compat-readline5       ########################################### [ 17%]<br />2:ruby-libs              ########################################### [ 33%]<br />3:ruby                   ########################################### [ 50%]<br />4:ruby-irb               ########################################### [ 67%]<br />5:ruby-rdoc              ########################################### [ 83%]<br />6:rubygems               ########################################### [100%]<br />[root@ANSIBLESRV ruby]# ruby -version<br />ruby 1.8.7 (2013-06-27 patchlevel 374) [x86_64-linux]<br />-e:1: undefined local variable or method `rsion' for main:Object (NameError)</p>


</blockquote>

	<p><strong>■ServerSpecをGemsでinstallする</strong></p>


<blockquote>

	<p>[root@ANSIBLESRV ruby]# gem install serverspec<br />Successfully installed rspec-support-3.2.1<br />Successfully installed rspec-core-3.2.0<br />Successfully installed diff-lcs-1.2.5<br />Successfully installed rspec-expectations-3.2.0<br />Successfully installed rspec-mocks-3.2.0<br />Successfully installed rspec-3.2.0<br />Successfully installed rspec-its-1.1.0<br />Successfully installed multi_json-1.10.1<br />Successfully installed net-ssh-2.9.2<br />Successfully installed net-scp-1.2.1<br />Successfully installed specinfra-2.13.1<br />Successfully installed serverspec-2.8.2<br />12 gems installed<br />Installing ri documentation for rspec-support-3.2.1...<br />Installing ri documentation for rspec-core-3.2.0...<br />Installing ri documentation for diff-lcs-1.2.5...<br />Installing ri documentation for rspec-expectations-3.2.0...<br />Installing ri documentation for rspec-mocks-3.2.0...<br />Installing ri documentation for rspec-3.2.0...<br />Installing ri documentation for rspec-its-1.1.0...<br />Installing ri documentation for multi_json-1.10.1...<br />Installing ri documentation for net-ssh-2.9.2...<br />Installing ri documentation for net-scp-1.2.1...<br />Installing ri documentation for specinfra-2.13.1...<br />Installing ri documentation for serverspec-2.8.2...<br />Installing RDoc documentation for rspec-support-3.2.1...<br />Installing RDoc documentation for rspec-core-3.2.0...<br />Installing RDoc documentation for diff-lcs-1.2.5...<br />Installing RDoc documentation for rspec-expectations-3.2.0...<br />Installing RDoc documentation for rspec-mocks-3.2.0...<br />Installing RDoc documentation for rspec-3.2.0...<br />Installing RDoc documentation for rspec-its-1.1.0...<br />Installing RDoc documentation for multi_json-1.10.1...<br />Installing RDoc documentation for net-ssh-2.9.2...<br />Installing RDoc documentation for net-scp-1.2.1...<br />Installing RDoc documentation for specinfra-2.13.1...<br />Installing RDoc documentation for serverspec-2.8.2...</p>


</blockquote>

	<p>rakeもInstallする</p>


<blockquote>

	<p>[root@ANSIBLESRV 192.168.100.216]# gem install rake<br />Successfully installed rake-10.4.2<br />1 gem installed<br />Installing ri documentation for rake-10.4.2...<br />Installing RDoc documentation for rake-10.4.2...</p>


</blockquote>

	<p><strong>■初期設定</strong></p>


<blockquote>

	<p>[root@ANSIBLESRV tmp]# serverspec-init<br />Select OS type:</p>


	<p>1) UN*X<br />2) Windows</p>


	<p>Select number: 1</p>


	<p>Select a backend type:</p>


	<p>1) SSH<br />2) Exec (local)</p>


	<p>Select number: 1</p>


	<p>Vagrant instance y/n: n<br />Input target host name: 192.168.100.216<br />+ spec/<br />+ spec/192.168.100.216/<br />+ spec/192.168.100.216/sample_spec.rb<br />+ spec/spec_helper.rb<br />+ Rakefile<br />+ .rspec</p>


</blockquote>

	<p><strong>■コードの修正</strong></p>


	<p>以下追記</p>


<blockquote>

	<p>[root@ANSIBLESRV spec]# vim spec_helper.rb</p>


	<p>1 require 'rubygems'<br />26 set :request_pty, true</p>


</blockquote>

	<p><strong>■実行してみる</strong></p>


	<p>sshの設定を追加</p>


	<p>rootの.ssh/configに以下を追記</p>


<blockquote>

	<p>[root@ANSIBLESRV spec]# cat ~/.ssh/config<br />Host TESTSRV1<br />HostName 192.168.100.216<br />Port 22<br />User servermanager<br />IdentityFile /home/servermanager/.ssh/id_rsa</p>


</blockquote>

	<p>実行する</p>


<blockquote>

	<p>[root@ANSIBLESRV spec]# rake spec<br />(in /var/tmp/serverspec)<br />/usr/bin/ruby -I/usr/lib/ruby/gems/1.8/gems/rspec-support-3.2.1/lib:/usr/lib/ruby/gems/1.8/gems/rspec-core-3.2.0/lib /usr/lib/ruby/gems/1.8/gems/rspec-core-3.2.0/exe/rspec --pattern spec/TESTSRV1/\*_spec.rb</p>


	<p>Package "httpd" <br />should be installed</p>


	<p>Service "httpd" <br />should be enabled<br />should be running</p>


	<p>Port "80" <br />should be listening</p>


	<p>Finished in 0.52499 seconds (files took 1.26 seconds to load)<br />4 examples, 0 failures</p>


</blockquote>
<hr />
<a name="Serverspec設計" />
<a name="Serverspec設計_Serverspec設計"></a>
<h1 >Serverspec設計<a href="#Serverspec設計_Serverspec設計" class="wiki-anchor">&para;</a></h1>


	<p>ここでは、Serverspecを使用する上での運用ルールを定義する。</p>


	<a name="Serverspec設計_ネットワークについて"></a>
<h2 >ネットワークについて<a href="#Serverspec設計_ネットワークについて" class="wiki-anchor">&para;</a></h2>


	<p>Serverspecパッケージをインストールするサーバはクラウド管理サーバとし、各通信を行うネットワークは<br />クラウド管理ネットワークを使用する。</p>


	<a name="Serverspec設計_Serverspecサーバについて"></a>
<h2 >Serverspecサーバについて<a href="#Serverspec設計_Serverspecサーバについて" class="wiki-anchor">&para;</a></h2>


	<a name="Serverspec設計_ディレクトリ構造"></a>
<h2 >ディレクトリ構造<a href="#Serverspec設計_ディレクトリ構造" class="wiki-anchor">&para;</a></h2>


<blockquote>

	<p>serverspec/<br />├ Rakefile<br />├ spec<br />｜ ├ ミドルウェア名称ディレクトリ<br />｜ ｜ └ *<strong><b></strong>*</b><strong>**</strong>_spec.rb<br />｜ ├ initialize<br />｜ ├ common<br />｜ ├ bind<br />｜ ├ ntpd<br />｜ └ postgresql<br />└ production.yml</p>


</blockquote>

	<ul>
	<li>serverspec<br />   serverspec用のファイルを保存する基本ディレクトリ。<br />   production.yml、Rakefileを配置する</li>
	</ul>


	<ul>
	<li>spec<br />   ミドルウェアごとのディレクトリを配置する</li>
	</ul>


	<ul>
	<li>ミドルウェア名称ディレクトリ<br />   specファイルを配置する</li>
	</ul>


	<ul>
	<li>Rakefile<br />   Serverspec実行用設定ファイル</li>
	</ul>


	<ul>
	<li>production.yml<br />   各ホストとミドルウェアを紐づけるためのファイル</li>
	</ul>


	<a name="Serverspec設計_productionymlについて"></a>
<h2 >production.ymlについて<a href="#Serverspec設計_productionymlについて" class="wiki-anchor">&para;</a></h2>


	<p>以下の書式にて定義する</p>


<blockquote>

	<p>TESTSRV1:　　　←ホスト名を定義する<br />:roles:<br />- initialize　　←ミドルウェア名称ディレクトリを定義<br />- common</p>


	<p>TESTSRV2:<br />:roles:<br />- initialize<br />- common</p>


</blockquote>

	<a name="Serverspec設計_Rakefileについて"></a>
<h2 >Rakefileについて<a href="#Serverspec設計_Rakefileについて" class="wiki-anchor">&para;</a></h2>


	<p>production.ymlを読み取りproduction.yml内で定義された情報を元にミドルウェアごとに用意されたspecファイルを実行する。<br />タスクは「spec」が用意されており、allまたはproduction.yml内に定義されたホスト名を指定する<br />allを指定した場合は、production.yml内に定義されている全てのホストに対してspecファイルを実行する<br />ホスト名称を指定した場合、指定されたホストのみspecファイルを実行する</p>


	<a name="Serverspec設計_スペックファイルについて"></a>
<h2 >スペックファイルについて<a href="#Serverspec設計_スペックファイルについて" class="wiki-anchor">&para;</a></h2>


	<p>ミドルウェアごとに用意する</p>


	<ul>
	<li>initialize_spec.rb

	<p>初期設定用のスペックファイル</p></li>
	</ul>


	<ul>
	<li>common_spec.rb

	<p>共通設定用のスペックファイル</p></li>
	</ul>


	<a name="Serverspec設計_実行の仕方について"></a>
<h2 >実行の仕方について<a href="#Serverspec設計_実行の仕方について" class="wiki-anchor">&para;</a></h2>


	<p>rakeコマンドにて実行する。<br />Rakefileにはspecタスクを記述しており、「rake spec:all」もしくは「rake spec:ホスト名」にて実行する。</p>


	<ol>
	<li>端末にrootユーザでログインする</li>
		<li>Rakefileの置き場所に移動する<br />　cd /var/tmp/serverspec</li>
		<li>rake spec:all又は、rake spec:TESTSERV1</li>
	</ol>


	<a name="Serverspec設計_ホストテスト割り当て"></a>
<h2 >ホストテスト割り当て<a href="#Serverspec設計_ホストテスト割り当て" class="wiki-anchor">&para;</a></h2>


<pre>
idc-occr01:
  :roles:
     - initialize
     - common
     - squid
     - postfix
idc-occr02:
  :roles:
     - initialize
     - common
     - squid
     - postfix
idc-ohyp01:
  :roles:
     - initialize
     - common
     - kvm
idc-ohyp02:
  :roles:
     - initialize
     - common
     - kvm
idc-ohyp03:
  :roles:
     - initialize
     - common
     - kvm
idc-ohyp04:
  :roles:
     - initialize
     - common
     - kvm
idc-ohyp05:
  :roles:
     - initialize
     - common
     - kvm
idc-ohyp06:
  :roles:
     - initialize
     - common
     - kvm
idc-ohyp07:
  :roles:
     - initialize
     - common
     - kvm
idc-omon01:
  :roles:
     - initialize
     - common
idc-omon02:
  :roles:
     - initialize
     - common
idc-onas01:
  :roles:
     - initialize
     - common
     - tftp
     - nfs
     - rsyslog

</pre>

	<a name="Serverspec設計_テスト内容"></a>
<h2 >テスト内容<a href="#Serverspec設計_テスト内容" class="wiki-anchor">&para;</a></h2>


	<a name="Serverspec設計_common"></a>
<h3 >common<a href="#Serverspec設計_common" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-a0644c3a-show" onclick="$(&#x27;#collapse-a0644c3a-show, #collapse-a0644c3a-hide&#x27;).toggle(); $(&#x27;#collapse-a0644c3a&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-a0644c3a-hide" onclick="$(&#x27;#collapse-a0644c3a-show, #collapse-a0644c3a-hide&#x27;).toggle(); $(&#x27;#collapse-a0644c3a&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-a0644c3a" style="display:none;"><ul>
	<li>/etc/hosts<br />  ファイルが存在すること<br />  権限が644であること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>/etc/resolv.conf<br />  ファイルが存在すること<br />  権限が644であること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>/etc/modprobe.d/disable-ipv6.conf<br />  ファイルが存在すること<br />  権限が644であること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>/etc/yum.repos.d/CentOS-Base.repo<br />  ファイルが存在すること<br />  権限が644であること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>/etc/init/control-alt-delete.override<br />  ファイルが存在すること<br />  権限が644であること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>/etc/ntp.conf<br />  ファイルが存在すること<br />  権限が644であること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>/etc/sysconfig/sysstat<br />  ファイルが存在すること<br />  権限が644であること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>/etc/inittab<br />  ファイルが存在していること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>auditd<br />  自動起動がOffになっていること<br />  プロセスが起動していないこと</li>
	</ul>


	<ul>
	<li>haldaemon<br />  自動起動がOffになっていること<br />  プロセスが起動していないこと</li>
	</ul>


	<ul>
	<li>ip6tables<br />  自動起動がOffになっていること<br />  プロセスが起動していないこと</li>
	</ul>


	<ul>
	<li>iptables<br />  自動起動がOffになっていること<br />  プロセスが起動していないこと</li>
	</ul>


	<ul>
	<li>mdmonitor<br />  自動起動がOffになっていること<br />  プロセスが起動していないこと</li>
	</ul>


	<ul>
	<li>messagebus<br />  自動起動がOffになっていること<br />  プロセスが起動していないこと</li>
	</ul>


	<ul>
	<li>postfix<br />  自動起動がOffになっていること<br />  プロセスが起動していないこと</li>
	</ul></div></p>


	<a name="Serverspec設計_initialize"></a>
<h3 >initialize<a href="#Serverspec設計_initialize" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-d0d3d790-show" onclick="$(&#x27;#collapse-d0d3d790-show, #collapse-d0d3d790-hide&#x27;).toggle(); $(&#x27;#collapse-d0d3d790&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-d0d3d790-hide" onclick="$(&#x27;#collapse-d0d3d790-show, #collapse-d0d3d790-hide&#x27;).toggle(); $(&#x27;#collapse-d0d3d790&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-d0d3d790" style="display:none;"><ul>
	<li>selinux<br />  無効になっていること</li>
	</ul>


	<ul>
	<li>isbグループ<br />  GIDが正しいこと<br />  グループが存在すること</li>
	</ul>


	<ul>
	<li>servermanagerグループ<br />  GIDが正しいこと<br />  グループが存在すること</li>
	</ul>


	<ul>
	<li>ta1ユーザ<br />  ユーザが存在すること<br />  グループがisbグループであること<br />  UIDが正しいこと<br />  ホームディレクトリが正しいこと<br />  ログインシェルが正しいこと<br />  authorized_keyの内容が正しいこと</li>
	</ul>


	<ul>
	<li>ta1ユーザのauthorized_key<br />  ファイルが存在していること<br />  権限が600であること<br />  所有者がta1であること<br />  グループがisbであること</li>
	</ul>


	<ul>
	<li>sugaユーザ<br />  ユーザが存在すること<br />  グループがisbグループであること<br />  UIDが正しいこと<br />  ホームディレクトリが正しいこと<br />  ログインシェルが正しいこと<br />  authorized_keyの内容が正しいこと</li>
	</ul>


	<ul>
	<li>sugaユーザのauthorized_key<br />  ファイルが存在していること<br />  権限が600であること<br />  所有者がsugaであること<br />  グループがisbであること</li>
	</ul>


	<ul>
	<li>kajiroユーザ<br />  ユーザが存在すること<br />  グループがisbグループであること<br />  UIDが正しいこと<br />  ホームディレクトリが正しいこと<br />  ログインシェルが正しいこと<br />  authorized_keyの内容が正しいこと</li>
	</ul>


	<ul>
	<li>kajiroユーザのauthorized_key<br />  ファイルが存在していること<br />  権限が600であること<br />  所有者がkajiroであること<br />  グループがisbであること</li>
	</ul>


	<ul>
	<li>akibaユーザ<br />  ユーザが存在すること<br />  グループがisbグループであること<br />  UIDが正しいこと<br />  ホームディレクトリが正しいこと<br />  ログインシェルが正しいこと<br />  authorized_keyの内容が正しいこと</li>
	</ul>


	<ul>
	<li>akibaユーザのauthorized_key<br />  ファイルが存在していること<br />  権限が600であること<br />  所有者がakibaであること<br />  グループがisbであること</li>
	</ul>


	<ul>
	<li>servermanagerユーザ<br />  ユーザが存在すること<br />  グループがisbグループであること<br />  UIDが正しいこと<br />  ホームディレクトリが正しいこと<br />  ログインシェルが正しいこと<br />  authorized_keyの内容が正しいこと</li>
	</ul>


	<ul>
	<li>servermanagerユーザのauthorized_key<br />  ファイルが存在していること<br />  権限が600であること<br />  所有者がservermanagerであること<br />  グループがisbであること</li>
	</ul>


	<ul>
	<li>/etc/sudoers<br />  ファイルが存在すること<br />  内容が正しいこと</li>
	</ul>


	<ul>
	<li>sshd<br />  自動起動がOnになっていること<br />  プロセスが起動されていること</li>
	</ul>


	<ul>
	<li>/etc/ssh/sshd_config<br />  ファイルが存在していること<br />  権限が600であること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>sshdの待ち受けポート<br />  22番ポートにてリッスンしていること</li>
	</ul></div></p>


	<a name="Serverspec設計_kvm"></a>
<h3 >kvm<a href="#Serverspec設計_kvm" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-8fa24198-show" onclick="$(&#x27;#collapse-8fa24198-show, #collapse-8fa24198-hide&#x27;).toggle(); $(&#x27;#collapse-8fa24198&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-8fa24198-hide" onclick="$(&#x27;#collapse-8fa24198-show, #collapse-8fa24198-hide&#x27;).toggle(); $(&#x27;#collapse-8fa24198&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-8fa24198" style="display:none;"><ul>
	<li>qemu-kvm<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>python-virtinst<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>virt-manager<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>virt-viewer<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>libvirt<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>libvirt-client<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>virt-who<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>libguestfs<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>equallogic-host-tools<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>kmod-dell-dm-switch<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>libvirtグループ<br />  グループが存在すること<br />  GIDが正しいこと<br />  libvirtグループにkvmmanagerグループが紐付いていること</li>
	</ul>


	<ul>
	<li>kvmmanagerグループ<br />  グループが存在すること<br />  GIDが正しいこと</li>
	</ul>


	<ul>
	<li>kvmmanagerユーザ<br />  ユーザが存在すること<br />  グループがkvmmanagerグループであること<br />  UIDが正しいこと<br />  ホームディレクトリが正しいこと<br />  ログインシェルが正しいこと<br />  authorized_keyの内容が正しいこと</li>
	</ul></div></p>


	<a name="Serverspec設計_nfs"></a>
<h3 >nfs<a href="#Serverspec設計_nfs" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-67bce03d-show" onclick="$(&#x27;#collapse-67bce03d-show, #collapse-67bce03d-hide&#x27;).toggle(); $(&#x27;#collapse-67bce03d&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-67bce03d-hide" onclick="$(&#x27;#collapse-67bce03d-show, #collapse-67bce03d-hide&#x27;).toggle(); $(&#x27;#collapse-67bce03d&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-67bce03d" style="display:none;"><ul>
	<li>nfs-utils<br />  インストールされていること</li>
	</ul>


	<ul>
	<li>nfs<br />  自動起動がOnになっていること<br />  プロセスが起動していること</li>
	</ul>


	<ul>
	<li>rpcbind<br />  自動起動がOnになっていること<br />  プロセスが起動していること</li>
	</ul>


	<ul>
	<li>NFSサーバの待ち受けポート<br />  69番ポートがリッスンになっていること</li>
	</ul>


	<ul>
	<li>/export<br />  ディレクトリが存在していること<br />  権限が755になっていること<br />  所有者が正しいこと<br />  グループが正しいこと</li>
	</ul>


	<ul>
	<li>/etc/exports<br />　ファイルが存在していること<br />　権限が644になっていること<br />　所有者がrootになっていること<br />　グループがrootになっていること<br />  内容が正しいこと</li>
	</ul>


	<ul>
	<li>/etc/netconfig<br />　ファイルが存在していること<br />　権限が644になっていること<br />　所有者がrootになっていること<br />　グループがrootになっていること<br />  内容が正しいこと</li>
	</ul></div></p>


	<a name="Serverspec設計_ntpd"></a>
<h3 >ntpd<a href="#Serverspec設計_ntpd" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-ad52daa4-show" onclick="$(&#x27;#collapse-ad52daa4-show, #collapse-ad52daa4-hide&#x27;).toggle(); $(&#x27;#collapse-ad52daa4&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-ad52daa4-hide" onclick="$(&#x27;#collapse-ad52daa4-show, #collapse-ad52daa4-hide&#x27;).toggle(); $(&#x27;#collapse-ad52daa4&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-ad52daa4" style="display:none;"><ul>
	<li>ntpのインストール<br />  ntpがインストールtされていること</li>
	</ul>


	<ul>
	<li>ntpd<br />  ntpdが自動起動ONになっていること<br />  ntpdのプロセスが起動していること</li>
	</ul>


	<ul>
	<li>ntpdの待ち受けポート<br />  123番ポートがリッスンになっていること</li>
	</ul>


	<ul>
	<li>/etc/ntp.conf<br />  ファイルの内容が正しいこと</li>
	</ul></div></p>


	<a name="Serverspec設計_postfix"></a>
<h3 >postfix<a href="#Serverspec設計_postfix" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-43393b7a-show" onclick="$(&#x27;#collapse-43393b7a-show, #collapse-43393b7a-hide&#x27;).toggle(); $(&#x27;#collapse-43393b7a&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-43393b7a-hide" onclick="$(&#x27;#collapse-43393b7a-show, #collapse-43393b7a-hide&#x27;).toggle(); $(&#x27;#collapse-43393b7a&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-43393b7a" style="display:none;"><ul>
	<li>postfixのインストール<br />  Postfixがインストールされていること</li>
	</ul>


	<ul>
	<li>postfix<br />  Postfixが自動起動Onになっていること<br />  Postfixのプロセスが起動していること</li>
	</ul>


	<ul>
	<li>postfixの待ち受けポート<br />  25番ポートがリッスンになっていること</li>
	</ul>


	<ul>
	<li>/etc/postfix/main.cf<br />  ファイルの内容が正しいこと</li>
	</ul></div></p>


	<a name="Serverspec設計_rsyslog"></a>
<h3 >rsyslog<a href="#Serverspec設計_rsyslog" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-2e7294bb-show" onclick="$(&#x27;#collapse-2e7294bb-show, #collapse-2e7294bb-hide&#x27;).toggle(); $(&#x27;#collapse-2e7294bb&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-2e7294bb-hide" onclick="$(&#x27;#collapse-2e7294bb-show, #collapse-2e7294bb-hide&#x27;).toggle(); $(&#x27;#collapse-2e7294bb&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-2e7294bb" style="display:none;"><ul>
	<li>rsyslogのインストール<br />  rsyslogがインストールされていること</li>
	</ul>


	<ul>
	<li>rsyslogd<br />  rsyslogが自動起動Onになっていること<br />  rsyslogのプロセスが起動していること</li>
	</ul>


	<ul>
	<li>rsyslogの待ち受けポート<br />  514番ポートがリッスンになっていること</li>
	</ul>


	<ul>
	<li>/etc/rsyslog.conf<br />  ファイルの内容が正しいこと</li>
	</ul></div></p>


	<a name="Serverspec設計_squid"></a>
<h3 >squid<a href="#Serverspec設計_squid" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-a37c7d0a-show" onclick="$(&#x27;#collapse-a37c7d0a-show, #collapse-a37c7d0a-hide&#x27;).toggle(); $(&#x27;#collapse-a37c7d0a&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-a37c7d0a-hide" onclick="$(&#x27;#collapse-a37c7d0a-show, #collapse-a37c7d0a-hide&#x27;).toggle(); $(&#x27;#collapse-a37c7d0a&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-a37c7d0a" style="display:none;"><ul>
	<li>squidのインストール<br />  suqidがインストールされていること</li>
	</ul>


	<ul>
	<li>squid<br />  squidが自動起動Onになっていること<br />  squidのプロセスが起動していること</li>
	</ul>


	<ul>
	<li>/etc/squid/squid.conf<br />  ファイルが存在していること<br />　権限が640であること<br />　所有者がrootであること<br />　グループがsquidであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>squidの待ち受けポート<br />  8080番ポートがリッスンになっていること</li>
	</ul></div></p>


	<a name="Serverspec設計_tftp"></a>
<h3 >tftp<a href="#Serverspec設計_tftp" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-1c3ead0c-show" onclick="$(&#x27;#collapse-1c3ead0c-show, #collapse-1c3ead0c-hide&#x27;).toggle(); $(&#x27;#collapse-1c3ead0c&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-1c3ead0c-hide" onclick="$(&#x27;#collapse-1c3ead0c-show, #collapse-1c3ead0c-hide&#x27;).toggle(); $(&#x27;#collapse-1c3ead0c&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-1c3ead0c" style="display:none;"><ul>
	<li>tftp-serverのインストール<br />  tftp-serverがインストールされていること</li>
	</ul>


	<ul>
	<li>xinetd<br />  xinetdが自動起動Onになっていること<br />  xinetdのプロセスが起動していること</li>
	</ul>


	<ul>
	<li>待ち受けポート<br />  69番ポートがリッスンになっていること</li>
	</ul>


	<ul>
	<li>/var/lib/tftpboot<br />  ディレクトリが存在すること<br />  権限が755になっていること<br />  所有者がnobodyになっていること<br />  グループがnobodyになっていること</li>
	</ul>


	<ul>
	<li>/etc/xinetd.d/tftp<br />  ファイルが存在していること<br />  権限が644であること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul></div></p>


	<a name="Serverspec設計_backup"></a>
<h3 >backup<a href="#Serverspec設計_backup" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-9583372d-show" onclick="$(&#x27;#collapse-9583372d-show, #collapse-9583372d-hide&#x27;).toggle(); $(&#x27;#collapse-9583372d&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-9583372d-hide" onclick="$(&#x27;#collapse-9583372d-show, #collapse-9583372d-hide&#x27;).toggle(); $(&#x27;#collapse-9583372d&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-9583372d" style="display:none;"><ul>
	<li>/export<br />  ディレクトリが存在すること<br />  権限が755になっていること<br />  所有者がnfsnobodyになっていること<br />  グループがnfsnobodyになっていること<br />  nfsのディレクトリをマウントしていること</li>
	</ul>


	<ul>
	<li>/etc/cron.d/backup_idc<br />  ファイルが存在していること<br />  権限が644になっていること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>/usr/local/bin/backup_idc.sh<br />  ファイルが存在すること<br />  権限が744であること<br />  所有者がrootであること<br />  グループがrootであること</li>
	</ul></div></p>


	<a name="Serverspec設計_zabbix-agent"></a>
<h3 >zabbix-agent<a href="#Serverspec設計_zabbix-agent" class="wiki-anchor">&para;</a></h3>


	<p><a class="collapsible collapsed" href="#" id="collapse-28f9f8c9-show" onclick="$(&#x27;#collapse-28f9f8c9-show, #collapse-28f9f8c9-hide&#x27;).toggle(); $(&#x27;#collapse-28f9f8c9&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-28f9f8c9-hide" onclick="$(&#x27;#collapse-28f9f8c9-show, #collapse-28f9f8c9-hide&#x27;).toggle(); $(&#x27;#collapse-28f9f8c9&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-28f9f8c9" style="display:none;"><ul>
	<li>zabbix-agentのインストール<br />  zabbix-agentがインストールされていること</li>
	</ul>


	<ul>
	<li>自動起動設定<br />  zabbix-agentが自動起動Onになっていること<br />　zabbix-agentのプロセスが起動していること</li>
	</ul>


	<ul>
	<li>/etc/zabbix/zabbix_agentd.conf<br />  ファイルが存在していること<br />  権限が644になっていること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>/etc/logrotate.d/zabbix-agent<br />  ファイルが存在していること<br />  権限が644になっていること<br />  所有者がrootであること<br />  グループがrootであること<br />  ファイルの内容が正しいこと</li>
	</ul>


	<ul>
	<li>zabbix-agentの待ち受けポート<br />  10050番ポートがリッスンになっていること</li>
	</ul></div></p>
<hr />
<a name="Squid" />
<a name="Squid_Squid"></a>
<h1 >Squid<a href="#Squid_Squid" class="wiki-anchor">&para;</a></h1>


	<a name="Squid_検証環境Install手順"></a>
<h2 >検証環境Install手順<a href="#Squid_検証環境Install手順" class="wiki-anchor">&para;</a></h2>


	<p>以下のコマンドにてInstallする</p>


<blockquote>

	<p>yum install squid</p>


</blockquote>
<hr />
<a name="TKVMについて" />
<a name="TKVMについて_TKVMについて"></a>
<h1 >TKVMについて<a href="#TKVMについて_TKVMについて" class="wiki-anchor">&para;</a></h1>


<hr />


	<a name="TKVMについて_目的"></a>
<h2 >目的<a href="#TKVMについて_目的" class="wiki-anchor">&para;</a></h2>


	<p>TKVMはクラウドの評価用として自由に提供可能なサーバである。<br />自由とは言っても、TKVMは既に関係各社に利用されているため、<br />相互に通信出来る状態はセキュリティ的にも望ましくない。<br />そのため、構築時には規律や指針が必要である。</p>


	<p>本書はTKVMの運用指針を記載しメンテナンスや新規環境の追加を容易にすることを目的とする。</p>


<hr />


	<a name="TKVMについて_構成"></a>
<h2 >構成<a href="#TKVMについて_構成" class="wiki-anchor">&para;</a></h2>


	<p>TKVMの構成を以下に示す。</p>


	<p><img src="/attachments/download/908/tkvm_architecture.png" alt="" /></p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>項目名</th>
			<th>内容</th>
		</tr>
		<tr>
			<th>os</th>
			<td>centos6.6</td>
		</tr>
		<tr>
			<th>cpu</th>
			<td></td>
		</tr>
		<tr>
			<th>memory</th>
			<td>128GB</td>
		</tr>
		<tr>
			<th>disk1</th>
			<td>300GB(OS領域)</td>
		</tr>
		<tr>
			<th>disk2</th>
			<td>4.8TB(データ領域)</td>
		</tr>
	</table>




	<p>TKVMは2台のシングル構成である。<br />centosインストール時にグループパッケージとして"ベース"、"開発ツール"、"日本語ベース"、<br />および仮想化関連のパッケージを導入した。</p>


	<p>TKVMはグローバルセグメントと物理的に接続しているが、<br />IPは持たない。</p>


	<p>TKVMのVMを踏まえた構成を記載する。<br />ここでは具体的な例として現在稼働しているTMMCを例に挙げる。</p>


	<p><img src="/attachments/download/906/tkvm_nw_architecture.png" alt="" /></p>


	<p>TKVMブリッジインターフェイスを設定し、VMのインターフェイスをブリッジ接続する。<br />ここではbr0のみを作成しているが、本番ではbr0.100等のVLANを切り論理ネットワークを分断する必要がある。<br />そのため、VM内部のネットワークを192.168.100.0/24等に変更すると通信出来てしまう懸念がある。</p>


	<p>各NWを作成する毎(図の例では192.168.3.0/24)にVRFを作成し、VRFをデフォルトゲートウェイとする。</p>


	<p>インターネットからVMへの通信経路を以下に示す。</p>


	<ol>
	<li>インターネット</li>
		<li>router(202.215.185.225)</li>
		<li>vrf external(202.215.185.234)</li>
		<li>vrf internal(192.168.3.254) ※</li>
		<li>vm(192.168.3.1)</li>
	</ol>


	<p>※destination natの設定が必要</p>


	<p>VMからインターネットへの通信経路を以下に示す。</p>


	<ol>
	<li>vm(192.168.3.1)</li>
		<li>vrf internal(192.168.3.254) ※</li>
		<li>vrf external(202.215.185.234)</li>
		<li>router(202.215.185.225)</li>
		<li>インターネット</li>
	</ol>


	<p>※source natの設定が必要</p>


	<p>なお厳密にはTKVMのethやbrを経由している。</p>


<hr />


	<a name="TKVMについて_仮想インスタンス作成手順"></a>
<h2 >仮想インスタンス作成手順<a href="#TKVMについて_仮想インスタンス作成手順" class="wiki-anchor">&para;</a></h2>


	<p>まず手順の概要を示す。</p>


	<ol>
	<li>isoファイルの取得</li>
		<li>lvm領域の作成</li>
		<li>virt-installコマンドの発行</li>
		<li>vnc接続によるOSインストール</li>
	</ol>


	<a name="TKVMについて_isoファイルの取得"></a>
<h3 >isoファイルの取得<a href="#TKVMについて_isoファイルの取得" class="wiki-anchor">&para;</a></h3>


	<p>VMのインストールイメージ(isoファイル)を取得する。<br />現在TKVMには/var/lib/libvirt/images配下にcentos6.6、vyos1.1.3のisoが存在する。</p>


	<a name="TKVMについて_lvm領域の作成"></a>
<h3 >lvm領域の作成<a href="#TKVMについて_lvm領域の作成" class="wiki-anchor">&para;</a></h3>


	<p>以下のコマンドを発行し、lvm領域を作成する。<br /><pre>
lvcreate -L 20GB vg_vm
</pre></p>


	<p>データ領域はvg_vmというボリューム領域に属している。<br />そこから必要な領域を切り出す。<br />lvm領域の名前を指定する場合は-nオプションを指定し、VM名を割り当てる。</p>


	<a name="TKVMについて_virt-installコマンドの発行"></a>
<h3 >virt-installコマンドの発行<a href="#TKVMについて_virt-installコマンドの発行" class="wiki-anchor">&para;</a></h3>


	<p>以下のコマンドを発行し、VMを作成する。<br /><pre>
virt-install -n &lt;VM名&gt; -r &lt;メモリ:MB&gt; --vcpus=&lt;cpu数&gt; --disk=/dev/vg_vm/&lt;lvm名&gt; --cdrom &lt;isoパス&gt; --vnc --vncport=&lt;VNC利用ポート&gt; --network bridge=&lt;接続インターフェイス名&gt;
</pre></p>


	<p>具体的な例を以下に示す。<br /><pre>
virt-install -n tmmc_finerezept -r 8192 --vcpus=2 --disk=/dev/vg_vm/lvol0 --cdrom /var/lib/libvirt/images/CentOS-6.6-x86_64-bin-DVD1.iso --vnc --vncport=5900 --network bridge=br0
</pre></p>


	<a name="TKVMについて_vnc接続によるOSインストール"></a>
<h3 >vnc接続によるOSインストール<a href="#TKVMについて_vnc接続によるOSインストール" class="wiki-anchor">&para;</a></h3>


	<p>上記手順後、5900ポートでvnc接続可能となる。</p>


	<p>5900ポートで接続できない場合は以下に注意する。</p>


	<ul>
	<li>macでのリモート接続の場合、パスワードが必須となる場合があるため別のvncクライアント利用を検討する</li>
		<li>/etc/libvirt/qemu.confのvncバインドアドレスが127.0.0.1になっている場合は設定変更後、libvirtdを再起動する</li>
	</ul>


	<p>接続後通常通りのOSインストールを行う。<br />OSインストールの再起動後、NWの設定を行う必要が有るが、<br />VNC接続ではvimを実行した際に「:」が打てない。<br />その場合はshift+1を2回押下すると「:.!」とvimが認識するので、「.!」を削除して「wq」を入れることで終了できる。</p>


	<p>また、ssh接続を検討する場合は以下の設定を行う。</p>


	<ul>
	<li>ソースIPアドレスの絞り込み</li>
		<li>公開鍵認証の許可設定</li>
		<li>パスワード認証の拒否設定</li>
	</ul>


<hr />


	<a name="TKVMについて_仮想インスタンスNW設定手順"></a>
<h2 >仮想インスタンスNW設定手順<a href="#TKVMについて_仮想インスタンスNW設定手順" class="wiki-anchor">&para;</a></h2>


	<a name="TKVMについて_VRF"></a>
<h3 >VRF<a href="#TKVMについて_VRF" class="wiki-anchor">&para;</a></h3>


	<p>今回VRFとしてvyosを利用した。<br />vyosの設定は以下を参考にする。</p>


	<p><a href="http://wiki.vyos-users.jp/%E3%83%A6%E3%83%BC%E3%82%B6%E3%83%BC%E3%82%AC%E3%82%A4%E3%83%89" class="external">vyos日本語ユーザガイド</a></p>


	<p>基本的な設定項目を以下に示す。</p>


	<ul>
	<li>firewall
	<ul>
	<li>ソースIPでの絞り込みは上位の物理ルータで行っている点に注意する</li>
	</ul>
	</li>
		<li>interface</li>
		<li>destination nat
	<ul>
	<li>インターネットからVMにアクセスが必要な場合に設定が必要</li>
	</ul>
	</li>
		<li>source nat
	<ul>
	<li>VMからインターネットへアクセスする際に設定が必要</li>
	</ul>
	</li>
		<li>static route
	<ul>
	<li>グローバルIPを持つ場合は202.215.185.225をデフォルトゲートウェイに設定する必要がある</li>
	</ul>
	</li>
		<li>dns forwarding
	<ul>
	<li>ベクタントのDNS(163.139.230.168)を設定する</li>
	</ul>
	</li>
		<li>ssh
	<ul>
	<li>ssh接続が必要な場合は設定するが、別途管理IPを持ち(192.168.100/24が妥当)そのIPからのみを待ち受けるようにlisten-addressを設定する</li>
	</ul></li>
	</ul>


	<a name="TKVMについて_複数NICを設定する場合のlibvirtでの注意点について"></a>
<h3 >複数NICを設定する場合のlibvirtでの注意点について<a href="#TKVMについて_複数NICを設定する場合のlibvirtでの注意点について" class="wiki-anchor">&para;</a></h3>


	<p>VMが複数のNICを設定する場合はvirsh attach-interfaceにて稼働中のVMに設定できる。<br />ただし、恒久的な設定ではないため、virsh dumpxmlでxmlファイル作成後、<br />virsh undefine、virsh defineを行って設定を保存すること。</p>


<hr />
<hr />
<a name="TMMC用試験環境" />
<a name="TMMC用試験環境_TMMC用試験環境"></a>
<h1 >TMMC用試験環境<a href="#TMMC用試験環境_TMMC用試験環境" class="wiki-anchor">&para;</a></h1>


	<p><img src="/attachments/download/902/tmmc_nw_architecture.png" alt="" /></p>
<hr />
<a name="Vmware形式の仮想マシンをkvmへ移行" />
<a name="Vmware形式の仮想マシンをkvmへ移行_Vmware形式の仮想マシンをkvmへ移行"></a>
<h1 >Vmware形式の仮想マシンをkvmへ移行<a href="#Vmware形式の仮想マシンをkvmへ移行_Vmware形式の仮想マシンをkvmへ移行" class="wiki-anchor">&para;</a></h1>


	<a name="Vmware形式の仮想マシンをkvmへ移行_vmdkファイルimgファイルへ変換"></a>
<h3 >vmdkファイル→imgファイルへ変換<a href="#Vmware形式の仮想マシンをkvmへ移行_vmdkファイルimgファイルへ変換" class="wiki-anchor">&para;</a></h3>


	<p>★準備するもの<br />vmwareサーバからエクスポートしてきた仮想マシン構成ファイル一式。</p>


	<p>vmwareサーバから仮想マシンをエクスポートすると、.vmdk、.nvram、.vmx等の構成ファイル一式が<br />できあがる。この中から、ディスクを構成するファイルである.vmdkをkvmで読み込めるディスクイメージ形式<br />の.imgに変換する。</p>


<pre>
-rw-r--r-- 1 root root   440860672  8月  8 10:07 2014 CentOS-s001.vmdk ★エクスポート時にディスクが分割されているので番号付きの.vmdkがたくさんある。
-rw-r--r-- 1 root root     1769472  8月  8 10:07 2014 CentOS-s002.vmdk
-rw-r--r-- 1 root root   605028352  8月  8 10:07 2014 CentOS-s003.vmdk
-rw-r--r-- 1 root root   260833280  8月  8 10:07 2014 CentOS-s004.vmdk
-rw-r--r-- 1 root root    17039360  8月  8 10:07 2014 CentOS-s005.vmdk
-rw-r--r-- 1 root root     1703936  8月  8 10:07 2014 CentOS-s006.vmdk
-rw-r--r-- 1 root root   313982976  8月  8 10:07 2014 CentOS-s007.vmdk
-rw-r--r-- 1 root root  1748631552  8月  8 10:07 2014 CentOS-s008.vmdk
-rw-r--r-- 1 root root  1191510016  8月  8 10:07 2014 CentOS-s009.vmdk
-rw-r--r-- 1 root root      458752  8月  8 10:07 2014 CentOS-s010.vmdk
-rw-r--r-- 1 root root       65536  8月  8 10:07 2014 CentOS-s011.vmdk
-rw-r--r-- 1 root root        8684  8月  8 10:07 2014 CentOS.nvram
-rw-r--r-- 1 root root         882  8月  8 09:58 2014 CentOS.vmdk　　　★こいつがターゲットとなるvmdkの親ファイル。imgファイルに変換する際はこいつを指定する。
-rw-r--r-- 1 root root           0  6月 23 15:24 2014 CentOS.vmsd
-rw-r--r-- 1 root root        2945  2月 14 23:01 2015 CentOS.vmx
-rw-r--r-- 1 root root        3420  6月 23 15:38 2014 CentOS.vmxf
drwxr-xr-x 3 root root        4096  2月 12 23:13 2015 caches
-rw-r--r-- 1 root root        2941  8月  8 10:07 2014 org_CentOS.vmx
-rw-r--r-- 1 root root      175498  8月  8 09:28 2014 vmware-0.log
-rw-r--r-- 1 root root      202100  8月  7 18:27 2014 vmware-1.log
-rw-r--r-- 1 root root      179388  8月  7 11:31 2014 vmware-2.log
-rw-r--r-- 1 root root      206473  8月  8 10:07 2014 vmware.log
-rw-r--r-- 1 root root      190946  8月  8 10:07 2014 vprintproxy.log
</pre>

	<p>変換するためのコマンドはqemu-imgで用意されてる。.vmdkを.imgに変換<br /><pre>
qemu-img convert -f vmdk &lt;vmdk親ファイルパス&gt; -O raw &lt;出力するイメージファイル名&gt;
</pre></p>


	<p>変換処理が終了すると、.imgファイルが出来上がる。<br /><pre>
-rw-r--r-- 1 root root   440860672  8月  8 10:07 2014 CentOS-s001.vmdk
-rw-r--r-- 1 root root     1769472  8月  8 10:07 2014 CentOS-s002.vmdk
-rw-r--r-- 1 root root   605028352  8月  8 10:07 2014 CentOS-s003.vmdk
-rw-r--r-- 1 root root   260833280  8月  8 10:07 2014 CentOS-s004.vmdk
-rw-r--r-- 1 root root    17039360  8月  8 10:07 2014 CentOS-s005.vmdk
-rw-r--r-- 1 root root     1703936  8月  8 10:07 2014 CentOS-s006.vmdk
-rw-r--r-- 1 root root   313982976  8月  8 10:07 2014 CentOS-s007.vmdk
-rw-r--r-- 1 root root  1748631552  8月  8 10:07 2014 CentOS-s008.vmdk
-rw-r--r-- 1 root root  1191510016  8月  8 10:07 2014 CentOS-s009.vmdk
-rw-r--r-- 1 root root      458752  8月  8 10:07 2014 CentOS-s010.vmdk
-rw-r--r-- 1 root root       65536  8月  8 10:07 2014 CentOS-s011.vmdk
-rw-r--r-- 1 root root        8684  8月  8 10:07 2014 CentOS.nvram
-rw-r--r-- 1 root root         882  8月  8 09:58 2014 CentOS.vmdk
-rw-r--r-- 1 root root           0  6月 23 15:24 2014 CentOS.vmsd
-rw-r--r-- 1 root root        2945  2月 14 23:01 2015 CentOS.vmx
-rw-r--r-- 1 root root        3420  6月 23 15:38 2014 CentOS.vmxf
-rw-r--r-- 1 root root 21474836480  2月 14 23:06 2015 CentOS.img　★imgファイルが出来上がった。
drwxr-xr-x 3 root root        4096  2月 12 23:13 2015 caches
-rw-r--r-- 1 root root        2941  8月  8 10:07 2014 org_CentOS.vmx
-rw-r--r-- 1 root root      175498  8月  8 09:28 2014 vmware-0.log
-rw-r--r-- 1 root root      202100  8月  7 18:27 2014 vmware-1.log
-rw-r--r-- 1 root root      179388  8月  7 11:31 2014 vmware-2.log
-rw-r--r-- 1 root root      206473  8月  8 10:07 2014 vmware.log
-rw-r--r-- 1 root root      190946  8月  8 10:07 2014 vprintproxy.log
</pre></p>


	<a name="Vmware形式の仮想マシンをkvmへ移行_仮想マシン起動準備"></a>
<h3 >仮想マシン起動準備<a href="#Vmware形式の仮想マシンをkvmへ移行_仮想マシン起動準備" class="wiki-anchor">&para;</a></h3>


	<p>変換した.imgファイルを読み込んで仮想マシンをkvmで管理できるようにする。</p>


	<p>imgファイルを一時的に移動。ちなみに/rootにimgファイルを置いておくと、virsh start時にpermission denedで起動できない。<br /><pre>
cp -p CentOS_v2v.img /var/iso/
</pre></p>


	<p>kvmでの仮想マシン構成ファイル.xmlを作成する。元ネタは既存仮想マシン構成ファイルをコピーする形でよい。<br /><pre>
cp -p /etc/libvirt/qemu/tmmc_corticon.xml /etc/libvirt/qemu/ricoh_db.xml
</pre></p>


	<p>.xmlの中身を編集。</p>


	<p>変種前。既存構成ファイルをそのままコピーしただけの状態。<br /><pre>
&lt;domain type='kvm'&gt;
  &lt;name&gt;tmmc_corticon&lt;/name&gt;
  &lt;uuid&gt;3f9d808a-fd1f-7877-6179-a963604f91a5&lt;/uuid&gt;
  &lt;memory unit='KiB'&gt;4194304&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;4194304&lt;/currentMemory&gt;
  &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;
  &lt;os&gt;
    &lt;type arch='x86_64' machine='rhel6.6.0'&gt;hvm&lt;/type&gt;
    &lt;boot dev='hd'/&gt;
  &lt;/os&gt;
  &lt;features&gt;
    &lt;acpi/&gt;
    &lt;apic/&gt;
    &lt;pae/&gt;
  &lt;/features&gt;
  &lt;clock offset='utc'/&gt;
  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
  &lt;on_crash&gt;restart&lt;/on_crash&gt;
  &lt;devices&gt;
    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;
    &lt;disk type='block' device='disk'&gt;
      &lt;driver name='qemu' type='raw' cache='none' io='native'/&gt;
      &lt;source dev='/dev/vg_vm/lvol4'/&gt;
      &lt;target dev='hda' bus='ide'/&gt;
      &lt;address type='drive' controller='0' bus='0' target='0' unit='0'/&gt;
    &lt;/disk&gt;
    &lt;disk type='block' device='cdrom'&gt;
      &lt;driver name='qemu' type='raw'/&gt;
      &lt;target dev='hdc' bus='ide'/&gt;
      &lt;readonly/&gt;
      &lt;address type='drive' controller='0' bus='1' target='0' unit='0'/&gt;
    &lt;/disk&gt;
    &lt;controller type='usb' index='0' model='ich9-ehci1'&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x7'/&gt;
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci1'&gt;
      &lt;master startport='0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0' multifunction='on'/&gt;
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci2'&gt;
      &lt;master startport='2'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x1'/&gt;
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci3'&gt;
      &lt;master startport='4'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x2'/&gt;
    &lt;/controller&gt;
    &lt;controller type='ide' index='0'&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/&gt;
    &lt;/controller&gt;
    &lt;interface type='bridge'&gt;
      &lt;mac address='52:54:00:24:0c:7b'/&gt;
      &lt;source bridge='br0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&gt;
    &lt;/interface&gt;
    &lt;serial type='pty'&gt;
      &lt;target port='0'/&gt;
    &lt;/serial&gt;
    &lt;console type='pty'&gt;
      &lt;target type='serial' port='0'/&gt;
    &lt;/console&gt;
    &lt;input type='mouse' bus='ps2'/&gt;
    &lt;graphics type='vnc' port='5904' autoport='no'/&gt;
    &lt;video&gt;
      &lt;model type='cirrus' vram='9216' heads='1'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/&gt;
    &lt;/video&gt;
    &lt;memballoon model='virtio'&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/&gt;
    &lt;/memballoon&gt;
  &lt;/devices&gt;
&lt;/domain&gt;
</pre></p>


	<p>編集後。ポイントは以下の通り。<br />・わかる範囲でハード情報（メモリ、CPUコア等）はvmware時代と合わせる。<br />・address type関連の記述はすべて削除する。mac address等も。<br /><pre>
&lt;domain type='kvm'&gt;
  &lt;name&gt;ricoh_db&lt;/name&gt;　　　　　　　　　　　　　　　　　　　★名前を変更。
　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★削除
  &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;　　　　　　　　　　　　★メモリ変更
  &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;　　　　　★メモリ変更
  &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;　　　　　　　　　　　　　★CPUコア変更
  &lt;os&gt;
    &lt;type arch='x86_64' machine='rhel6.6.0'&gt;hvm&lt;/type&gt;　　　　★OSタイプ変更
    &lt;boot dev='hd'/&gt;
  &lt;/os&gt;
  &lt;features&gt;
    &lt;acpi/&gt;
    &lt;apic/&gt;
    &lt;pae/&gt;
  &lt;/features&gt;
  &lt;clock offset='utc'/&gt;
  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
  &lt;on_crash&gt;restart&lt;/on_crash&gt;
  &lt;devices&gt;
    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;　　　　　　　　　　　　　　　★disk type=fileに変更
      &lt;driver name='qemu' type='raw' cache='none' io='native'/&gt;
      &lt;source file='/var/iso/CentOS.img'/&gt;　　　　　　　　　　　　★.imgファイルパスを指定
      &lt;target dev='hda' bus='ide'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/disk&gt;
    &lt;disk type='block' device='cdrom'&gt;
      &lt;driver name='qemu' type='raw'/&gt;
      &lt;target dev='hdc' bus='ide'/&gt;
      &lt;readonly/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/disk&gt;
    &lt;controller type='usb' index='0' model='ich9-ehci1'&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci1'&gt;
      &lt;master startport='0'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci2'&gt;
      &lt;master startport='2'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci3'&gt;
      &lt;master startport='4'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/controller&gt;
    &lt;controller type='ide' index='0'&gt;
     　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/controller&gt;
    &lt;interface type='bridge'&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★mac address削除
      &lt;source bridge='br0'/&gt;　　　　　　　　　　　　　　　　　　　★使用するインターフェースブリッジを正しいものへ変更。複数のインターフェースを使用する場合は項目をコピーして増やす。
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/interface&gt;
    &lt;serial type='pty'&gt;
      &lt;target port='0'/&gt;
    &lt;/serial&gt;
    &lt;console type='pty'&gt;
      &lt;target type='serial' port='0'/&gt;
    &lt;/console&gt;
    &lt;input type='mouse' bus='ps2'/&gt;
    &lt;graphics type='vnc' port='5920' autoport='no'/&gt;　　　　　　　★VNCポート番号が他の仮想マシンとかぶらないように変更
    &lt;video&gt;
      &lt;model type='cirrus' vram='9216' heads='1'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/video&gt;
    &lt;memballoon model='virtio'&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/memballoon&gt;
  &lt;/devices&gt;
&lt;/domain&gt;
</pre></p>


	<p>定義ファイルを登録。<br /><pre>
virsh define /etc/libvirt/qemu/ricoh_db.xml
</pre></p>


	<p>仮想マシンスタート。<br /><pre>
virsh start ricoh_db
</pre></p>


	<p>VNC接続で起動しているか確認する。</p>


	<a name="Vmware形式の仮想マシンをkvmへ移行_imgファイルをLVMボリューム領域へ移設"></a>
<h3 >.imgファイルをLVMボリューム領域へ移設<a href="#Vmware形式の仮想マシンをkvmへ移行_imgファイルをLVMボリューム領域へ移設" class="wiki-anchor">&para;</a></h3>


	<p>lvmボリューム領域を作成<br /><pre>
lvcreate -L &lt;ディスクサイズ(ギガ)&gt; &lt;VGパス&gt; -n ボリューム名
</pre></p>


	<p>ddコマンドでimgをボリューム領域へコピー。<br /><pre>
dd if=&lt;.imgファイルパス&gt; of=&lt;lvmボリュームパス&gt; 
</pre></p>


	<p>仮想マシン定義ファイルをいったん削除。削除前にコピーしておく。<br /><pre>
cp -p ricoh_db.xml /var/iso/ricoh_db.xml
</pre><br /><pre>
virsh undefine ricoh_db
</pre></p>


	<p>コピーしておいた定義ファイルを編集。<br /><pre>
編集前
domain type='kvm'&gt;
  &lt;name&gt;ricoh_db&lt;/name&gt;
  &lt;uuid&gt;2fabc6ae-cb45-7a2b-d925-58a2c99cc2a8&lt;/uuid&gt;
  &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;
  &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;
  &lt;os&gt;
    &lt;type arch='i686' machine='rhel6.6.0'&gt;hvm&lt;/type&gt;
    &lt;boot dev='hd'/&gt;
  &lt;/os&gt;
  &lt;features&gt;
    &lt;acpi/&gt;
    &lt;apic/&gt;
    &lt;pae/&gt;
  &lt;/features&gt;
  &lt;clock offset='utc'/&gt;
  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
  &lt;on_crash&gt;restart&lt;/on_crash&gt;
  &lt;devices&gt;
    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='raw' cache='none' io='native'/&gt;
      &lt;source file='/var/iso/CentOS.img'/&gt;
      &lt;target dev='hda' bus='ide'/&gt;
      &lt;address type='drive' controller='0' bus='0' target='0' unit='0'/&gt;
    &lt;/disk&gt;
    &lt;disk type='block' device='cdrom'&gt;
      &lt;driver name='qemu' type='raw'/&gt;
      &lt;target dev='hdc' bus='ide'/&gt;
      &lt;readonly/&gt;
      &lt;address type='drive' controller='0' bus='1' target='0' unit='0'/&gt;
    &lt;/disk&gt;
    &lt;controller type='usb' index='0' model='ich9-ehci1'&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x7'/&gt;
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci1'&gt;
      &lt;master startport='0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0' multifunction='on'/&gt;
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci2'&gt;
      &lt;master startport='2'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x1'/&gt;
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci3'&gt;
      &lt;master startport='4'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x2'/&gt;
    &lt;/controller&gt;
    &lt;controller type='ide' index='0'&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/&gt;
    &lt;/controller&gt;
    &lt;interface type='bridge'&gt;
      &lt;mac address='52:54:00:f2:6d:40'/&gt;
      &lt;source bridge='br0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&gt;
    &lt;/interface&gt;
    &lt;serial type='pty'&gt;
      &lt;target port='0'/&gt;
    &lt;/serial&gt;
    &lt;console type='pty'&gt;
      &lt;target type='serial' port='0'/&gt;
    &lt;/console&gt;
    &lt;input type='mouse' bus='ps2'/&gt;
    &lt;graphics type='vnc' port='5920' autoport='no'/&gt;
    &lt;video&gt;
      &lt;model type='cirrus' vram='9216' heads='1'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/&gt;
    &lt;/video&gt;
    &lt;memballoon model='virtio'&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/&gt;
    &lt;/memballoon&gt;
  &lt;/devices&gt;
&lt;/domain&gt;
</pre><br /><pre>
編集後
&lt;domain type='kvm'&gt;
  &lt;name&gt;ricoh_db&lt;/name&gt;
                                                    ★UUID削除
  &lt;memory unit='KiB'&gt;8388608&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;8388608&lt;/currentMemory&gt;
  &lt;vcpu placement='static'&gt;2&lt;/vcpu&gt;
  &lt;os&gt;
    &lt;type arch='i686' machine='rhel6.6.0'&gt;hvm&lt;/type&gt;
    &lt;boot dev='hd'/&gt;
  &lt;/os&gt;
  &lt;features&gt;
    &lt;acpi/&gt;
    &lt;apic/&gt;
    &lt;pae/&gt;
  &lt;/features&gt;
  &lt;clock offset='utc'/&gt;
  &lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
  &lt;on_reboot&gt;restart&lt;/on_reboot&gt;
  &lt;on_crash&gt;restart&lt;/on_crash&gt;
  &lt;devices&gt;
    &lt;emulator&gt;/usr/libexec/qemu-kvm&lt;/emulator&gt;
    &lt;disk type='block' device='disk'&gt;                                        ★disk type=blockに変更
      &lt;driver name='qemu' type='raw' cache='none' io='native'/&gt;
      &lt;source dev='/dev/vg_vm/lvol6'/&gt;　　　　　　　　　　　　　　　　　　　　★source fileをsource devに変更、lvmボリュームを指定
      &lt;target dev='hda' bus='ide'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/disk&gt;
    &lt;disk type='block' device='cdrom'&gt;
      &lt;driver name='qemu' type='raw'/&gt;
      &lt;target dev='hdc' bus='ide'/&gt;
      &lt;readonly/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/disk&gt;
    &lt;controller type='usb' index='0' model='ich9-ehci1'&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci1'&gt;
      &lt;master startport='0'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci2'&gt;
      &lt;master startport='2'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/controller&gt;
    &lt;controller type='usb' index='0' model='ich9-uhci3'&gt;
      &lt;master startport='4'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/controller&gt;
    &lt;controller type='ide' index='0'&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/controller&gt;
    &lt;interface type='bridge'&gt;
      &lt;mac address='52:54:00:00:81:25'/&gt;
      &lt;source bridge='br0'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/interface&gt;
    &lt;serial type='pty'&gt;
      &lt;target port='0'/&gt;
    &lt;/serial&gt;
    &lt;console type='pty'&gt;
      &lt;target type='serial' port='0'/&gt;
    &lt;/console&gt;
    &lt;input type='mouse' bus='ps2'/&gt;
    &lt;graphics type='vnc' port='5920' autoport='no'/&gt;
    &lt;video&gt;
      &lt;model type='cirrus' vram='9216' heads='1'/&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/video&gt;
    &lt;memballoon model='virtio'&gt;
      　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　★address type削除
    &lt;/memballoon&gt;
  &lt;/devices&gt;
&lt;/domain&gt;
</pre></p>


	<p>kvmに再度登録。<br /><pre>
virsh define ricoh_db.xml
</pre></p>


	<p>仮想マシン起動。正常に動作するか確認。最後にネットワークインターフェース周りの設定をすればOK。</p>
<hr />
<a name="VyOs" />
<a name="VyOs_VyOs"></a>
<h1 >VyOs<a href="#VyOs_VyOs" class="wiki-anchor">&para;</a></h1>


<hr />


	<a name="VyOs_パーティションをオフセットしてマウント"></a>
<h2 >パーティションをオフセットしてマウント<a href="#VyOs_パーティションをオフセットしてマウント" class="wiki-anchor">&para;</a></h2>


	<p>vyOsではFSにoverlayfsというのを利用している。<br />ext4をラップしているような仕組みだが、<br />このlvm領域をマウントする際には少しコツがいる。<br />デバイスファイルの始めのセクタからマウントしようとするとできない。<br />そのためext4が始まるセクタを探し、そこからマウントしようとする必要がある。</p>


	<p>以下に例を示す。</p>


<pre>
[root@kvm-01 ~]# parted /dev/ISCSIVolGroup00/vrtest unit b print
モデル: Linux device-mapper (linear) (dm)
ディスク /dev/dm-5: 2147483648B
セクタサイズ (論理/物理): 512B/512B
パーティションテーブル: msdos

番号  開始      終了         サイズ       タイプ   ファイルシステム  フラグ
 1    1048576B  2147483647B  2146435072B  primary  ext4              boot

</pre>

	<p>開始の位置を確認後、以下のコマンドを発行する。</p>


<pre>
mount -o loop,offset=1048576 -t ext4 /dev/ISCSIVolGroup00/vrtest vyos/
</pre>

<hr />


	<p>以下に設定自動更新の方法をメモが有ります。</p>


	<p><a href="/issues/2681" class="issue tracker-3 status-5 priority-4 priority-default closed created-by-me" title="vyOsへ動的に情報を更新できない (終了)">#2681</a></p>
<hr />
<a name="Vyosインストール" />
<a name="Vyosインストール_Vyosインストールと基本設定"></a>
<h1 >Vyosインストールと基本設定<a href="#Vyosインストール_Vyosインストールと基本設定" class="wiki-anchor">&para;</a></h1>


	<a name="Vyosインストール_vyosインストール"></a>
<h2 >vyosインストール<a href="#Vyosインストール_vyosインストール" class="wiki-anchor">&para;</a></h2>


	<p>KVM上にvyosをインストールする場合、通常の仮想マシンと同様に、virt-installコマンドで仮想マシン作成/インストールを行う。</p>


	<p>仮想マシン作成<br /><pre>
&gt;virt install \
--name ricoh_vrf \
--vcpus 2 \
--ram 1024 \
--disk path=/dev/vg_vm/lvol9 \
--cdrom /var/lib/libvirt/images/vyos-1.1.3-amd64.iso \
--network bridge=br0 \
--network bridge=br1 \
--vnc --vncport=5923 \
--keymap=ja \
--connect qemu:///system
</pre></p>


	<p>vnc接続でvyosをインストール。以下のユーザガイドを参考にインストール。そのまま真似すればいける。</p>


	<p><a class="external" href="http://wiki.vyos-users.jp/%E3%83%A6%E3%83%BC%E3%82%B6%E3%83%BC%E3%82%AC%E3%82%A4%E3%83%89">http://wiki.vyos-users.jp/%E3%83%A6%E3%83%BC%E3%82%B6%E3%83%BC%E3%82%AC%E3%82%A4%E3%83%89</a></p>


	<a name="Vyosインストール_ネットワークインターフェース設定"></a>
<h3 >ネットワークインターフェース設定<a href="#Vyosインストール_ネットワークインターフェース設定" class="wiki-anchor">&para;</a></h3>


	<p>最も基本的な構成はWAN側/LAN側の２つのインターフェース構成となる。以下、インターフェースの設定。<br /><pre>
#set interface  ethernet &lt;ethernet-number&gt; address &lt;**.**.**.**/**&gt;

#set interfaces ethernet &lt;ethernet-number&gt; description &lt;description&gt;
</pre></p>


	<p>カスタムネットワークを追加したい場合は、ネットワークの数だけインターフェースがあることが望ましい。<br />ネットワークインターフェースを追加したい場合は、KVM側で以下の通り仮想マシン定義ファイルを編集する。<br />なお、追加の際は仮想マシン側、仮想ルータ側双方のインターフェースを追加すること。<br /><pre>
virsh edit 仮想マシン名
</pre><br />編集前<br /><pre>
    &lt;interface type='bridge'&gt;
      &lt;mac address='52:54:00:33:73:b1'/&gt;
      &lt;source bridge='br0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&gt;
    &lt;/interface&gt;
    &lt;interface type='bridge'&gt;
      &lt;mac address='52:54:00:8b:09:d9'/&gt;
      &lt;source bridge='br1'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/&gt;
    &lt;/interface&gt;
</pre><br />編集後<br /><pre>
    &lt;interface type='bridge'&gt;
      &lt;mac address='52:54:00:33:73:b1'/&gt;
      &lt;source bridge='br0'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/&gt;
    &lt;/interface&gt;　　　　　　　　★インターフェース追加
    &lt;interface type='bridge'&gt; 　★　
      &lt;source bridge='br0'/&gt;　　★LAN側とつながっているブリッジインターフェースを指定。つまり、br0は複数のネットワークが流れることになる。
    &lt;/interface&gt;　　　　　　　　★
    &lt;interface type='bridge'&gt;
      &lt;mac address='52:54:00:8b:09:d9'/&gt;
      &lt;source bridge='br1'/&gt;
      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/&gt;
    &lt;/interface&gt;
</pre></p>


	<p>仮想マシン再起動後、ネットワークインターフェースが追加されていることを確認する。</p>


	<a name="Vyosインストール_NAT設定"></a>
<h3 >NAT設定<a href="#Vyosインストール_NAT設定" class="wiki-anchor">&para;</a></h3>


	<p>LAN→WANのNAT設定。<br /><pre>
#set nat source rule &lt;rule-number&gt; outbound-interface '&lt;ethernet-number&gt;'

#set nat source rule &lt;rule-number&gt; source address '&lt;**.**.**.**/24&gt;

#set nat source rule &lt;rule-number&gt; translation address masquerade
</pre></p>


	<a name="Vyosインストール_DNSフォワーディング設定"></a>
<h3 >DNSフォワーディング設定<a href="#Vyosインストール_DNSフォワーディング設定" class="wiki-anchor">&para;</a></h3>


<pre>
#set service dns forwarding cache-size '0'

#set service dns forwarding listen-on '&lt;ethernet-number&gt;'

#set service dns forwarding name-server &lt;nameserver-address&gt;
</pre>
<hr />
<a name="Wiki" />
<a name="Wiki_Wiki"></a>
<h1 >Wiki<a href="#Wiki_Wiki" class="wiki-anchor">&para;</a></h1>


	<ul class="toc right"><li><a href="#Wiki_Wiki">Wiki</a><ul><li><a href="#Wiki_監視の画面が表示できない場合のvpnサーバについて-参照-2692">監視の画面が表示できない場合のvpnサーバについて 参照 #2692</a></li><li><a href="#Wiki_議事録">議事録</a></li><li><a href="#Wiki_TKVMについて">TKVMについて</a></li><li><a href="#Wiki_TMMC用試験環境">TMMC用試験環境</a></li><li><a href="#Wiki_東新宿IPアドレス">東新宿IPアドレス</a></li><li><a href="#Wiki_QNAP">QNAP</a></li><li><a href="#Wiki_OSミドルウェアライブラリとインストール方法や技術内容等">OS,ミドルウェア,ライブラリとインストール方法や技術内容等</a></li><li><a href="#Wiki_共通設計">共通設計</a></li><li><a href="#Wiki_仮想ルータ設計">仮想ルータ設計</a></li><li><a href="#Wiki_仮想インスタンス運用手順">仮想インスタンス運用手順</a></li><li><a href="#Wiki_NW設計">NW設計</a></li><li><a href="#Wiki_KVMホスト運用手順">KVMホスト運用手順</a></li><li><a href="#Wiki_KVM設計">KVM設計</a></li><li><a href="#Wiki_KVMクラスタ設計">KVMクラスタ設計</a></li><li><a href="#Wiki_CA設計">CA設計</a></li><li><a href="#Wiki_DB設計">DB設計</a></li><li><a href="#Wiki_ログサーバ設計">ログサーバ設計</a></li><li><a href="#Wiki_clocon設計">clocon設計</a></li><li><a href="#Wiki_監視設計">監視設計</a></li><li><a href="#Wiki_監視プロキシ設計">監視プロキシ設計</a></li><li><a href="#Wiki_共通監視設計">共通監視設計</a></li><li><a href="#Wiki_Dns設計">Dns設計</a></li><li><a href="#Wiki_kickstart設計-dhcp-tftp-リポジトリサーバ周り">kickstart設計 (dhcp, tftp, リポジトリサーバ周り)</a></li><li><a href="#Wiki_ansible設計">ansible設計</a></li><li><a href="#Wiki_serverspec設計">serverspec設計</a></li><li><a href="#Wiki_HTTPProxy設計">HTTPProxy設計</a></li><li><a href="#Wiki_iSCSI設計">iSCSI設計</a></li><li><a href="#Wiki_Mail設計">Mail設計</a></li><li><a href="#Wiki_NTP設計">NTP設計</a></li><li><a href="#Wiki_NAS設計">NAS設計</a></li><li><a href="#Wiki_hadoop設計">hadoop設計</a></li><li><a href="#Wiki_ノウハウまとめ">ノウハウまとめ</a></li><li><a href="#Wiki_NW機器サポート問い合わせ先">NW機器サポート問い合わせ先</a></li><li><a href="#Wiki_笑い男ロゴ">笑い男ロゴ</a></li></ul></li></ul>


	<p><img src="/attachments/download/934/warai_flat.png" alt="" /></p>


	<a name="Wiki_監視の画面が表示できない場合のvpnサーバについて-参照-2692"></a>
<h2 >監視の画面が表示できない場合のvpnサーバについて 参照 <a href="/issues/2692" class="issue tracker-3 status-1 priority-4 priority-default" title="ポートフォワードを使用せずに、コンダクト用のZabbixが見れるようにする (新規)">#2692</a><a href="#Wiki_監視の画面が表示できない場合のvpnサーバについて-参照-2692" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_議事録"></a>
<h2 ><a href="#議事録" class="wiki-page">議事録</a><a href="#Wiki_議事録" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_TKVMについて"></a>
<h2 ><a href="#TKVMについて" class="wiki-page">TKVMについて</a><a href="#Wiki_TKVMについて" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_TMMC用試験環境"></a>
<h2 ><a href="#TMMC用試験環境" class="wiki-page">TMMC用試験環境</a><a href="#Wiki_TMMC用試験環境" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_東新宿IPアドレス"></a>
<h2 >東新宿IPアドレス<a href="#Wiki_東新宿IPアドレス" class="wiki-anchor">&para;</a></h2>


<pre>
ip_1:   172.31.208.98
ip_2:   172.31.208.99
mask:   255.255.255.0
router: 172.31.208.254
dns:    163.139.230.168
</pre>

<hr />


	<a name="Wiki_QNAP"></a>
<h2 >QNAP<a href="#Wiki_QNAP" class="wiki-anchor">&para;</a></h2>


	<p>IP：192.168.100.212<br />ユーザ名：admin<br />パス：admin</p>


<hr />


	<a name="Wiki_OSミドルウェアライブラリとインストール方法や技術内容等"></a>
<h2 >OS,ミドルウェア,ライブラリとインストール方法や技術内容等<a href="#Wiki_OSミドルウェアライブラリとインストール方法や技術内容等" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>名前</th>
			<th>バージョン</th>
			<th>ライセンス</th>
		</tr>
		<tr>
			<td><a href="#Centos" class="wiki-page">centos</a></td>
			<td>6系の最新 2014/11/13現在は6.6</td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#VyOs" class="wiki-page">vyOs</a></td>
			<td>1.1.0</td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Libvirt" class="wiki-page">libvirt</a></td>
			<td>0.10.2</td>
			<td>LGPL</td>
		</tr>
		<tr>
			<td><a href="https://redmine.m2m.iiss-isb.com/projects/damplr/wiki/Postgresql" title="m2mweb redmine wikiへのリンク ※権限がないとみれません" class="external">postgresql</a></td>
			<td></td>
			<td>BSD</td>
		</tr>
		<tr>
			<td><a href="#Bind" class="wiki-page">bind</a> <a href="#クラコン構築メモ" class="wiki-page">クラコン構築メモ</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Linux_ha_pacemaker" class="wiki-page">linux ha pacemaker</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Drbd" class="wiki-page">drbd</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Kickstart" class="wiki-page">kickstart</a> ※dhcpの記載有</td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Mrepo" class="wiki-page">mrepo</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Postfix" class="wiki-page">postfix</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Ansible" class="wiki-page">ansible</a></td>
			<td>ansible 1.8.2</td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Serverspec" class="wiki-page">serverspec</a></td>
			<td>serverspec-2.8.2</td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Zabbix" class="wiki-page">zabbix</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Firewall" class="wiki-page">firewall</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Squid" class="wiki-page">squid</a></td>
			<td>Version 3.1.10</td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Ntpd" class="wiki-page">ntpd</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Nginx" class="wiki-page">nginx</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Fio" class="wiki-page">fio</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Keepalivedとlvs" class="wiki-page">keepalivedとlvs</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Openvpn" class="wiki-page">openvpn</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#バックアップ方法まとめ" class="wiki-page">バックアップ方法まとめ</a></td>
			<td></td>
			<td></td>
		</tr>
		<tr>
			<td><a href="#Nodejs" class="wiki-page">node.js</a></td>
			<td></td>
			<td></td>
		</tr>
	</table>




	<p>※dual licensed:どちらかのライセンスを選べる</p>


<hr />


	<a name="Wiki_共通設計"></a>
<h2 ><a href="#共通設計" class="wiki-page">共通設計</a><a href="#Wiki_共通設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_仮想ルータ設計"></a>
<h2 ><a href="#仮想ルータ設計" class="wiki-page">仮想ルータ設計</a><a href="#Wiki_仮想ルータ設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_仮想インスタンス運用手順"></a>
<h2 ><a href="#仮想インスタンス運用手順" class="wiki-page">仮想インスタンス運用手順</a><a href="#Wiki_仮想インスタンス運用手順" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_NW設計"></a>
<h2 ><a href="#NW設計" class="wiki-page">NW設計</a><a href="#Wiki_NW設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_KVMホスト運用手順"></a>
<h2 ><a href="#KVMホスト運用手順" class="wiki-page">KVMホスト運用手順</a><a href="#Wiki_KVMホスト運用手順" class="wiki-anchor">&para;</a></h2>


	<a name="Wiki_KVM設計"></a>
<h2 ><a href="#KVM設計" class="wiki-page">KVM設計</a><a href="#Wiki_KVM設計" class="wiki-anchor">&para;</a></h2>


	<a name="Wiki_KVMクラスタ設計"></a>
<h2 ><a href="#KVMクラスタ設計" class="wiki-page">KVMクラスタ設計</a><a href="#Wiki_KVMクラスタ設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_CA設計"></a>
<h2 ><a href="#CA設計" class="wiki-page">CA設計</a><a href="#Wiki_CA設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_DB設計"></a>
<h2 ><a href="#DB設計" class="wiki-page">DB設計</a><a href="#Wiki_DB設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_ログサーバ設計"></a>
<h2 ><a href="#ログサーバ設計" class="wiki-page">ログサーバ設計</a><a href="#Wiki_ログサーバ設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_clocon設計"></a>
<h2 ><a href="#Clocon設計" class="wiki-page">clocon設計</a><a href="#Wiki_clocon設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_監視設計"></a>
<h2 ><a href="#監視設計" class="wiki-page">監視設計</a><a href="#Wiki_監視設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_監視プロキシ設計"></a>
<h2 ><a href="#監視プロキシ設計" class="wiki-page">監視プロキシ設計</a><a href="#Wiki_監視プロキシ設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_共通監視設計"></a>
<h2 ><a href="#共通監視設計" class="wiki-page">共通監視設計</a><a href="#Wiki_共通監視設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_Dns設計"></a>
<h2 ><a href="#Dns設計" class="wiki-page">Dns設計</a><a href="#Wiki_Dns設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_kickstart設計-dhcp-tftp-リポジトリサーバ周り"></a>
<h2 ><a href="#Kickstart設計" class="wiki-page">kickstart設計</a> (dhcp, tftp, リポジトリサーバ周り)<a href="#Wiki_kickstart設計-dhcp-tftp-リポジトリサーバ周り" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_ansible設計"></a>
<h2 ><a href="#Ansible設計" class="wiki-page">ansible設計</a><a href="#Wiki_ansible設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_serverspec設計"></a>
<h2 ><a href="#Serverspec設計" class="wiki-page">serverspec設計</a><a href="#Wiki_serverspec設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_HTTPProxy設計"></a>
<h2 ><a href="#HTTPProxy設計" class="wiki-page">HTTPProxy設計</a><a href="#Wiki_HTTPProxy設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_iSCSI設計"></a>
<h2 ><a href="#ISCSI設計" class="wiki-page">iSCSI設計</a><a href="#Wiki_iSCSI設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_Mail設計"></a>
<h2 ><a href="#Mail設計" class="wiki-page">Mail設計</a><a href="#Wiki_Mail設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_NTP設計"></a>
<h2 ><a href="#NTP設計" class="wiki-page">NTP設計</a><a href="#Wiki_NTP設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_NAS設計"></a>
<h2 ><a href="#NAS設計" class="wiki-page">NAS設計</a><a href="#Wiki_NAS設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_hadoop設計"></a>
<h2 ><a href="#Hadoop設計" class="wiki-page">hadoop設計</a><a href="#Wiki_hadoop設計" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_ノウハウまとめ"></a>
<h2 ><a href="#ノウハウまとめ" class="wiki-page">ノウハウまとめ</a><a href="#Wiki_ノウハウまとめ" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_NW機器サポート問い合わせ先"></a>
<h2 ><a href="#NW機器サポート問い合わせ先" class="wiki-page">NW機器サポート問い合わせ先</a><a href="#Wiki_NW機器サポート問い合わせ先" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="Wiki_笑い男ロゴ"></a>
<h2 ><a href="#笑い男ロゴ" class="wiki-page">笑い男ロゴ</a><a href="#Wiki_笑い男ロゴ" class="wiki-anchor">&para;</a></h2>


<hr />
<hr />
<a name="Zabbix" />
<a name="Zabbix_Zabbix"></a>
<h1 >Zabbix<a href="#Zabbix_Zabbix" class="wiki-anchor">&para;</a></h1>


<hr />


	<a name="Zabbix_lvm領域devvg_datalv_postgresの作成"></a>
<h3 >lvm領域(/dev/vg_data/lv_postgres)の作成<a href="#Zabbix_lvm領域devvg_datalv_postgresの作成" class="wiki-anchor">&para;</a></h3>


<pre>
[root@idc-omon01 ~]# pvcreate /dev/sdb1 
  Physical volume "/dev/sdb1" successfully created
[root@idc-omon01 ~]# vgcreate vg_data /dev/sdb1 
  Volume group "vg_data" successfully created
[root@idc-omon01 ~]# pvscan
  PV /dev/sdb1   VG vg_data     lvm2 [1.80 TiB / 1.80 TiB free]
  PV /dev/sda5   VG vg_system   lvm2 [276.34 GiB / 164.34 GiB free]
  Total: 2 [2.07 TiB] / in use: 2 [2.07 TiB] / in no VG: 0 [0   ]
[root@idc-omon01 ~]# lvcreate vg_data -L 1T -n lv_postgres
  Logical volume "lv_postgres" created
[root@idc-omon01 ~]# lvscan 
  ACTIVE            '/dev/vg_data/lv_postgres' [1.00 TiB] inherit
  ACTIVE            '/dev/vg_system/lv_swap' [12.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_root' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_home' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_tmp' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_usr' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_var' [10.00 GiB] inherit
  ACTIVE            '/dev/vg_system/lv_var_log' [50.00 GiB] inherit
</pre>

<hr />


	<a name="Zabbix_drbdの準備"></a>
<h3 >drbdの準備<a href="#Zabbix_drbdの準備" class="wiki-anchor">&para;</a></h3>


	<p>以下の手順を踏むことでdrbdディスクファイルを作成し、ext4まで作成する。<br />FSの作成はprimaryノードのみで行う(secondaryでは書き込めない)。</p>


<pre>
# yum install http://10.16.33.82:8888/additional_package/drbd84-utils-8.9.1-1.el6.elrepo.x86_64.rpm
# yum install http://10.16.33.82:8888/additional_package/kmod-drbd84-8.4.5-2.el6.elrepo.x86_64.rpm
# egrep -v "(^$|^#)" /etc/drbd.d/global_common.conf | egrep -v "[\t]?[#]" 
global {
        usage-count no;
}
common {
        protocol C;
        handlers {
        }
        startup {
                wfc-timeout 10;
                degr-wfc-timeout 10;
                outdated-wfc-timeout 10;
        }
        options {
        }
        disk {
        }
        net {
        }
}
# cat postgres.res
resource postgres {
  device /dev/drbd0;
  disk   /dev/vg_data/lv_postgres;
  meta-disk internal;
  on idc-omon01 {
    address 10.16.32.146:7788;
  }
  on idc-omon02 {
    address 10.16.32.147:7788;
  }
}
# /etc/init.d/drbd start
# drbdadm create-md postgres
# drbdadm attach postgres
# drbdadm -- --overwrite-data-of-peer primary postgres
# /etc/init.d/drbd status
drbd driver loaded OK; device status:
version: 8.4.5 (api:1/proto:86-101)
GIT-hash: 1d360bde0e095d495786eaeb2a1ac76888e4db96 build by phil@Build64R6, 2014-10-28 10:32:53
m:res       cs          ro                 ds                     p  mounted  fstype
0:postgres  SyncSource  Primary/Secondary  UpToDate/Inconsistent  C
...         sync'ed:    0.1%               (1048192/1048540)M
# drbd-overview # /etc/init.d/drbd statusと内容はほぼ同一
 0:postgres/0  SyncSource Primary/Secondary UpToDate/Inconsistent
        [&gt;....................] sync'ed:  0.4% (1044636/1048540)M
# drbdadm role postgres
Primary/Secondary
# mkfs.ext4 /dev/drbd0 # primaryノードのみ
### 同期が完了し両者のdiskがUpToDateまでは起動しておく
# drbdadm dstate postgres
UpToDate/UpToDate
# chkconfig drbd off # pacemaker管理であるため自動起動をoffに
# chkconfig --list drbd
drbd            0:off   1:off   2:off   3:off   4:off   5:off   6:off
### 最後にdrbdを一旦終了する。slave,primaryの順番で停止すること。
# /etc/init.d/drbd stop
</pre>

<hr />


	<a name="Zabbix_postgresの準備"></a>
<h3 >postgresの準備<a href="#Zabbix_postgresの準備" class="wiki-anchor">&para;</a></h3>


	<p>drbdの準備で作成したデバイスファイルをマウントした状態でinitdbを実行する。</p>


<pre>
# yum install http://10.16.33.82:8888/additional_package/postgresql94-libs-9.4.1-1PGDG.rhel6.x86_64.rpm
# yum install http://10.16.33.82:8888/additional_package/postgresql94-9.4.1-1PGDG.rhel6.x86_64.rpm
# yum install http://10.16.33.82:8888/additional_package/postgresql94-server-9.4.1-1PGDG.rhel6.x86_64.rpm
# yum install http://10.16.33.82:8888/additional_package/postgresql94-contrib-9.4.1-1PGDG.rhel6.x86_64.rpm
# ll /var/lib/pgsql/9.4/data/
合計 0
# mount /dev/drbd0 /var/lib/pgsql/9.4/data/
# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/mapper/vg_system-lv_root
                      9.8G  390M  8.9G   5% /
tmpfs                  16G     0   16G   0% /dev/shm
/dev/sda3             477M   59M  393M  13% /boot
/dev/mapper/vg_system-lv_home
                      9.8G   23M  9.2G   1% /home
/dev/mapper/vg_system-lv_tmp
                      9.8G   23M  9.2G   1% /tmp
/dev/mapper/vg_system-lv_usr
                      9.8G  1.1G  8.2G  12% /usr
/dev/mapper/vg_system-lv_var
                      9.8G  146M  9.1G   2% /var
/dev/mapper/vg_system-lv_var_log
                       50G   64M   47G   1% /var/log
/dev/drbd0           1008G   72M  957G   1% /var/lib/pgsql/9.4/data
# chown postgres. /var/lib/pgsql/9.4/data/
# rmdir /var/lib/pgsql/9.4/data/*
# /etc/init.d/postgresql-9.4 initdb
# pwd
/var/lib/pgsql/9.4/data
# sed "s/\t/  /g" /var/lib/pgsql/9.4/data/postgresql.conf | egrep -v "(^$|^#|^ *#)" | sed "s/ *#.*//g" 
max_connections = 100
shared_buffers = 8192MB
dynamic_shared_memory_type = posix
log_destination = 'stderr'
logging_collector = on
log_directory = '/var/log/pgsql'
log_filename = 'postgresql-%Y%m%d.log'
log_truncate_on_rotation = on
log_rotation_age = 1d
log_rotation_size = 0
log_line_prefix = '[%m][%p][%u][%d] '
log_timezone = 'Japan'
datestyle = 'iso, ymd'
timezone = 'Japan'
lc_messages = 'ja_JP.UTF-8'
lc_monetary = 'ja_JP.UTF-8'
lc_numeric = 'ja_JP.UTF-8'
lc_time = 'ja_JP.UTF-8'
default_text_search_config = 'pg_catalog.simple'
# egrep -v "(^$|^#)" pg_hba.conf
local   all             all                                     trust
host    all             all             127.0.0.1/32            trust
host    all             all             ::1/128                 trust
# chkconfig --list postgresql-9.4
postgresql-9.4  0:off   1:off   2:off   3:off   4:off   5:off   6:off
# mkdir /var/log/pgsql
# chown postgres. /var/log/pgsql
# /etc/init.d/postgresql-9.4 start
postgresql-9.4 サービスを開始中:                           [  OK  ]
# /etc/init.d/postgresql-9.4 stop
postgresql-9.4 サービスを停止中:                           [  OK  ]
</pre>

	<p>zabbix用のdatabaseおよびユーザ等を作成していく。</p>


<pre>
# psql postgres postgres
psql (9.4.1)
"help" でヘルプを表示します.
postgres=# create user zabbix encrypted PASSWORD 'zabbixzabbix';
CREATE ROLE
postgres=# create database zabbix owner zabbix ;
CREATE DATABASE
postgres=# \du
                                         ロール一覧
 ロール名 |                                 属性                                 | メンバー
----------+----------------------------------------------------------------------+----------
 postgres | スーパーユーザ, ロールを作成できる, DBを作成できる, レプリケーション | {}
 zabbix   |                                                                      | {}

postgres=# \l
                                         データベース一覧
   名前    |  所有者  | エンコーディング |  照合順序   | Ctype(変換演算子) |      アクセス権
-----------+----------+------------------+-------------+-------------------+-----------------------
 postgres  | postgres | UTF8             | ja_JP.UTF-8 | ja_JP.UTF-8       |
 template0 | postgres | UTF8             | ja_JP.UTF-8 | ja_JP.UTF-8       | =c/postgres          +
           |          |                  |             |                   | postgres=CTc/postgres
 template1 | postgres | UTF8             | ja_JP.UTF-8 | ja_JP.UTF-8       | =c/postgres          +
           |          |                  |             |                   | postgres=CTc/postgres
 zabbix    | zabbix   | UTF8             | ja_JP.UTF-8 | ja_JP.UTF-8       |
(4 行)
postgres=# \q
</pre>

<hr />


	<a name="Zabbix_zabbixの準備"></a>
<h3 >zabbixの準備<a href="#Zabbix_zabbixの準備" class="wiki-anchor">&para;</a></h3>


<pre>
# yum install libtool-ltdl.x86_64 lm_sensors-libs.x86_64 net-snmp-libs.x86_64 OpenIPMI-libs.x86_64 net-snmp.x86_64 unixODBC.x86_64
# yum install http://10.16.33.82:8888/additional_package/iksemel-1.4-2.el6.x86_64.rpm http://10.16.33.82:8888/additional_package/zabbix-2.4.3-1.el6.x86_64.rpm http://10.16.33.82:8888/additional_package/fping-2.4b2-16.el6.x86_64.rpm http://10.16.33.82:8888/additional_package/zabbix-server-2.4.3-1.el6.x86_64.rpm http://10.16.33.82:8888/additional_package/zabbix-server-pgsql-2.4.3-1.el6.x86_64.rpm http://10.16.33.82:8888/additional_package/zabbix-web-pgsql-2.4.3-1.el6.noarch.rpm http://10.16.33.82:8888/additional_package/zabbix-web-2.4.3-1.el6.noarch.rpm http://10.16.33.82:8888/additional_package/zabbix-web-japanese-2.4.3-1.el6.noarch.rpm
# cd /usr/share/doc/zabbix-server-pgsql-2.4.3/create
# psql zabbix zabbix &lt; schema.sql
# psql zabbix zabbix &lt; images.sql
# psql zabbix zabbix &lt; data.sql
# egrep -v "(^$|^#)" /etc/zabbix/zabbix_server.conf
LogFile=/var/log/zabbix/zabbix_server.log
LogFileSize=0
PidFile=/var/run/zabbix/zabbix_server.pid
DBHost=127.0.0.1
DBName=zabbix
DBUser=zabbix
DBPort=5432
SNMPTrapperFile=/var/log/snmptt/snmptt.log
AlertScriptsPath=/usr/lib/zabbix/alertscripts
ExternalScripts=/usr/lib/zabbix/externalscripts
# /etc/init.d/zabbix-server start
# egrep -v "(^$|^#)" /etc/httpd/conf.d/zabbix.conf
Alias /zabbix /usr/share/zabbix
&lt;Directory "/usr/share/zabbix"&gt;
    Options FollowSymLinks
    AllowOverride None
    Order allow,deny
    Allow from all
    php_value max_execution_time 300
    php_value memory_limit 128M
    php_value post_max_size 16M
    php_value upload_max_filesize 2M
    php_value max_input_time 300
    # 以下を変更
    # php_value date.timezone Europe/Riga
    php_value date.timezone Asia/Tokyo
&lt;/Directory&gt;
&lt;Directory "/usr/share/zabbix/conf"&gt;
    Order deny,allow
    Deny from all
    &lt;files *.php&gt;
        Order deny,allow
        Deny from all
    &lt;/files&gt;
&lt;/Directory&gt;
&lt;Directory "/usr/share/zabbix/api"&gt;
    Order deny,allow
    Deny from all
    &lt;files *.php&gt;
        Order deny,allow
        Deny from all
    &lt;/files&gt;
&lt;/Directory&gt;
&lt;Directory "/usr/share/zabbix/include"&gt;
    Order deny,allow
    Deny from all
    &lt;files *.php&gt;
        Order deny,allow
        Deny from all
    &lt;/files&gt;
&lt;/Directory&gt;
&lt;Directory "/usr/share/zabbix/include/classes"&gt;
    Order deny,allow
    Deny from all
    &lt;files *.php&gt;
        Order deny,allow
        Deny from all
    &lt;/files&gt;
&lt;/Directory&gt;
# chown -R apache. /var/log/httpd/
# /etc/init.d/httpd start
# echo "hello world" &gt; /var/www/html/index.html
# curl localhost
hello world
</pre>

	<p>ブラウザで以下にアクセスしてphpの設定を変更していく。<br />ホスト名等はVIPの値を設定する。</p>


<pre>
http://10.16.33.146/zabbix/setup.php#
</pre>

	<p><img src="/attachments/download/971/zabbix_php_setting01.png" alt="" /></p>


	<p>デフォルトユーザ名:パスワードは以下です。</p>


<pre>
Admin:zabbix
</pre>

	<p>最後に停止しておく。</p>


<pre>
# /etc/init.d/httpd stop
</pre>

<hr />


	<a name="Zabbix_pacemakerの準備"></a>
<h3 >pacemakerの準備<a href="#Zabbix_pacemakerの準備" class="wiki-anchor">&para;</a></h3>


<pre>
# yum install http://10.16.33.82:8888/additional_package/pacemaker-repo-1.1.12-1.1.el6.x86_64.rpm
# yum install pacemaker-all
# egrep -v "(^$|^#)" /etc/sysconfig/pacemaker
export PCMK_fail=yes
export PCMK_logfile=/var/log/cluster/pacemaker.log
# egrep -v "(^$|^#)" /etc/init/pacemaker.combined.conf |egrep -v "^ *#" 
stop on runlevel [0123456]
kill timeout 3600
respawn
env prog=pacemakerd
env rpm_sysconf=/etc/sysconfig/pacemaker
env rpm_lockfile=/var/lock/subsys/pacemaker
env deb_sysconf=/etc/default/pacemaker
env deb_lockfile=/var/lock/pacemaker
script
    [ -f "$rpm_sysconf" ] &#38;&#38; . $rpm_sysconf
    [ -f "$deb_sysconf" ] &#38;&#38; . $deb_sysconf
    exec $prog
end script
pre-start script
    [ -c /dev/watchdog ] || modprobe softdog soft_margin=60
    pidof corosync || start corosync
    sleep 2
    pidof corosync || { exit 1; }
end script
post-start script
    [ -f "$rpm_sysconf" ] &#38;&#38; . $rpm_sysconf
    [ -f "$deb_sysconf" ] &#38;&#38; . $deb_sysconf
    [ -z "$LOCK_FILE" -a -d /etc/sysconfig ] &#38;&#38; LOCK_FILE="$rpm_lockfile" 
    [ -z "$LOCK_FILE" -a -d /etc/default ] &#38;&#38; LOCK_FILE="$deb_lockfile" 
    touch $LOCK_FILE
    pidof $prog &gt; /var/run/$prog.pid
end script
post-stop script
    [ -f "$rpm_sysconf" ] &#38;&#38; . $rpm_sysconf
    [ -f "$deb_sysconf" ] &#38;&#38; . $deb_sysconf
    [ -z "$LOCK_FILE" -a -d /etc/sysconfig ] &#38;&#38; LOCK_FILE="$rpm_lockfile" 
    [ -z "$LOCK_FILE" -a -d /etc/default ] &#38;&#38; LOCK_FILE="$deb_lockfile" 
    rm -f $LOCK_FILE
    rm -f /var/run/$prog.pid
    pidof corosync || false
    pidof crmd || stop corosync
end script
# sed "s/\t/  /g" /etc/corosync/corosync.conf | egrep -v "(^$|^#|^ *#)" 
compatibility: whitetank
service {
  name: pacemaker
  ver: 0
  use_mgmud: yes
}
totem {
  version: 2
  crypto_cipher: none
  crypto_hash: none
  secauth: off
  rrp_mode: none
  interface {
    ringnumber: 0
    bindnetaddr: 10.16.32.0
    mcastport: 5405
    ttl: 1
  }
  transport: udpu
}
logging {
  fileline: off
  to_stderr: no
  to_logfile: yes
  logfile: /var/log/cluster/corosync.log
  logfile_priority: info
  to_syslog: no
  debug: off
  timestamp: on
  logger_subsys {
    subsys: QUORUM
    debug: off
  }
}
nodelist {
  node {
    ring0_addr: 10.16.32.146
    nodeid: 1
  }
  node {
    ring0_addr: 10.16.32.147
    nodeid: 2
  }
}
quorum {
  provider: corosync_votequorum
  expected_votes: 2
}
# initctl start pacemaker.combined
### 止めるときは以下のコマンド
# initctl stop pacemaker.combined
</pre>

	<p>以降、crmでの設定を行う</p>


<pre>
# crm configure property stonith-enabled="false" 
# crm configure property no-quorum-policy="ignore" 
# crm configure property crmd-transition-delay="2s" 
# crm configure rsc_defaults resource-stickiness="INFINITY" 
# crm configure rsc_defaults migration-threshold="1" 
### 以降業務的な定義
# crm configure primitive pr_vip ocf:heartbeat:IPaddr2 params ip=10.16.33.145 nic="bond0" cidr_netmask="24" op monitor interval="10s" 
# crm configure primitive pr_ping2gw ocf:pacemaker:ping params name="ping_to_gateway" host_list="10.16.33.254" multiplier="100" dampen="1" op monitor interval="10s" timeout="60" op start timeout="60" 
# crm configure primitive pr_postgres_drbd ocf:linbit:drbd params drbd_resource="postgres" op start interval="0s" timeout="240s" op stop interval="0s" timeout="100s" op monitor interval="15s" timeout="60s" role="Master" op monitor interval="30s" timeout="60s" role="Slave" 
# crm configure primitive pr_fs_postgres_drbd ocf:heartbeat:Filesystem params device="/dev/drbd0" directory="/var/lib/pgsql/9.4/data" fstype="ext4" op start interval="0s" timeout="60s" op stop interval="0s" timeout="60s" op monitor interval="30s" timeout="40s" 
# crm configure primitive pr_postgres lsb:postgresql-9.4 op start interval="0s" timeout="60s" op stop interval="0s" timeout="60s" op monitor interval="15s" timeout="60s" 
# crm configure primitive pr_zabbix lsb:zabbix-server op start interval="0s" timeout="30s" op stop interval="0s" timeout="30s" op monitor interval="15s" timeout="30s" 
# crm configure primitive pr_httpd ocf:heartbeat:apache params configfile="/etc/httpd/conf/httpd.conf" op start interval="0s" timeout="40s" op stop interval="0s" timeout="60s" op monitor interval="15s" timeout="30s" 
# crm configure ms ms_postgres_drbd pr_postgres_drbd meta master-max="1" master-node-max="1" clone-max="2" clone-node-max="1" notify="true" is-managed="true" target-role="Started" 
# crm configure group grp_postgres pr_ping2gw pr_vip pr_fs_postgres_drbd pr_postgres pr_zabbix pr_httpd
# crm configure colocation col_postgres_on_drbd inf: grp_postgres ms_postgres_drbd:Master
# crm configure order ord_postgres_after_drbd inf: ms_postgres_drbd:promote grp_postgres:start
### ここまででリソース起動が失敗しているので、全て解除する
# crm resource cleanup postgres_drbd
# crm resource cleanup fs_postgres_drbd
# crm resource cleanup postgres
### 手順を失敗した場合はpacemakerを停止後以下のコマンドを実行する
# rm -f /var/lib/pacemaker/cib/cib*
</pre>

	<p>上記とコマンドのcrm configure部分を除外することで設定の入込みも可能になる。</p>


<pre>
# crm configure &lt; command.txt
</pre>

<hr />


	<a name="Zabbix_logrotate"></a>
<h3 >logrotate<a href="#Zabbix_logrotate" class="wiki-anchor">&para;</a></h3>


<pre>
# egrep -v "(^$|^#)" httpd
/var/log/httpd/*log {
    daily
    create 600 root root
    rotate 400
    ifempty
    missingok
    dateext
    compress
    sharedscripts
    postrotate
        /sbin/service httpd reload &gt; /dev/null 2&gt;/dev/null || true
    endscript
}
# egrep -v "(^$|^#)" zabbix-server
/var/log/zabbix/zabbix_server.log {
    daily
    create 600 zabbix zabbix
    rotate 400
    ifempty
    missingok
    dateext
    compress
}
# egrep -v "(^$|^#)"  pacemaker
/var/log/cluster/pacemaker.log
/var/log/cluster/corosync.log {
    daily
    create 600 hacluster haclient
    rotate 400
    ifempty
    missingok
    dateext
    compress
    copytruncate
}
</pre>

	<p>以下のコマンドでsyntax checkが可能</p>


<pre>
# logrotate -d zabbix-server
</pre>
<hr />
<a name="クラコン構築メモ" />
<a name="クラコン構築メモ_クラコン構築メモ"></a>
<h1 >クラコン構築メモ<a href="#クラコン構築メモ_クラコン構築メモ" class="wiki-anchor">&para;</a></h1>


<hr />


	<a name="クラコン構築メモ_冗長化されたpostgresの準備"></a>
<h2 >冗長化されたpostgresの準備<a href="#クラコン構築メモ_冗長化されたpostgresの準備" class="wiki-anchor">&para;</a></h2>


	<p><a href="#Zabbix" class="wiki-page">Zabbix</a>を参考にdrbd,postgres,pacemakerを構築する。</p>


	<p>以下注意点。</p>


	<ul>
	<li>クラコンはディスクが無いので、vg_systemから10GB割り当てる</li>
		<li>DBは#1,2からアクセスするため、10.16.32.81をリッスンさせ、10.16.32.0/24からのアクセスをtrustに設定する</li>
		<li>shared_memoryは2048MBを設定</li>
	</ul>


	<p>登録するリソースは以下。</p>


<pre>
$ sudo crm
crm(live)# configure
crm(live)configure# property stonith-enabled="false" 
crm(live)configure# property no-quorum-policy="ignore" 
crm(live)configure# 
crm(live)configure# property crmd-transition-delay="2s" 
crm(live)configure# rsc_defaults resource-stickiness="INFINITY" 
crm(live)configure# rsc_defaults migration-threshold="1" 
crm(live)configure# primitive pr_vip ocf:heartbeat:IPaddr2 params ip=10.16.32.81 nic="bond3" cidr_netmask="24" op monitor interval="10s" 
crm(live)configure# primitive pr_postgres_drbd ocf:linbit:drbd params drbd_resource="postgres" op start interval="0s" timeout="240s" op stop interval="0s" timeout="100s" op monitor interval="15s" timeout="60s" role="Master" op monitor interval="30s" timeout="60s" role="Slave" 
crm(live)configure# primitive pr_fs_postgres_drbd ocf:heartbeat:Filesystem params device="/dev/drbd0" directory="/var/lib/pgsql/9.4/data" fstype="ext4" op start interval="0s" timeout="60s" op stop interval="0s" timeout="60s" op monitor interval="30s" timeout="40s" 
crm(live)configure# primitive pr_postgres lsb:postgresql-9.4 op start interval="0s" timeout="60s" op stop interval="0s" timeout="60s" op monitor interval="15s" timeout="60s" 
crm(live)configure# ms ms_postgres_drbd pr_postgres_drbd meta master-max="1" master-node-max="1" clone-max="2" clone-node-max="1" notify="true" is-managed="true" target-role="Started" 
crm(live)configure# group grp_postgres pr_vip pr_fs_postgres_drbd pr_postgres
crm(live)configure# colocation col_postgres_on_drbd inf: grp_postgres ms_postgres_drbd:Master
crm(live)configure# order ord_postgres_after_drbd inf: ms_postgres_drbd:promote grp_postgres:start
</pre>

	<p>最後にcommitを忘れずに！</p>


<hr />


	<a name="クラコン構築メモ_bind-bind-sdb"></a>
<h2 >bind bind-sdb<a href="#クラコン構築メモ_bind-bind-sdb" class="wiki-anchor">&para;</a></h2>


<pre>
$ sudo yum install bind bind-sdb
$ sudo chkconfig named on
$ psql postgres postgres -h 10.16.32.81
psql (9.4.1)
"help" でヘルプを表示します.

postgres=# create user named encrypted PASSWORD 'namednamed';
CREATE ROLE
postgres=# create database named owner named;
CREATE DATABASE
postgres=# \q
</pre>

	<p>テストとして以下のSQLを流す</p>


<pre>
create table idc_local  (name varchar(255) default NULL,ttl integer default NULL,rdtype varchar(255) default NULL,rdata varchar(255) default NULL);
insert into idc_local values ('idc.local','86400','SOA','idc.local. root.idc.local. 2014120101 3H 2M 1W 1D');
insert into idc_local values ('idc.local','86400','NS','idc.local.');
insert into idc_local values ('idc.local','86400','A','10.16.33.252');
create table idc_local_reverse (name varchar(255) default NULL,ttl integer default NULL,rdtype varchar(255) default NULL,rdata varchar(255) default NULL);
insert into idc_local_reverse values ('33.16.10.in-addr.arpa','86400','SOA','isb.local. root.isb.local. 2014120102 3H 2M 1W 1D');
insert into idc_local_reverse values ('33.16.10.in-addr.arpa','86400','NS','isb.local.');
insert into idc_local_reverse values ('252.33.16.10.in-addr.arpa','86400','PTR','isb.local.');
</pre>

	<p>設定ファイルは以下</p>


<pre>
# egrep -v "(^$|^#)" /etc/named.conf |egrep -v "(^//)" 
options {
    version none;
    listen-on port 53 { any; };
    listen-on-v6 port 53 { ::1; };
    directory     "/var/named";
    dump-file     "/var/named/data/cache_dump.db";
        statistics-file "/var/named/data/named_stats.txt";
        memstatistics-file "/var/named/data/named_mem_stats.txt";
    allow-query     { any; };
    recursion yes;
    forwarders { 8.8.8.8; 8.8.4.4; };
    forward only;
    dnssec-enable yes;
    dnssec-validation yes;
    dnssec-lookaside auto;
    /* Path to ISC DLV key */
    bindkeys-file "/etc/named.iscdlv.key";
    managed-keys-directory "/var/named/dynamic";
};
logging {
        channel default_debug {
                file "data/named.run";
                severity dynamic;
        print-time yes;
        print-severity yes;
        print-category yes;
        };
};
zone "." IN {
    type hint;
    file "named.ca";
};
include "/etc/named.root.key";
zone "idc.local" {
        type master;
        database "pgsql named idc_local 10.16.32.81 named namednamed";
};
zone "33.16.10.in-addr.arpa" {
        type master;
        database "pgsql named idc_local_reverse 10.16.32.81 named namednamed";
};
# egrep -v "(^$|^#)" /etc/sysconfig/named 
ENABLE_SDB=yes
</pre>

<hr />


	<a name="クラコン構築メモ_セキュリティ設定についての検討メモ"></a>
<h2 >セキュリティ設定についての検討メモ<a href="#クラコン構築メモ_セキュリティ設定についての検討メモ" class="wiki-anchor">&para;</a></h2>


	<p>以下の点に注意する。</p>


	<ul>
	<li>バージョン情報の非表示(「version none;」で可能)</li>
		<li>内部的な情報は問い合わせの範囲を制限する(DNS自体は公開する必要があるため、「ACL」や「allow-query」を内部zoneに設定して制限。もしくは「allow-recursion {"internal";};」を指定することで)
	<ul>
	<li>internalに指定するプライベート</li>
	</ul>
	</li>
		<li>ゾーン転送の禁止(今回、DBを利用している関係上ゾーン転送は不要である「allow-transfer {none;};」により全てを禁止とする)</li>
		<li>プロセスのユーザ指定(9系ではデフォルトでnamedユーザとなる)</li>
		<li>chrootでの制限</li>
		<li>メッセージIDのランダマイズ(9系ではデフォルトでuse-id-poolが有効となる)</li>
		<li>DOS攻撃等で攻撃元がわかる場合の対処(blackhole {ipaddress/networkaddress;};)</li>
	</ul>


	<p>また、拡張性を考慮すると今回のbind-sdbを利用している構成ではzoneファイルの作成はDBテーブルの作成を意味する。<br />これを自動化するのは問題があるため動的に更新が必要な部分を切り出し、レコードの追加だけに留める。</p>


	<p>以下に上記を考慮した設定ファイルの案を記載する。<br />ここでは仮に管理すうドメインをidc.isb.co.jpとし、<br />外部DNSサービスにNSレコードがns.idc.isb.co.jp、<br />ns.idc.isb.co.jpのAレコードが1.2.3.4で登録されているとする。<br />※DNSは冗長構成となっているが、外部からの問い合わせをどのように冗長化するかを検討する必要がある</p>


<pre>
### private ip and loopback ip
acl "internal" { 10.0.0.0/8; 172.16.0.0/12; 192.168.0.0/24; 127.0.0.0/8 };

options {
    ### バージョン情報の非表示
    version none;
    ### 外部公開するためanyを指定する。
    listen-on port 53 { any; };
    ### ipv6は利用しないので、とりあえずlocalhostのみを指定
    listen-on-v6 port 53 { ::1; };
    ### メッセージIDのランダマイズ（デフォルトで有効であるが明示的に指定する）
    use-id-pool yes;
    directory     "/var/named";
    dump-file     "/var/named/data/cache_dump.db";
    statistics-file "/var/named/data/named_stats.txt";
    memstatistics-file "/var/named/data/named_mem_stats.txt";
    ### allow-queryをinternalのみに制限すると管理するドメインの外部からの要求を拒否してしまう
    ### view毎に設定できるためグローバルとしてはセキュリティが高い方を選択する
    ### 上記の問題に対してallow-recursionは外部からの問い合わせについては非再帰的な要求として回答する
    allow-query     { "internal"; };
    allow-recursion { "internal"; };
    ### 攻撃元がわかる場合は以下に設定を入込み、拒否する
    blackhole {
    };
    ### 以下の設定がnoであるとそもそも再帰的な問い合わせを行わない view毎に設定できるためグローバルとしてはセキュリティが高い方を選択する
    ### 外部公開用のDNSであれば以下をnoに設定するだけでOK
    recursion no;
    ### 外部DNSサービスの登録(以下はgoogleのパブリックドメインであるが、外部DNSサービスとしては信頼性は高いかもしれない)
    forwarders { 8.8.8.8; 8.8.4.4; };
    ### forwardersで指定した上位DNSへ名前解決が失敗した場合にルートDNSへのリクエストを行わず、そのまま失敗を返却する
    forward only;
    ### ゾーン情報の転送禁止
    allow-transfer { none; };
    ### 未検討
    dnssec-enable yes;
    dnssec-validation yes;
    dnssec-lookaside auto;
    /* Path to ISC DLV key */
    bindkeys-file "/etc/named.iscdlv.key";
    managed-keys-directory "/var/named/dynamic";
};
### 未検討 基本的には/var/log/named/配下にログを出力したい
logging {
    channel default_debug {
        file "data/named.run";
        severity dynamic;
        print-time yes;
        print-severity yes;
        print-category yes;
    };
    channel "log_default" { #ログの出力方法をlog_defaultとして定義
        file "/var/log/named.log" versions 7 size 10m;
        severity info;
        print-time yes;
        print-category yes;
    };
    channel "log_security" {
        file "/var/log/security.log" versions 3 size 10m;
        severity info;
        print-time yes;
        print-category yes;
    };
    category default { "log_default"; };
    category security { "log_security"; };
    category client { "log_security"; };
    category queries { "log_queries"; };
};

### 未検討
include "/etc/named.root.key";
### 内部用
view "internal" {
    ### 以下に一致するクライアントのみがviewに設定されたものを参照可能
    match-clients { "internal"; };
    ### 内部情報に関しては再帰問い合わせを無条件で許可する
    recursion yes;
    ### 内部のアドレスはhostsファイルで管理するが逆引きできないため登録する
    zone "local.idc.isb.co.jp" {
        type master;
        ### databaseのテーブル名はzone名の[.]を[_]に変換した名前とする
        database "pgsql named local_idc_isb_co_jp 10.16.32.81 named namednamed";
    };
    ### 内部逆引き用
    zone "10.in-addr.arpa" {
        type master;
        ### 逆引き用のテーブル名は正引き用のテーブル名にreverseを接尾語としてつける
        database "pgsql named local_idc_isb_co_jp_reverse 10.16.32.81 named namednamed";
    };
    ### 要検討
    ### テナント管理用
    zone "tenant.local.idc.isb.co.jp" {
        type master;
        database "pgsql named tenant_local_idc_isb_co_jp 10.16.32.81 named namednamed";
    };
    ### 要検討
    ### テナント管理用逆引き
    zone "10.in-addr.arpa" {
        type master;
        database "pgsql named tenant_local_idc_isb_co_jp_reverse 10.16.32.81 named namednamed";
    };
    ### 外部公開
    ### 以下のドメインのサブドメインとしてテナントに向かせる
    zone "idc.isb.co.jp" {
        type master;
        database "pgsql named idc_isb_co_jp 10.16.32.81 named namednamed";
    };
    ### 外部公開の逆引き用
    zone "3.2.1.in-addr.arpa" {
        type master;
        database "pgsql named idc_isb_co_jp_reverse 10.16.32.81 named namednamed";
    };
    ### 未検討
    zone "." IN {
        type hint;
        file "named.ca";
    };
};
### 外部用
view "external" {
    ### 外部公開するためクライアント制限を設けない
    match-clients { any; };
    ### 外部からの再帰問い合わせは行わない
    recursion no;
    ### 外部公開
    zone "idc.isb.co.jp" {
        type master;
        database "pgsql named idc_isb_co_jp 10.16.32.81 named namednamed";
    };
    ### 外部公開の逆引き用
    zone "3.2.1.in-addr.arpa" {
        type master;
        database "pgsql named idc_isb_co_jp_reverse 10.16.32.81 named namednamed";
    };
};
</pre>

	<p>テーブルに追加すべきレコード情報は以下となる。</p>


	<p>local_idc_isb_co_jp <br />local_idc_isb_co_jp_reverse <br />idc_isb_co_jp <br />idc_isb_co_jp_reverse <br />tenant_local_idc_isb_co_jp <br />tenant_local_idc_isb_co_jp_reverse</p>
<hr />
<a name="クラスタ設計" />
<a name="クラスタ設計_クラスタ設計"></a>
<h1 >クラスタ設計<a href="#クラスタ設計_クラスタ設計" class="wiki-anchor">&para;</a></h1>


	<p>■必要パッケージ（グループ）<br />①KVm#1/#2に導入したパッケージ<br />・"Virtualization"　　　　　　#（KVM用）<br />・"Virtualization Client"　　 #（KVM用）<br />・"Virtualization Platform"   #（KVM用）<br />・"Virtualization Tools"      #（KVM用）</p>


	<p>・cman                        #（クラスタマネージメント機能）<br />・lvm2-cluster                #（クラスタ環境での論理ボリューム管理（LVM））<br />・ccs                         #（cluster.confファイルを作成、管理する為のツール）</p>


	<p>■関連パッケージインストール<br />[root@dtj-kt01 iscsi]# yum install cman<br />[root@dtj-kt01 iscsi]# yum install lvm2-cluster<br />[root@dtj-kt01 cman-notify.d]# yum install ccs</p>


	<p>■cluster.confファイルを作成<br />vim /etc/cluster/cluster.conf<br />★設定内容について精査が必要！！！！<br />-------------------------------------------------------------<br />ファイル作成後、KVM#2にcluster.confをコピー（scp等で）</p>


	<p><?xml version="1.0"?><br />&lt;cluster name="dae-ohyp_clust" config_version="22"&gt;<br />&lt;cman expected_votes="3" quorum_dev_poll="21000"/&gt;</p>


	<p>&lt;clusternodes&gt;<br /> &lt;clusternode name="dae-ohyp01" nodeid="1" votes="1"&gt;<br />   &lt;fence&gt;<br />      &lt;method name="1"&gt;<br />          &lt;device name="scsifence" key="1"/&gt;<br />             &lt;/method&gt;<br />               &lt;/fence&gt;<br />                 &lt;unfence&gt;<br />                         &lt;device name="scsifence" key="1" action="on"/&gt;<br />                           &lt;/unfence&gt;<br /> &lt;/clusternode&gt;</p>


	<pre><code>&amp;lt;clusternode name="dae-ohyp02" nodeid="2" votes="1"&amp;gt;<br />   &amp;lt;fence&amp;gt;<br />      &amp;lt;method name="1"&amp;gt;<br />          &amp;lt;device name="scsifence" key="2"/&amp;gt;<br />             &amp;lt;/method&amp;gt;<br />               &amp;lt;/fence&amp;gt;<br />                 &amp;lt;unfence&amp;gt;<br />                         &amp;lt;device name="scsifence" key="2" action="on"/&amp;gt;<br />                           &amp;lt;/unfence&amp;gt;<br /> &amp;lt;/clusternode&amp;gt;<br /> &amp;lt;/clusternodes&amp;gt;</code></pre>


	<p>&lt;totem token="20000" window_size="300" miss_count_const="100"/&gt;<br />&lt;quorumd label="dae-qdisk01" interval="1" tko="10" min_score="1" votes="1"&gt;<br /> &lt;heuristic program="ping -c1 -w1 10.16.145.254" score="1" interval="2" tko="3"/&gt;<br /> &lt;/quorumd&gt;</p>


	<p>&lt;fencedevices&gt;<br /> &lt;fencedevice devices="/dev/eql/dae-02" agent="fence_scsi" name="scsifence" logfile="/var/log/cluster/f<br />ence_scsi.log"/><br /> &lt;/fencedevices&gt;</p>


	<p>&lt;rm&gt;<br />&lt;/rm&gt;<br />&lt;/cluster&gt;<br />---------------------------------------------------------------------------------------------------------</p>


	<p>■サービス自動起動をoff<br />chkconfig cman off<br />chkconfig clvmd off<br />chkconfig libvirtd off</p>


	<p>★clvmdは必ずcman起動後に起動する！！！<br />★各サービスの起動順序は以下の通りが最適か！？<br />1. iscsi/iscsid(自動起動)<br />2. cman<br />3. clvmd<br />4. libvirtd<br />5. heartbeat（後述）</p>


	<p>■クォーラムディスク設定<br />※前提条件：iSCSIストレージ内にクォーラムディスク用の領域が用意されていること。<br />　容量は100Mくらいでいいらしい、が、一応1Gで作成しておいた。</p>
<hr />
<a name="ノウハウまとめ" />
<a name="ノウハウまとめ_ノウハウまとめ"></a>
<h1 >ノウハウまとめ<a href="#ノウハウまとめ_ノウハウまとめ" class="wiki-anchor">&para;</a></h1>


<hr />


	<a name="ノウハウまとめ_macアドレスとuuidの生成"></a>
<h2 >macアドレスとuuidの生成<a href="#ノウハウまとめ_macアドレスとuuidの生成" class="wiki-anchor">&para;</a></h2>


	<p>macアドレス<br />/dev/randomはブロックが頻発するので、乱数の信頼性はrandomよりも劣るがmacアドレスとして利用するためであれば問題なし<br /><pre>
[root@kvm-01 ~]# od -vAn -N6 -tx1 &lt; /dev/urandom| cut -d" " -f2- --output-delimiter=:
01:0d:a7:bb:91:7c
</pre></p>


	<p>uuidはコマンドがあるのでそれで</p>


<pre>
[root@kvm-01 ~]# uuidgen
9c02f342-08b5-434a-8713-78d1801ab210
</pre>

<hr />


	<a name="ノウハウまとめ_鍵の生成"></a>
<h2 >鍵の生成<a href="#ノウハウまとめ_鍵の生成" class="wiki-anchor">&para;</a></h2>


	<p>同一名称のファイルが有る場合に上書きを行うかどうかを標準入力でyes/no指定させるため、注意が必要<br />ちなみに、bitの指定の最大値は32768(2の15乗)でした。</p>


<pre>
# ssh-keygen -f kajiro1_id_rsa -N kajiro1_id_rsa_passphrase -b 4096 # デフォルトのbitは2048。manページにはこれで十分との記述もある。
</pre>

<hr />


	<a name="ノウハウまとめ_BondingModeまとめ"></a>
<h2 >BondingModeまとめ<a href="#ノウハウまとめ_BondingModeまとめ" class="wiki-anchor">&para;</a></h2>


	<p>BondingMode一覧<br /><pre>
mode0:balance-rr
　負荷分散と耐障害性を提供します。
　負荷分散方式はラウンドロビンです。
　モードを指定しなかった場合にはこのモードが利用されます。
　※送信のみ負荷分散で、受信はSW依存
　※異なるspeed/duplex設定の混在はNG
　※trunk（Ether Channel）に対応したネットワークスイッチが必要です。
　※監視モードはMII／ARPどちらも対応

mode1:active-backup
　耐障害性を提供します。
　1つのNICのみが通信を行い、障害が発生した場合は、他のNICに切り替わります。
　非常に良く利用されるモードです。 
　※異なるspeed/duplex設定の混在OK
　※特別なネットワーク機器は特に必要ありません
　※監視モードはMII／ARPどちらも対応

mode2:balance-xor
　負荷分散と耐障害性を提供します。
　分散方式は送信先/送信元MACアドレスのxorが使われます。
　オプションでMACアドレスをキーとしたハッシュを利用することも可能です。
　※送信のみXORスタイルで負荷分散（偏る場合有）、受信はSW依存
　※異なるspeed/duplex設定の混在はNG
　※trunk（Ether Channel）に対応したネットワークスイッチが必要です。 
　※監視モードはMII／ARPどちらも対応

mode3:broadcast
　特殊な環境を除き、ほとんど利用されることはありません。
　束ねたNIC全てからパケットが送信されます。 

mode4:802.3ad
　IEEE 802.3ad Dynamic link aggregation規格で接続します。
　※送信のみXORスタイルで負荷分散（偏る場合有）、受信はSW依存
　※IEEE 802.3adに対応したネットワークスイッチが必要です。 
　※監視モードはMIIのみ対応

mode5:balance-tlb
　負荷分散と耐障害性を提供します。
　NICの速度及び負荷に応じて負荷分散を行います。
　※送信のみ負荷分散で、受信はSW依存
　※特別なネットワーク機器は特に必要ありません。 
　※異なるspeed/duplex設定の混在OK
　※監視モードはMIIのみ対応

mode6:balance-alb
　負荷分散と耐障害性を提供します。
　NICの速度及び負荷に応じて負荷分散を行います。
　balance-tlbの機能に加え、受信も負荷分散する方式
　※送受信を負荷分散
　※特別なネットワーク機器は特に必要ありません。
　※異なるspeed/duplex設定の混在OK
　※監視モードはMIIのみ対応　
</pre></p>


	<p>/etc/sysconfig/network-scripts/配下の設定スクリプトへの指定方法<br /><pre>
mode：bondigの動作モードを指定します。詳細は以下を参照
primary：アクティブにするインターフェースを指定します。
miimon：MII監視間隔を1/1000秒単位で指定します。
updelay：リンクアップを検知後、該当NICに切り替えるまでの時間を1/1000秒単位で指定します。対向のスイッチとの通信を確立させるタイミングを調整するために利用します。
----------------------------
（例）
BONDING_OPTS="mode=1 primary=eth0 miimon=100 updelay=5000" 
----------------------------
</pre></p>


<hr />


	<a name="ノウハウまとめ_手動でpanic-error"></a>
<h2 >手動でpanic error<a href="#ノウハウまとめ_手動でpanic-error" class="wiki-anchor">&para;</a></h2>


	<p>echo 1 > /proc/sys/kernel/sysrq<br />echo c > /proc/sysrq-trigger</p>


	<a name="ノウハウまとめ_KVM仮想マシンテンプレートからのリストア"></a>
<h2 ><a href="#KVM仮想マシンテンプレートからのリストア" class="wiki-page">KVM仮想マシンテンプレートからのリストア</a><a href="#ノウハウまとめ_KVM仮想マシンテンプレートからのリストア" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="ノウハウまとめ_vmware形式の仮想マシンをkvmへ移行"></a>
<h2 ><a href="#Vmware形式の仮想マシンをkvmへ移行" class="wiki-page">vmware形式の仮想マシンをkvmへ移行</a><a href="#ノウハウまとめ_vmware形式の仮想マシンをkvmへ移行" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="ノウハウまとめ_vyosインストール"></a>
<h2 ><a href="#Vyosインストール" class="wiki-page">vyosインストール</a><a href="#ノウハウまとめ_vyosインストール" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="ノウハウまとめ_iSCSIボリュームの認識接続まで"></a>
<h2 ><a href="#ISCSIボリュームの認識〜接続まで" class="wiki-page">iSCSIボリュームの認識〜接続まで</a><a href="#ノウハウまとめ_iSCSIボリュームの認識接続まで" class="wiki-anchor">&para;</a></h2>


<hr />


	<a name="ノウハウまとめ_iSCSIストレージボリュームの拡張Linux上での認識まで"></a>
<h2 ><a href="#ISCSIストレージボリュームの拡張〜Linux上での認識まで〜" class="wiki-page">iSCSIストレージボリュームの拡張〜Linux上での認識まで〜</a><a href="#ノウハウまとめ_iSCSIストレージボリュームの拡張Linux上での認識まで" class="wiki-anchor">&para;</a></h2>


<hr />
<hr />
<a name="バックアップ方法まとめ" />
<a name="バックアップ方法まとめ_バックアップ方法まとめ"></a>
<h1 >バックアップ方法まとめ<a href="#バックアップ方法まとめ_バックアップ方法まとめ" class="wiki-anchor">&para;</a></h1>


	<a name="バックアップ方法まとめ_mysql"></a>
<h2 >mysql<a href="#バックアップ方法まとめ_mysql" class="wiki-anchor">&para;</a></h2>


<pre>
#!/bin/bash

myLog() {
  local d="`date +%FT%H:%M:%S`" 
  local m="$1" 
  echo "${d}[backupMysql]:${m}" 
}

backupdir="/backup/db" 
target="${backupdir}/mysql_`date +%F-%s`.dump.gz" 
dbname="missnon_production" 

mysqldump --opt -u root -ppassword ${dbname} | gzip &gt; ${target}

ls ${target} &gt; /dev/null &#38;&#38; myLog "Success." 
exit 0
</pre>

	<a name="バックアップ方法まとめ_postgresql"></a>
<h2 >postgresql<a href="#バックアップ方法まとめ_postgresql" class="wiki-anchor">&para;</a></h2>


<pre>
#!/bin/bash

myLog() {
  local d="`date +%FT%H:%M:%S`" 
  local m="$1" 
  echo "${d}[backupPostgresql]:${m}" 
}

backupdir="/backup/db" 
target="${backupdir}/pgdump_`date +%F-%s`.sql.gz" 

pg_dumpall -c -U postgres | gzip &gt; ${target}

ls ${target} &gt; /dev/null &#38;&#38; myLog "Success." 
exit 0
</pre>

	<a name="バックアップ方法まとめ_nfs"></a>
<h2 >nfs<a href="#バックアップ方法まとめ_nfs" class="wiki-anchor">&para;</a></h2>


<pre>
#!/bin/bash

myLog() {
  local d="`date +%FT%H:%M:%S`" 
  local m="$1" 
  echo "${d}[backupNfs]:${m}" 
}

backupdir="/backup/nfs" 
target="${backupdir}/nfs_`date +%F-%s`.tar.gz" 
datadir="/var/lib/mysql/image" 

cd ${datadir}
/bin/tar czf ${target} .

ls ${target} &gt; /dev/null &#38;&#38; myLog "Success." 
exit 0
</pre>

	<a name="バックアップ方法まとめ_世代管理スクリプト"></a>
<h2 >世代管理スクリプト<a href="#バックアップ方法まとめ_世代管理スクリプト" class="wiki-anchor">&para;</a></h2>


<pre>
#!/bin/bash

myLog() {
  local d="`date +%FT%H:%M:%S`" 
  local m="$1" 
  echo "${d}[rotateBackup]:${m}" 
}

### 世代管理したいディレクトリを空白区切りで追加していく
### ファイル数で世代管理しているので、一回のバックアップに一ファイルとなるようにする
targets="/backup/db /backup/nfs" 
generation=7

for i in ${targets};do
  backupfiles=`ls ${i} | wc -l`
  if [ ${backupfiles} -gt ${generation} ];then
    deletetarget=`ls ${i} | sort | head -n 1`
    /bin/rm -f ${i}/${deletetarget}
    myLog "Delete ${i}/${deletetarget}" 
  fi
done

exit 0
</pre>

	<a name="バックアップ方法まとめ_cronへの登録"></a>
<h2 >cronへの登録<a href="#バックアップ方法まとめ_cronへの登録" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>/etc/cron.d/backup<br /><pre>
### backup at 02:05 am
5 2 * * * root /usr/local/bin/backupMysql.sh &gt;&gt; /var/log/backup.log 2&gt;&#38;1
### backup at 03:05 am
5 3 * * * root /usr/local/bin/backupNfs.sh &gt;&gt; /var/log/backup.log 2&gt;&#38;1
### rotate at 04:05 am
5 4 * * * root /usr/local/bin/rotateBackup.sh &gt;&gt; /var/log/backup.log 2&gt;&#38;1
</pre></li>
	</ul>
<hr />
<a name="マルチパス設定" />
<a name="マルチパス設定_マルチパス設定"></a>
<h1 >マルチパス設定<a href="#マルチパス設定_マルチパス設定" class="wiki-anchor">&para;</a></h1>


	<a name="マルチパス設定_インストール"></a>
<h2 >インストール<a href="#マルチパス設定_インストール" class="wiki-anchor">&para;</a></h2>


	<p>ストレージへの接続および設定が、Linuxで作るiSCSI Targetとは<br />少し勝手が違うので構築手順をメモしておく。</p>


	<p>■Equallogicへ接続する為の準備作業</p>


	<p>Equallogicへの接続には、専用のユーティリティソフトを<br />サーバ上にインストールする必要がある為、以下のファイルを<br />DELLのサポートサイトからダウンロードし、サーバ上に設置しておく。<br />iSCSI-initiatorももちろん必要。<br /><pre>
　・equallogic-host-tools-x.x.x-x.iso　★isoファイルで配布されている。xはバージョン。
　・RPM-GPG-KEY-DELLEQL　　　　　　　　★インストール時にGPG-KEYが必要。インストール時にスキップできるが一応準備しておく。
</pre></p>


	<p>■equallogic-host-toolsインストール作業</p>


	<p>GPG-KEYファイルをインストール<br /><pre>
[root@dae-ohyp01 equallogic-host-tools]# rpm --import RPM-GPG-KEY-DELLEQL　★コマンド実行後、特にメッセージは出力されない。
</pre></p>


	<p>isoファイルを任意のディレクトリにマウント。</p>


	<p>展開されたiso内の[install]スクリプトを実行する。<br />以下、対話形式のインストールが開始される。<br /><pre>
[root@dae-ohyp01 mnt]# /mnt/install
Dell EqualLogic install script version 1.3.0 Build 388434
Copyright (c) 2010-2014 Dell Inc.

======================================================================
Beginning install of Dell EqualLogic Host Integration Tools
======================================================================

Detected compatible pre-compiled kernel module packages:
  /mnt/packages/el6/x86_64/kmod-dell-dm-switch-1.3.0-0.x86_64.rpm

Displaying the End User License Agreement..
DELL END USER LICENSE AGREEMENT Ver. 1.3 TYPE-A SOFTWARE

THIS IS A LEGAL AGREEMENT BETWEEN YOU (EITHER AN INDIVIDUAL OR AN ENTITY) AND
DELL PRODUCTS L.P. OR DELL GLOBAL B.V. (SINGAPORE BRANCH), ON BEHALF OF DELL
INC. AND ITS WORLDWIDE SUBSIDIARIES AND AFFILIATES (COLLECTIVELY, "Dell" OR
"DELL"), WHICH GOVERNS YOUR USE OF THE SOFTWARE. THE SOFTWARE SHALL MEAN
COLLECTIVELY THE SOFTWARE PROGRAM, THE ASSOCIATED MEDIA, PRINTED MATERIALS,
ONLINE OR ELECTRONIC DOCUMENTATION, AND ANY COPIES THEREOF, TO WHICH THIS
AGREEMENT IS ATTACHED OR OTHERWISE ASSOCIATED (the "Software" or "SOFTWARE").
PLEASE READ THE TERMS AND CONDITIONS OF THIS AGREEMENT CAREFULLY, INCLUDING,
WITHOUT LIMITATION, ANY SUPPLEMENTAL TERMS AND CONDITIONS APPEARING OR
REFERENCED BELOW, WHICH ARE HEREBY MADE PART OF THIS END USER LICENSE AGREEMENT
(COLLECTIVELY, "EULA"), BEFORE DOWNLOADING, INSTALLING, ACTIVIATING AND/OR
OTHERWISE USING THE SOFTWARE. BY EXPRESSLY ACCEPTING THESE TERMS OR DOWNLOADING,
INSTALLING, ACTIVATING AND/OR OTHERWISE USING THE SOFTWARE, YOU ARE AGREEING
THAT YOU HAVE READ, AND THAT YOU AGREE TO COMPLY WITH AND TO BE BOUND BY THE
TERMS AND CONDITIONS OF THIS EULA AND ALL APPLICABLE LAWS AND REGULATIONS IN
THEIR ENTIRETY WITHOUT LIMITATION OR QUALIFICATION. IF YOU DO NOT AGREE TO BE
BOUND BY THE TERMS AND CONDITIONS OF THIS EULA, THEN YOU MAY NOT DOWNLOAD,
INSTALL, ACTIVATE OR OTHERWISE USE ANY OF THE SOFTWARE AND YOU MUST PROMPTLY
RETURN THE SOFTWARE AND ANY HARDWARE TO WHICH IT IS ATTACHED, AS DIRECTED BY
DELL OR ITS RESELLER (IF APPLICABLE) FOR A FULL REFUND.  IF YOU ARE AN
INDIVIDUAL REPRESENTING AN ENTITY, YOU ACKNOWLEDGE THAT YOU HAVE THE APPROPRIATE
Press &lt;Space&gt; to view the next page, or 'q' to quit viewing the EULA (1/19)
～省略～

Displaying the End User License Agreement..

Please indicate your response to EULA terms and conditions by typing (Accept/Reject):　　★EULAに同意する旨を”Accept”として入力

A copy of this End User License Agreement will be installed at /usr/share/doc/equallogic-host-tools-1.3.0/EULA for future reference.

================================================================================
 Package               Arch   Version Repository                           Size
================================================================================
Installing:
 equallogic-host-tools x86_64 1.3.0-2.el6
                                      /equallogic-host-tools-1.3.0-2.el6.x86_64
                                                                           18 M
 kmod-dell-dm-switch   x86_64 1.3.0-0 /kmod-dell-dm-switch-1.3.0-0.x86_64 274 k

Transaction Summary
================================================================================
Install       2 Package(s)
</pre></p>


<pre>
Is this ok [y/N]:　★インストールの確認にて”y”を入力

Installing version 1.3.0-2.el6 of equallogic-host-tools

Install succeeded.

======================================================================
Running eqlconfig to perform initial configuration
======================================================================

Dell EqualLogic 'eqlconfig' version 1.3.0 Build 388434
Copyright (c) 2010-2014 Dell Inc.

======================================================================
Configuring MPIO Parameters
======================================================================
</pre>

<pre>
Would you like ehcmd to actively manage MPIO and iSCSI sessions (Yes/No) [Yes]?　★Yesを入力
</pre>

<pre>
Choose address protocol (IPv4/IPv6) [IPv4]: IPv4　★IPv4を入力
</pre>

<pre>
Found the following subnets for MPIO:

        1.) 10.16.161.0/24
        2.) 10.16.163.0/24
        3.) 10.16.168.0/21
        4.) Choose individual NICs

Enter a comma-separated list of subnets that you want to use for MPIO, e.g., 1, 2 or select individual NICs (1, 2, 3, 4) [1, 2, 3]: 4 ★MPIO用にどのサブネットorNICを使うか選択。試しにNICでやってみる。
</pre>

<pre>
Found the following NICs in the system. Which ones would you like to configure?

        1.) bond0 (10.16.161.2)
        2.) br83 (10.16.163.2)
        3.) eth2 (10.16.168.2)
        4.) eth3 (10.16.169.2)
        5.) eth6 (10.16.170.2)
        6.) eth7 (10.16.171.2)
        7.) Choose subnets

Enter a comma-separated list of NICs that you want to use for MPIO, e.g., 1, 2 (1, 2, 3, 4, 5, 6, 7):3,4,5,6
★eth2,3,6,7を使うので3,4,5,6番を選択
</pre>

<pre>
To check the adapters used for MPIO, see 'ehcmcli status'
To change additional parameters for MPIO, use 'rswcli --mpio-parameters'

======================================================================
Configuring ASM Parameters
======================================================================

What document directory should ASM use to store Smart Copy backup documents [/var/lib/equallogic/asm/smart-copies]:  ★そのまま
</pre>

<pre>
No iSCSI target portals configured.  Once you have configured iSCSI
access to your EqualLogic PS Series group, see 'asmcli help group-access'
for instructions.

======================================================================
Running 'eqltune' to check system settings
======================================================================

Dell EqualLogic 'eqltune' version 1.3.0 Build 388434
Copyright (c) 2010-2014 Dell Inc.

Checking your Linux system for optimal iSCSI performance...

Sysctl Tunables          Critical    Warnings    Suggestions    Ok
-------------------------------------------------------------------
ARP Flux                      8           0            0         0
RP Filter                     4           0            0         0
Network Buffers               0           0           10         0
Scheduler                     0           0            1         0

Ethernet Devices
-------------------------------------------------------------------
eth2                          1           0            0         2
eth3                          1           0            0         2
eth6                          1           0            0         2
eth7                          1           0            0         2

iSCSI Settings
-------------------------------------------------------------------
iscsid.conf defaults          1           4            1         1

External Utility Settings
-------------------------------------------------------------------
Blacklists                    1           0            0         0

EqualLogic Host Tools
-------------------------------------------------------------------
Running system checks         0           0            0         2
eqlvolume checks              0           0            0         3

Run in verbose mode (eqltune -v) for more details and instructions on how
to adjust your settings.

Run in fix mode (eqltune fix) to automatically repair all Critical issues.

Critical errors detected.  Eqltune can fix these automatically.
How would you like to proceed (Fix/List/Ignore) [Fix]?　★自動チェック結果に対して自動的に修正するかどうか選択。Fixを入力。
</pre>

<pre>
Dell EqualLogic 'eqltune' version 1.3.0 Build 388434
Copyright (c) 2010-2014 Dell Inc.

Repairing all critical system issues...

Block Devices
-------------
No issues to fix

Sysctl Tunables
---------------
Fixing: Critical: net/ipv4/conf/eth2/arp_ignore=0, must be 1
Fixing: Critical: net/ipv4/conf/eth2/arp_announce=0, must be 2
Fixing: Critical: net/ipv4/conf/eth3/arp_ignore=0, must be 1
Fixing: Critical: net/ipv4/conf/eth3/arp_announce=0, must be 2
Fixing: Critical: net/ipv4/conf/eth6/arp_ignore=0, must be 1
Fixing: Critical: net/ipv4/conf/eth6/arp_announce=0, must be 2
Fixing: Critical: net/ipv4/conf/eth7/arp_ignore=0, must be 1
Fixing: Critical: net/ipv4/conf/eth7/arp_announce=0, must be 2
Fixing: Critical: net/ipv4/conf/eth2/rp_filter=1, must be one of [0, 2]
Fixing: Critical: net/ipv4/conf/eth3/rp_filter=1, must be one of [0, 2]
Fixing: Critical: net/ipv4/conf/eth6/rp_filter=1, must be one of [0, 2]
Fixing: Critical: net/ipv4/conf/eth7/rp_filter=1, must be one of [0, 2]

Ethernet Devices
----------------
Fixing: Critical: eth2 Generic Receive Offload=on, must be off
Fixing: Critical: eth3 Generic Receive Offload=on, must be off
Fixing: Critical: eth6 Generic Receive Offload=on, must be off
Fixing: Critical: eth7 Generic Receive Offload=on, must be off

iSCSI Settings
--------------
Fixing: Critical: node.session.iscsi.FastAbort=Yes, must be No

External Utility Settings
-------------------------
Fixing: Critical: LVM device filter=not set, must be set

EqualLogic Host Tools
---------------------
No issues to fix

Notice
------
eqltune has edited 3 system configuration files, with back-ups as follows:
  * /var/lib/equallogic/eqltune.backup/iscsid.conf
  * /var/lib/equallogic/eqltune.backup/sysctl.conf
  * /var/lib/equallogic/eqltune.backup/lvm.conf

Completed successfully

======================================================================
Starting MPIO Services
======================================================================

Starting service: ehcmd
Starting service: scsi_reserve_eql

======================================================================
Configuration complete
======================================================================

To run the configuration utility again, use 'eqlconfig'
</pre>

	<p>ここまででインストール完了。</p>


	<p>■設定の確認<br />以下のコマンドで設定内容がわかる。<br />使用するNICの数、ストレージ領域数によってチューニングする必要あり。</p>


<pre>
[root@dae-ohyp01 mnt]# rswcli --mpio-parameters

Processing mpio-parameters command...

MPIO Parameters:

Max sessions per volume slice: 2　★ストレージボリュームに対する最大セッション数？今回は４セッション使うので足りない。
Max sessions per entire volume: 6　★要調査
Minimum adapter speed: 1000
Default load balancing policy configuration: Least Queue Depth (LQD)
IOs Per Path: 16
Use MPIO for snapshots: Yes
Internet Protocol: IPv4

The mpio-parameters command succeeded.

Warning: No IPv4 discovery portals configured. Add the array IPv4 group IP as a discovery portal in the iSCSI Initiator.
</pre><br />上記の★で示した部分を修正する。

<pre>
[root@daesohyp01 lcrso]# rswcli --mpio-parameters --max-sessions-per-volume-slice 4 ★Max sessions per volume sliceの値を4にしてみる。

Processing mpio-parameters command...

MPIO Parameters:

Max sessions per volume slice: 4　★変わった
Max sessions per entire volume: 6
Minimum adapter speed: 1000
Default load balancing policy configuration: Least Queue Depth (LQD)
IOs Per Path: 16
Use MPIO for snapshots: Yes
Internet Protocol: IPv4

The mpio-parameters command succeeded.　
</pre>

<pre>
[root@daesohyp01 lcrso]# rswcli --mpio-parameters --max-sessions-per-entire-volume 4 ★Max sessions per entire volumenの値を4にしてみる。

Processing mpio-parameters command...

MPIO Parameters:

Max sessions per volume slice: 4
Max sessions per entire volume: 4　★変わった
Minimum adapter speed: 1000
Default load balancing policy configuration: Least Queue Depth (LQD)
IOs Per Path: 16
Use MPIO for snapshots: Yes
Internet Protocol: IPv4

The mpio-parameters command succeeded. 
</pre>

	<p>再度設定を確認<br /><pre>
[root@dae-ohyp01 ~]# rswcli --mpio-parameters

Processing mpio-parameters command...

MPIO Parameters:

Max sessions per volume slice: 4
Max sessions per entire volume: 4
Minimum adapter speed: 1000
Default load balancing policy configuration: Least Queue Depth (LQD)
IOs Per Path: 16
Use MPIO for snapshots: Yes
Internet Protocol: IPv4

The mpio-parameters command succeeded.

Warning: No IPv4 discovery portals configured. Add the array IPv4 group IP as a discovery portal in the iSCSI Initiator.

接続先のEquallogicを登録する
# rswcli --add-group-access --group-name=idc-oist0a --group-ip=10.16.24.129 ★ストレージのホスト名とIPを登録
</pre></p>


	<p>接続状態を確認<br /><pre>
[root@daesohyp02 ~]# rswcli --list-group

Processing list-group command...

Groups accessible from this computer:

Group Name: idc-oist0a
Group IP Address: 10.16.24.129
</pre></p>


	<p>iscsiadmコマンドでもストレージが見えることを確認<br /><pre>
[root@daesohyp02 ~]#  iscsiadm -m discoverydb
10.16.24.129:3260 via sendtargets　★ストレージのVIPが見えている
</pre></p>


	<p>■クォーラムディスク用iSCSIターゲットへの接続</p>


	<p>・iscsiadmコマンドでターゲットを探す<br /><pre>
[root@dae-ohyp01 ~]# iscsiadm -m discovery -t sendtargets -p 10.16.24.129
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8a390acbe-d45a6b3e9e253d0a-dae-02
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8a390acbe-d45a6b3e9e253d0a-dae-02
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8a390acbe-d45a6b3e9e253d0a-dae-02
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8a390acbe-d45a6b3e9e253d0a-dae-02
</pre></p>


	<p>・接続可能なターゲット表示<br /><pre>
[root@dae-ohyp01 ~]# iscsiadm -m node
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8a390acbe-d45a6b3e9e253d0a-dae-02
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8a390acbe-d45a6b3e9e253d0a-dae-02
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8a390acbe-d45a6b3e9e253d0a-dae-02
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8a390acbe-d45a6b3e9e253d0a-dae-02
</pre></p>


	<p>ターゲットに接続<br /><pre>
[root@dae-ohyp01 ~]# ehcmcli login --target iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
Login succeeded.  Device to mount:
/dev/eql/dae-01
</pre></p>


	<p>接続が成功すると、/dev/eql/配下にボリュームがマウントされた。<br /><pre>
[root@dae-ohyp01 ~]# ll /dev/eql/
合計 0
lrwxrwxrwx 1 root root 7  8月  1 00:56 2014 dae-01 -&gt; ../dm-9　★daes01としてマウントされた
</pre></p>


	<p>接続確認<br /><pre>
[root@dae-ohyp01 network-scripts]# iscsiadm -m session
tcp: [5] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
tcp: [6] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
tcp: [7] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01
tcp: [8] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-b9b90acbe-fc0a6b3e9ec53d0a-dae-01

★今回のケースではボリューム１つにつきNIC４つ使用なので計４つのセッションが確立されていることを確認する。
　セッションが４つ以下になっている場合は、しばらくしてから再度確認すれば大丈夫。
</pre></p>


	<p>■iSCSiボリュームの設定（クォーラムディスク）<br />★クォーラムディスク設定<br />fdisk -lコマンドを発行すると、以下のように領域が見えた。<br />1GB　　クォーラムディスク領域<br /><pre>
-------------------------------------------------------------------------------------------
/dev/sdc
/dev/sdb
/dev/sdd
/dev/sde
/dev/mapper/eql-8-661fc6-c8790acbe-97fa6b3e9fb53d0a_a
/dev/mapper/eql-8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01
-------------------------------------------------------------------------------------------
</pre></p>


	<p>DELLの資料では、「Multipath管理下の仮想デバイスが/dev/eql配下に作成されます。」とあったので<br />先程確認した/dev/eql/daes01に対してmkqdiskコマンド（クォーラムディスクを作成するコマンド）を発行した。<br /><pre>
# mkqdisk -c /dev/eql/daes01 -l daeqdisk01　★/dev/eql/daes01のデバイスに対してクォーラムディスクを設定。「daeqdisk01」というラベルを設定する。
</pre></p>


	<p>設定後に確認。クォーラムディスクとして作成されているようだが、以下コマンドで情報が表示されたりされなかったりする....<br /><pre>
[root@daesohyp01 cluster]# mkqdisk -L
mkqdisk v3.0.12.1

/dev/block/253:7:
/dev/disk/by-id/dm-name-eql-8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01:
/dev/dm-7:
/dev/eql/daes01:
/dev/mapper/eql-8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01:
        Magic:                eb7a62c2
        Label:                daeqdisk01
        Created:              Tue Jul 29 20:45:15 2014
        Host:                 daesohyp01
        Kernel Sector Size:   4096
        Recorded Sector Size: 4096
</pre></p>


	<p>■iSCSiボリュームの設定（仮想マシン）</p>


	<p>接続可能なターゲット表示</p>


<pre>
[root@daesohyp01 ~]# iscsiadm -m node
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02
10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02
</pre>

	<p>ターゲットに接続<br /><pre>
[root@daesohyp01 ~]# iscsiadm -m node --target iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02 --login
Logging in to [iface: eql.eth3, target: iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02, portal: 10.16.24.129,3260] (multiple)
Logging in to [iface: eql.eth7, target: iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02, portal: 10.16.24.129,3260] (multiple)
Logging in to [iface: eql.eth2, target: iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02, portal: 10.16.24.129,3260] (multiple)
Logging in to [iface: eql.eth6, target: iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02, portal: 10.16.24.129,3260] (multiple)
Login to [iface: eql.eth3, target: iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02, portal: 10.16.24.129,3260] successful.
Login to [iface: eql.eth7, target: iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02, portal: 10.16.24.129,3260] successful.
Login to [iface: eql.eth2, target: iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02, portal: 10.16.24.129,3260] successful.
Login to [iface: eql.eth6, target: iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02, portal: 10.16.24.129,3260] successful.
</pre></p>


	<p>マウント確認<br /><pre>
[root@daesohyp01 ~]# ll /dev/eql/
合計 0
lrwxrwxrwx 1 root root 7  7月 29 21:18 2014 daes01 -&gt; ../dm-7
lrwxrwxrwx 1 root root 7  7月 30 19:50 2014 daes02 -&gt; ../dm-9　★daes02としてマウントされた
</pre></p>


	<p>接続確認<br /><pre>
[root@daesohyp01 ~]# iscsiadm -m session
tcp: [13] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01
tcp: [14] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01
tcp: [15] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01
tcp: [16] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-c8790acbe-97fa6b3e9fb53d0a-daes01
tcp: [17] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02　★接続されている
tcp: [18] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02　★
tcp: [19] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02　★
tcp: [20] 10.16.24.129:3260,1 iqn.2001-05.com.equallogic:8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02　★
</pre></p>


	<p>fdiskで確認してみる<br /><pre>
[root@daesohyp01 ~]# fdisk -l
～省略～
ディスク /dev/sdh: 322.1 GB, 322122547200 バイト
ヘッド 255, セクタ 63, シリンダ 4895
Units = シリンダ数 of 16065 * 4096 = 65802240 バイト
セクタサイズ (論理 / 物理): 4096 バイト / 4096 バイト
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
ディスク識別子: 0x23575571

デバイス ブート      始点        終点     ブロック   Id  システム
注意: セクタサイズが 4096 です (512 ではなく)

ディスク /dev/sdg: 322.1 GB, 322122547200 バイト
ヘッド 255, セクタ 63, シリンダ 4895
Units = シリンダ数 of 16065 * 4096 = 65802240 バイト
セクタサイズ (論理 / 物理): 4096 バイト / 4096 バイト
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
ディスク識別子: 0x23575571

デバイス ブート      始点        終点     ブロック   Id  システム
注意: セクタサイズが 4096 です (512 ではなく)

ディスク /dev/sdi: 322.1 GB, 322122547200 バイト
ヘッド 255, セクタ 63, シリンダ 4895
Units = シリンダ数 of 16065 * 4096 = 65802240 バイト
セクタサイズ (論理 / 物理): 4096 バイト / 4096 バイト
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
ディスク識別子: 0x23575571

デバイス ブート      始点        終点     ブロック   Id  システム
注意: セクタサイズが 4096 です (512 ではなく)

ディスク /dev/sdf: 322.1 GB, 322122547200 バイト
ヘッド 255, セクタ 63, シリンダ 4895
Units = シリンダ数 of 16065 * 4096 = 65802240 バイト
セクタサイズ (論理 / 物理): 4096 バイト / 4096 バイト
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
ディスク識別子: 0x23575571

デバイス ブート      始点        終点     ブロック   Id  システム
注意: セクタサイズが 4096 です (512 ではなく)

ディスク /dev/mapper/eql-8-661fc6-8d390acbe-345a6b3e9e553d0a_a: 322.1 GB, 322122547200 バイト
ヘッド 255, セクタ 63, シリンダ 4895
Units = シリンダ数 of 16065 * 4096 = 65802240 バイト
セクタサイズ (論理 / 物理): 4096 バイト / 4096 バイト
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
ディスク識別子: 0x23575571

                                          デバイス ブート      始点        終点     ブロック   Id  システム
注意: セクタサイズが 4096 です (512 ではなく)

ディスク /dev/mapper/eql-8-661fc6-8d390acbe-345a6b3e9e553d0a-daes02: 322.1 GB, 322122547200 バイト
ヘッド 255, セクタ 63, シリンダ 4895
Units = シリンダ数 of 16065 * 4096 = 65802240 バイト
セクタサイズ (論理 / 物理): 4096 バイト / 4096 バイト
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
ディスク識別子: 0x23575571

                                           デバイス ブート      始点        終点     ブロック   Id  システム

★/dev/sdf～/dev/sdiの４つのデバイスが認識されている。
</pre></p>


	<p>ボリューム作成</p>


	<p>★fdisk /dev/eql/daes02からボリューム作成を試みるも、作成はされるがpvcreateにてLVM物理ボリュームとしての登録ができなかった。<br />　よって、fdisk /dev/eql/daes02からボリューム作成は行わず、pvcreateで/dev/eql/daes02をそのまま指定した。</p>


	<p>VG、LV作成<br />通常通り。</p>


<hr />


	<a name="マルチパス設定_Equallogic-Group-Access設定"></a>
<h2 >Equallogic Group-Access設定<a href="#マルチパス設定_Equallogic-Group-Access設定" class="wiki-anchor">&para;</a></h2>


	<p>・groupname:[rswcli -l]コマンドで出力されるグループ名<br />・username :Equallogiの管理画面にアクセスする際のユーザ名<br />・ipaddress:Equallogic管理画面のIP<br />・password :Equallogiの管理画面にアクセスする際のユーザパスワード<br /><pre>
[root@dae-ohyp01 ~]# asmcli create group-access --name idc-oist0a --user-name grpadmin --ip-address 10.16.17.129
Dell EqualLogic Auto-Snapshot Manager CLI Version 1.3.0 Build 388434
Copyright (c) 2010-2014 Dell Inc.

Password for user grpadmin on idc-oist0a: ***********
Successfully created credentials record in file /etc/equallogic/asm-group-access.
</pre></p>


	<a name="マルチパス設定_グループアクセス設定の確認"></a>
<h2 >グループアクセス設定の確認<a href="#マルチパス設定_グループアクセス設定の確認" class="wiki-anchor">&para;</a></h2>


<pre>
[root@dae-ohyp01 ~]# asmcli list group-access
Dell EqualLogic Auto-Snapshot Manager CLI Version 1.3.0 Build 388434
Copyright (c) 2010-2014 Dell Inc.

========================================================
Group Access
========================================================

Group name:     idc-oist0a
IP address:     10.16.17.129
User names:     grpadmin

1 Group defined.
</pre>

	<p>OS設定をEquallogic用に最適化</p>


	<p>現在の状態を確認<br /><pre>
[root@dae-ohyp01 ~]# eqltune
Dell EqualLogic 'eqltune' version 1.3.0 Build 388434
Copyright (c) 2010-2014 Dell Inc.

Checking your Linux system for optimal iSCSI performance...

Block Devices            Critical    Warnings    Suggestions    Ok
-------------------------------------------------------------------
EqualLogic (8)                0           0            0         8

Sysctl Tunables
-------------------------------------------------------------------
ARP Flux                      0           0            0         8
RP Filter                     0           0            0         4
Network Buffers               0           0           10         0
Scheduler                     0           0            1         0

Ethernet Devices
-------------------------------------------------------------------
eth2                          0           0            0         3
eth3                          0           0            0         3
eth6                          0           0            0         3
eth7                          0           0            0         3

iSCSI Settings
-------------------------------------------------------------------
iscsid.conf defaults          0           4            1         2

External Utility Settings
-------------------------------------------------------------------
Blacklists                    0           0            0         1

EqualLogic Host Tools
-------------------------------------------------------------------
Running system checks         0           0            0         2
eqlvolume checks              0           0            0         3

Run in verbose mode (eqltune -v) for more details and instructions on how
to adjust your settings.
</pre></p>


	<p>設定すべき各パラメータを確認<br /><pre>
[root@dae-ohyp01 ~]# eqltune -v
Dell EqualLogic 'eqltune' version 1.3.0 Build 388434
Copyright (c) 2010-2014 Dell Inc.

Checking your Linux system for optimal iSCSI performance...

======================================================================
Block Devices
======================================================================
To make BlockIO settings persist across reboots, our HIT kit has already
installed a default set of udev rules in:
  /etc/udev/rules.d/99-eqlsd.rules

sdb
---
/sys/block/sdb/device/timeout=60 is ok

Total for sdb: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok ★OK

sdc
---
/sys/block/sdc/device/timeout=60 is ok

Total for sdc: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok ★OK

sdd
---
/sys/block/sdd/device/timeout=60 is ok

Total for sdd: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok ★OK

sde
---
/sys/block/sde/device/timeout=60 is ok

Total for sde: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok ★OK

sdf
---
/sys/block/sdf/device/timeout=60 is ok

Total for sdf: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok ★OK

sdg
---
/sys/block/sdg/device/timeout=60 is ok

Total for sdg: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok ★OK

sdh
---
/sys/block/sdh/device/timeout=60 is ok

Total for sdh: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok ★OK

sdi
---
/sys/block/sdi/device/timeout=60 is ok

Total for sdi: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok ★OK

======================================================================
Sysctl Tunables
======================================================================
To make sysctl values persistent across reboots, they can be added
to /etc/sysctl.conf

ARP Flux
--------
IPv4 ARP ignore: A value of 1 means NICs will ignore any ARP requests
not directed explicitly to them.  The default value of 0 will allow NICs to
reply to ARP requests on ANY other NIC, which can provide false positives
to externally-originating ping tests, as well as unexpected ARP resets.

IPv4 ARP announce: A value of 2 means NICs will only announce their own IP
address, not the address of other NICs on the same subnet. Any other value
can cause ARP flux or inconsistent caches out on the network in a
multiple-NIC-per-subnet configuration.

The actual value computed by Linux is the maximum of the interface-specific
value (net.ipv4.conf.$NIC.setting) and the global override value
(net.ipv4.conf.all.setting), so set your values accordingly in
/etc/sysctl.conf.  An easy solution is to add these lines:
  # Prevent ARP Flux for multiple NICs on the same subnet:
  net.ipv4.conf.all.arp_ignore = 1
  net.ipv4.conf.all.arp_announce = 2

If you have set up source-based routing and have set up arp filtering instead
(net.ipv4.conf.*.arp_filter), you can disregard this section.

net/ipv4/conf/eth2/arp_ignore=1 is ok

net/ipv4/conf/eth2/arp_announce=2 is ok

net/ipv4/conf/eth3/arp_ignore=1 is ok

net/ipv4/conf/eth3/arp_announce=2 is ok

net/ipv4/conf/eth6/arp_ignore=1 is ok

net/ipv4/conf/eth6/arp_announce=2 is ok

net/ipv4/conf/eth7/arp_ignore=1 is ok

net/ipv4/conf/eth7/arp_announce=2 is ok

Total for ARP Flux: 0/8 critical, 0/8 warnings, 0/8 suggestions, 8/8 ok ★OK

RP Filter
---------
IPv4 Return Path Filtering: A value of 0 disables reverse path filtering
and a value of 2 enables loose reverse path filtering, either of which
allows all inbound packets to be processed by the stack in a multiple-NIC-
per-subnet configuration.

The actual value used by Linux is the maximum of the interface-specific
value (net.ipv4.conf.$NIC.setting) and the global override value
(net.ipv4.conf.all.setting), so set your values accordingly in
/etc/sysctl.conf.  An easy solution is to add these lines:
  # Loosen RP Filter to alow multiple iSCSI connections
  net.ipv4.conf.all.rp_filter = 2

net/ipv4/conf/eth2/rp_filter=2 is ok

net/ipv4/conf/eth3/rp_filter=2 is ok

net/ipv4/conf/eth6/rp_filter=2 is ok

net/ipv4/conf/eth7/rp_filter=2 is ok

Total for RP Filter: 0/4 critical, 0/4 warnings, 0/4 suggestions, 4/4 ok ★OK

Network Buffers
---------------
Increasing the default and maximum network buffer sizes will provide
better performance, especially on systems with large RAM resources.

Suggestion: net.core.rmem_default=124928 is ok, but may be tuned to any value greater than 129024　★変更を推奨
    Default socket RX buffer size

Suggestion: net.core.rmem_max=124928 is ok, but may be tuned to any value greater than 131071　★変更を推奨
    Maximum socket RX buffer size

Suggestion: net.core.wmem_default=124928 is ok, but may be tuned to any value greater than 129024　★変更を推奨
    Default socket TX buffer size

Suggestion: net.core.wmem_max=124928 is ok, but may be tuned to any value greater than 131071　★変更を推奨
    Maximum socket TX buffer size

Suggestion: net.ipv4.tcp_rmem[0]=4096 is ok, but may be tuned to any value greater than 4096　★変更を推奨
    IPv4 TCP minimum socket RX buffer size

Suggestion: net.ipv4.tcp_rmem[1]=87380 is ok, but may be tuned to any value greater than 87380　★変更を推奨
    IPv4 TCP default socket RX buffer size

Suggestion: net.ipv4.tcp_rmem[2]=4194304 is ok, but may be tuned to any value greater than 4194304　★変更を推奨
    IPv4 TCP maximum socket RX buffer size

Suggestion: net.ipv4.tcp_wmem[0]=4096 is ok, but may be tuned to any value greater than 4096　★変更を推奨
    IPv4 TCP minimum socket TX buffer size

Suggestion: net.ipv4.tcp_wmem[1]=16384 is ok, but may be tuned to any value greater than 16384　★変更を推奨
    IPv4 TCP default socket TX buffer size

Suggestion: net.ipv4.tcp_wmem[2]=4194304 is ok, but may be tuned to any value greater than 4194304　★変更を推奨
    IPv4 TCP maximum socket TX buffer size

Total for Network Buffers: 0/10 critical, 0/10 warnings, 10/10 suggestions, 0/10 ok

Scheduler
---------
Suggestion: kernel.sched_compat_yield=0 is ok, but may be tuned to one of ['0', '1']
    Setting this to 1 will grant more CPU to high-CPU tasks, at the cost
    of other non-CPU intensive tasks.

Total for Scheduler: 0/1 critical, 0/1 warnings, 1/1 suggestions, 0/1 ok

======================================================================
Ethernet Devices
======================================================================

eth2
----
eth2 Generic Receive Offload=off is ok

eth2 Flow control=on is ok

eth2 MTU=9000 is ok

Total for eth2: 0/3 critical, 0/3 warnings, 0/3 suggestions, 3/3 ok　★OK

eth3
----
eth3 Generic Receive Offload=off is ok

eth3 Flow control=on is ok

eth3 MTU=9000 is ok

Total for eth3: 0/3 critical, 0/3 warnings, 0/3 suggestions, 3/3 ok　★OK

eth6
----
eth6 Generic Receive Offload=off is ok

eth6 Flow control=on is ok

eth6 MTU=9000 is ok

Total for eth6: 0/3 critical, 0/3 warnings, 0/3 suggestions, 3/3 ok　★OK

eth7
----
eth7 Generic Receive Offload=off is ok

eth7 Flow control=on is ok

eth7 MTU=9000 is ok

Total for eth7: 0/3 critical, 0/3 warnings, 0/3 suggestions, 3/3 ok　★OK

======================================================================
iSCSI Settings
======================================================================
The default settings in /etc/iscsi/iscsid.conf are propagated to
individual nodes on discovery or redescovery.

The following command will re-discover all existing nodes (Warning: resets
any per-node settings back to the defaults in iscsid.conf, including
'node.startup'):
  iscsiadm -m discovery -t st -p &lt;portal&gt;

iscsid.conf defaults
--------------------
These settings must be manually edited in /etc/iscsi/iscsid.conf, but
will only take effect for newly-discovered nodes.

Warning: node.startup=automatic, should be manual　　　　　　　　　　　　　　　　★変更を推奨
    If node.startup is 'automatic', ALL discovered nodes will be logged
    in at boot.  If by default this is 'manual', you can designate only those
    nodes you actually want to auto-login on a case-by-case basis with ehcmcli:
      ehcmcli login --login-at-boot --target &lt;target&gt; [--portal &lt;portal&gt;]

node.session.iscsi.FastAbort=No is ok

Warning: node.session.initial_login_retry_max=8, should be 12　　　★変更を推奨
    More retries will make it more likely that login will succeed at boot,
    at the cost of a slighly longer time to actually fail.

Suggestion: node.conn[0].iscsi.MaxRecvDataSegmentLength=262144 is ok, but may be tuned to between 65536 and 524288
    A lower value improves latency at the cost of higher IO throughput

Warning: node.session.cmds_max=128, should be 1024　　　　　★変更を推奨
    Maximum number of queued iSCSI commands per session.
    Must be an even power of 2.

Warning: node.session.queue_depth=32, should be 128　　★変更を推奨
    The device queue depth

node.conn[0].timeo.noop_out_interval=5 is ok

Total for iscsid.conf defaults: 0/7 critical, 4/7 warnings, 1/7 suggestions, 2/7 ok

======================================================================
External Utility Settings
======================================================================
A collection of miscellaneous settings in important external utilities, such
as LVM and Multipathd.

Blacklists
----------
LVM device filter=set is ok

Total for Blacklists: 0/1 critical, 0/1 warnings, 0/1 suggestions, 1/1 ok

======================================================================
EqualLogic Host Tools
======================================================================
This summarizes whether the EqualLogic host tools have detected warnings
or errors on the running system.

Running system checks
---------------------
To see more details and an explanation of how to fix these runtime issues,
please issue the command:
  ehcmcli status

dm-switch module=present is ok

ehcmd warnings=none is ok

Total for Running system checks: 0/2 critical, 0/2 warnings, 0/2 suggestions, 2/2 ok

eqlvolume checks
----------------
The eqlvolume utility locates mount points that are good candidates for rethinning
and locates mount points that have been incorrectly mounted with the "-o discard" 
option.  For more information about these issues, please enter:
  eqlvolume help

eqlvolume group access check=0 is ok

eqlvolume rethin recommendations=0 is ok

eqlvolume discard usage errors=0 is ok

Total for eqlvolume checks: 0/3 critical, 0/3 warnings, 0/3 suggestions, 3/3 ok

======================================================================
Overall: 0/56 critical, 4/56 warnings, 12/56 suggestions, 40/56 ok
======================================================================
</pre></p>


	<p>■以下追記および変更</p>


	<p>/etc/sysctl.conf<br /><pre>
以下を追記
★以下の値はちゃんと計算して入れる必要あり。
----------------------------------------------------------------------
### For Equallogic Setiings ###
net.core.rmem_default = 129024
net.core.rmem_max = 131071
net.core.wmem_default = 129024
net.core.wmem_max = 131071
net.ipv4.tcp_rmem[0] = 4096
net.ipv4.tcp_rmem[1] = 87380
net.ipv4.tcp_rmem[2] = 4194304
net.ipv4.tcp_wmem[0] = 4096
net.ipv4.tcp_wmem[1] = 16384
net.ipv4.tcp_wmem[2] = 4194304
### END Settings ###

### /proc/sys/kernel/sched_compat_yield Setting for Equallogic ###
kernel.sched_compat_yield = 1
### END of Settings ###
-------------------------------------------------------------
</pre></p>


	<p>iscsid.confの各値を変更<br />/etc/iscsi/iscsid.conf<br /><pre>
-------------------------------------------------------------------------------
#node.startup = automatic
node.startup = manual

#node.session.initial_login_retry_max = 8
node.session.initial_login_retry_max = 12

#node.session.cmds_max = 128
node.session.cmds_max = 1024

#node.session.queue_depth = 32
node.session.queue_depth = 128
---------------------------------------------------------------------------------
</pre></p>


	<p>iscsi.confでのOS起動時iscsiターゲット自動接続を無効にしたので、equallogic側で自動接続を制御する。<br />以下のコマンドを実行<br /><pre>
[root@daesohyp01 ~]# ehcmcli login --login-at-boot --target iqn.2001-05.com.equallogic:8-661fc68790acbe-97fa6b3e9fb53d0a-daes01 --portal 10.16.24.129:3260,1
Login succeeded.  Device to mount:
/dev/eql/daes01

[root@daesohyp01 ~]# ehcmcli login --login-at-boot --target iqn.2001-05.com.equallogic:8-661fc6d390acbe-345a6b3e9e553d0a-daes02 --portal 10.16.24.129:3260,1
Login succeeded.  Device to mount:
/dev/eql/daes02
</pre></p>
<hr />
<a name="ログサーバ設計" />
<a name="ログサーバ設計_ログサーバ設計"></a>
<h1 >ログサーバ設計<a href="#ログサーバ設計_ログサーバ設計" class="wiki-anchor">&para;</a></h1>


	<p>idc-onas01にログサーバを構築する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>ファシリティ</th>
			<th>用途</th>
			<th>ログ出力先</th>
		</tr>
		<tr>
			<td>LOCAL0</td>
			<td>GWルータ</td>
			<td>/var/log/idc-oprt/</td>
		</tr>
		<tr>
			<td>LOCAL1</td>
			<td>FW</td>
			<td>/var/log/idc-opfw</td>
		</tr>
		<tr>
			<td>LOCAL2</td>
			<td>LB</td>
			<td>/var/log/idc-plb</td>
		</tr>
		<tr>
			<td>LOCAL3</td>
			<td>L3SW</td>
			<td>/var/log/idc-ol3s</td>
		</tr>
		<tr>
			<td>LOCAL4</td>
			<td>L2SW</td>
			<td>/var/log/idc-ol2s</td>
		</tr>
		<tr>
			<td>LOCAL5</td>
			<td>仮想ルータ</td>
			<td>/var/log/idc-ovrt</td>
		</tr>
		<tr>
			<td>LOCAL6</td>
			<td>zabbix proxy</td>
			<td>/var/log/idc-ompx</td>
		</tr>
		<tr>
			<td>LOCAL7</td>
			<td>-(boot.logで利用されているため除外)</td>
			<td>-</td>
		</tr>
	</table>




	<a name="ログサーバ設計_ログローテート"></a>
<h2 >ログローテート<a href="#ログサーバ設計_ログローテート" class="wiki-anchor">&para;</a></h2>


	<a name="ログサーバ設計_RSYSLOGサーバ設定"></a>
<h3 >RSYSLOGサーバ設定<a href="#ログサーバ設計_RSYSLOGサーバ設定" class="wiki-anchor">&para;</a></h3>


	<ul>
	<li>受信したログは以下のルールで振り分けるホストごとに分ける</li>
	</ul>


	<p>仮想ルータ、zabbix proxy等はファシリティを設定できないため、<br />全てのログをリモートログとして設定する。</p>


<pre>
$template remotelog,"/var/log/remotelog/%HOSTNAME%/%$year%%$month%%$day%_%syslogfacility-text%.log" 
*.*                                            ?remotelog
</pre>

	<ul>
	<li>messagesにlocal1を出力しないようにする</li>
	</ul>


<pre>
*.info;mail.none;authpriv.none;cron.none;local1.none;local6.none;daemon.none;user.none       /var/log/messages
</pre>

	<ul>
	<li>UDPにて待ち受けをするように設定</li>
	</ul>


<pre>
$ModLoad imudp
$UDPServerRun 514
</pre>

	<ul>
	<li>アクセス制限設定</li>
	</ul>


<pre>
$AllowedSender UDP, 127.0.0.1, 10.16.33.0/24, 10.16.38.0/24, 10.16.39.0/24
</pre>
<hr />
<a name="仮想インスタンス運用手順" />
<a name="仮想インスタンス運用手順_仮想インスタンス運用手順"></a>
<h1 >仮想インスタンス運用手順<a href="#仮想インスタンス運用手順_仮想インスタンス運用手順" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#仮想インスタンス運用手順_仮想インスタンス運用手順">仮想インスタンス運用手順</a></li><li><a href="#仮想インスタンス運用手順_Introduction">●Introduction</a><ul><li><a href="#仮想インスタンス運用手順_目的">目的</a></li></ul>
</li><li><a href="#仮想インスタンス運用手順_構成">●構成</a><ul><li><a href="#仮想インスタンス運用手順_パーティション構成">パーティション構成</a></li><li><a href="#仮想インスタンス運用手順_導入パッケージ">導入パッケージ</a></li><li><a href="#仮想インスタンス運用手順_システムサービス一覧">システムサービス一覧</a></li><li><a href="#仮想インスタンス運用手順_ポートアサイン">ポートアサイン</a></li><li><a href="#仮想インスタンス運用手順_selinux">selinux</a></li><li><a href="#仮想インスタンス運用手順_rootパスワード">rootパスワード</a></li><li><a href="#仮想インスタンス運用手順_zabbix-agent設定">zabbix agent設定</a></li></ul>
</li><li><a href="#仮想インスタンス運用手順_運用手順">●運用手順</a><ul><li><a href="#仮想インスタンス運用手順_準備">準備</a></li><li><a href="#仮想インスタンス運用手順_作成">作成</a><ul><li><a href="#仮想インスタンス運用手順_事業者の仮想インスタンス">事業者の仮想インスタンス</a></li><li><a href="#仮想インスタンス運用手順_事業者のzabbix-proxy">事業者のzabbix proxy</a></li><li><a href="#仮想インスタンス運用手順_事業者の仮想ルータ">事業者の仮想ルータ</a><ul><li><a href="#仮想インスタンス運用手順_仮想ルータの初期化">仮想ルータの初期化</a></li></ul>
</li></ul>
</li><li><a href="#仮想インスタンス運用手順_起動">起動</a></li><li><a href="#仮想インスタンス運用手順_停止">停止</a></li><li><a href="#仮想インスタンス運用手順_再起動">再起動</a></li><li><a href="#仮想インスタンス運用手順_強制停止">強制停止</a></li><li><a href="#仮想インスタンス運用手順_ライブマイグレーション">ライブマイグレーション</a></li><li><a href="#仮想インスタンス運用手順_バックアップの作成">バックアップの作成</a></li><li><a href="#仮想インスタンス運用手順_復旧">復旧</a></li></ul></li></ul>


<hr />


	<a name="仮想インスタンス運用手順_Introduction"></a>
<h1 >●Introduction<a href="#仮想インスタンス運用手順_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="仮想インスタンス運用手順_目的"></a>
<h2 >目的<a href="#仮想インスタンス運用手順_目的" class="wiki-anchor">&para;</a></h2>


	<p>本書はKVMサーバで提供する仮想インスタンスの運用について記述する。</p>


<hr />


	<a name="仮想インスタンス運用手順_構成"></a>
<h1 >●構成<a href="#仮想インスタンス運用手順_構成" class="wiki-anchor">&para;</a></h1>


	<a name="仮想インスタンス運用手順_パーティション構成"></a>
<h2 >パーティション構成<a href="#仮想インスタンス運用手順_パーティション構成" class="wiki-anchor">&para;</a></h2>


	<p>パーティション構成を以下に示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>デバイスファイル</th>
			<th>マウントポイント</th>
			<th>LVM</th>
			<th>FSタイプ</th>
			<th>サイズ</th>
			<th>備考</th>
		</tr>
		<tr>
			<td>/dev/sda1</td>
			<td>/boot</td>
			<td>-</td>
			<td>ext4</td>
			<td>500MB</td>
			<td>centosは500MBの割り当てを推奨している</td>
		</tr>
		<tr>
			<td>tmpfs</td>
			<td>/dev/shm</td>
			<td>-</td>
			<td>tmpfs</td>
			<td>メモリ容量に依存</td>
			<td><strong>※TODO 利用しないためfstabから除外しマウントしない方がよい</strong></td>
		</tr>
		<tr>
			<td>/dev/mapper/vg_system-lv_swap</td>
			<td>-</td>
			<td>○</td>
			<td>swap</td>
			<td>2GB</td>
			<td>メモリが多い程退避場所であるswapは必要となるがswap outする機会も減るため、無限に増加させる必要はない</td>
		</tr>
		<tr>
			<td>/dev/mapper/vg_system-lv_root</td>
			<td>/</td>
			<td>○</td>
			<td>ext4</td>
			<td>17GB</td>
			<td>上記以外の全てを割り当てる</td>
		</tr>
	</table>




	<p>以下に作成するボリューム領域を示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>ボリュームグループ名</th>
		</tr>
		<tr>
			<td>vg_system</td>
		</tr>
	</table>




	<a name="仮想インスタンス運用手順_導入パッケージ"></a>
<h2 >導入パッケージ<a href="#仮想インスタンス運用手順_導入パッケージ" class="wiki-anchor">&para;</a></h2>


	<p>サーバの構成はMinimalを選択する。<br />その他のパッケージを以下に示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>パッケージ名</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>acpid</td>
			<td>kvmからshutdownコマンド発行のために必要</td>
		</tr>
		<tr>
			<td>zabbix</td>
			<td>監視を行う場合は導入する</td>
		</tr>
		<tr>
			<td>zabbix-agent</td>
			<td>監視を行う場合は導入する。また設定等を入れ込む必要がある</td>
		</tr>
	</table>




	<p>また、yum updateを実施し2015/05/13の最新パッケージに更新する。</p>


	<a name="仮想インスタンス運用手順_システムサービス一覧"></a>
<h2 >システムサービス一覧<a href="#仮想インスタンス運用手順_システムサービス一覧" class="wiki-anchor">&para;</a></h2>


	<p>サービスはほぼデフォルト設定を用いる。<br />別途インストールするacpidは以下の設定にする。</p>


<pre>
acpid              0:off    1:off    2:on    3:on    4:on    5:on    6:off
### 監視設定を有効にする場合は以下も導入する
zabbix-agent    0:off   1:off   2:on    3:on    4:on    5:on    6:off
</pre>

	<a name="仮想インスタンス運用手順_ポートアサイン"></a>
<h2 >ポートアサイン<a href="#仮想インスタンス運用手順_ポートアサイン" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>インターフェイス</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>eth0</td>
			<td>ユーザ用デフォルトネットワーク。</td>
		</tr>
	</table>




	<p>デフォルトでdhcpの設定にしてある。<br />dhcp側で固定IPアドレスを割り当てるようにしているため、<br />dhcp利用を推奨する。</p>


	<p>dhcpからdns, ntp等の情報が取得できる。</p>


	<a name="仮想インスタンス運用手順_selinux"></a>
<h2 >selinux<a href="#仮想インスタンス運用手順_selinux" class="wiki-anchor">&para;</a></h2>


	<p>デフォルトで無効にする。</p>


	<a name="仮想インスタンス運用手順_rootパスワード"></a>
<h2 >rootパスワード<a href="#仮想インスタンス運用手順_rootパスワード" class="wiki-anchor">&para;</a></h2>


	<p>パスワードは以下を設定する。</p>


	<ul>
	<li>5tpZdmazFW</li>
	</ul>


	<a name="仮想インスタンス運用手順_zabbix-agent設定"></a>
<h2 >zabbix agent設定<a href="#仮想インスタンス運用手順_zabbix-agent設定" class="wiki-anchor">&para;</a></h2>


	<p>仮想インスタンスにzabbix agentは必須ではないが、<br />導入した場合の設定ファイルを以下に示す。</p>


	<ul>
	<li>/etc/zabbix/zabbix_agentd.conf</li>
	</ul>


	<p>ここで指定するIPアドレスは仮想インスタンスが持つ<br />、いづれかのインターフェイスのネットワークアドレスに第四オクテットを253にしたものである。</p>


<pre>
PidFile=/var/run/zabbix/zabbix_agentd.pid
LogFile=/var/log/zabbix/zabbix_agentd.log
LogFileSize=0
Server=[ネットワークアドレス].253
ServerActive=[ネットワークアドレス].253
HostnameItem=system.hostname
HostMetadataItem=system.hostname
AllowRoot=1
Include=/etc/zabbix/zabbix_agentd.d/
</pre>

	<p>zabbix_agentのログが肥大化しすぎないように<br />ログローテートの設定を追加する。</p>


	<ul>
	<li>/etc/logrotate.d/zabbix-agent</li>
	</ul>


<pre>
/var/log/zabbix/zabbix_agentd.log {
    daily
    create 664 zabbix zabbix
    rotate 7
    size 5M
    ifempty
    missingok
    dateext
    compress
}
</pre>

<hr />


	<a name="仮想インスタンス運用手順_運用手順"></a>
<h1 >●運用手順<a href="#仮想インスタンス運用手順_運用手順" class="wiki-anchor">&para;</a></h1>


	<p>以下の手順は明記しない限りKVMサーバで実行するものとする。</p>


	<a name="仮想インスタンス運用手順_準備"></a>
<h2 >準備<a href="#仮想インスタンス運用手順_準備" class="wiki-anchor">&para;</a></h2>


	<p>まずは仮想インスタンス作成前に所属する事業者ID等の情報を整理する。<br />NFS上の/export/cloconフォルダにスクリプトが用意してあり、各種情報を取得できるようにしている。<br />以降、各種情報を取得するスクリプトを説明する。</p>


	<ul>
	<li>macgen.py:仮想インスタンスに利用するMACアドレスを作成する。
<a class="collapsible collapsed" href="#" id="collapse-c821f9c0-show" onclick="$(&#x27;#collapse-c821f9c0-show, #collapse-c821f9c0-hide&#x27;).toggle(); $(&#x27;#collapse-c821f9c0&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-c821f9c0-hide" onclick="$(&#x27;#collapse-c821f9c0-show, #collapse-c821f9c0-hide&#x27;).toggle(); $(&#x27;#collapse-c821f9c0&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-c821f9c0" style="display:none;"><pre>
### 先頭24bit分は固定
# ./macgen.py
52:54:00:6f:6b:d8
</pre></div></li>
		<li>createMacAddress.sh:既存のxmlを参照し重複のないMACアドレスを作成する。
<a class="collapsible collapsed" href="#" id="collapse-a0979c49-show" onclick="$(&#x27;#collapse-a0979c49-show, #collapse-a0979c49-hide&#x27;).toggle(); $(&#x27;#collapse-a0979c49&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-a0979c49-hide" onclick="$(&#x27;#collapse-a0979c49-show, #collapse-a0979c49-hide&#x27;).toggle(); $(&#x27;#collapse-a0979c49&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-a0979c49" style="display:none;"><pre>
# ./createMacAddress.sh
52:54:00:6f:6b:d8
</pre></div></li>
		<li>printCompanyVlan.sh:事業者IDに割り当てられたvlan idを出力する。
<a class="collapsible collapsed" href="#" id="collapse-e6f42d2b-show" onclick="$(&#x27;#collapse-e6f42d2b-show, #collapse-e6f42d2b-hide&#x27;).toggle(); $(&#x27;#collapse-e6f42d2b&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-e6f42d2b-hide" onclick="$(&#x27;#collapse-e6f42d2b-show, #collapse-e6f42d2b-hide&#x27;).toggle(); $(&#x27;#collapse-e6f42d2b&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-e6f42d2b" style="display:none;"><pre>
# ./printCompanyVlan.sh 13
104 105 106 107 108 109 110 111 2152 2153 2154 2155 2156 2157 2158 2159
</pre></div></li>
		<li>printCompanyNwAddress.sh:事業者IDに割り当てられたネットワークアドレスを出力する。
<a class="collapsible collapsed" href="#" id="collapse-97485d7d-show" onclick="$(&#x27;#collapse-97485d7d-show, #collapse-97485d7d-hide&#x27;).toggle(); $(&#x27;#collapse-97485d7d&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-97485d7d-hide" onclick="$(&#x27;#collapse-97485d7d-show, #collapse-97485d7d-hide&#x27;).toggle(); $(&#x27;#collapse-97485d7d&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-97485d7d" style="display:none;"><pre>
# ./printCompanyNwAddress.sh 13
10.16.208.0/24
10.16.209.0/24
10.16.210.0/24
10.16.211.0/24
10.16.212.0/24
10.16.213.0/24
10.16.214.0/24
10.16.215.0/24
10.16.216.0/24
10.16.217.0/24
10.16.218.0/24
10.16.219.0/24
10.16.220.0/24
10.16.221.0/24
10.16.222.0/24
10.16.223.0/24
</pre></div></li>
		<li>printCompanyVlanNwAddress.sh:事業者IDに割り当てられたvlan idに対応するネットワークアドレスを出力する。
<a class="collapsible collapsed" href="#" id="collapse-65f9b088-show" onclick="$(&#x27;#collapse-65f9b088-show, #collapse-65f9b088-hide&#x27;).toggle(); $(&#x27;#collapse-65f9b088&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-65f9b088-hide" onclick="$(&#x27;#collapse-65f9b088-show, #collapse-65f9b088-hide&#x27;).toggle(); $(&#x27;#collapse-65f9b088&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-65f9b088" style="display:none;"><pre>
### vlan id直後に出力されているネットワークアドレスが対応するものである。
# ./printCompanyVlanNwAddress.sh 13
104
10.16.208.0/24
105
10.16.209.0/24
106
10.16.210.0/24
107
10.16.211.0/24
108
10.16.212.0/24
109
10.16.213.0/24
110
10.16.214.0/24
111
10.16.215.0/24
2152
10.16.216.0/24
2153
10.16.217.0/24
2154
10.16.218.0/24
2155
10.16.219.0/24
2156
10.16.220.0/24
2157
10.16.221.0/24
2158
10.16.222.0/24
2159
10.16.223.0/24
</pre></div></li>
		<li>printCompanyDefaultVlanNwAddress.sh:事業者IDに割り当てられたユーザ用デフォルトネットワーク(インターネットに接続可能なネットワーク)のvlan idに対応するネットワークアドレスを出力する。
<a class="collapsible collapsed" href="#" id="collapse-77f0c001-show" onclick="$(&#x27;#collapse-77f0c001-show, #collapse-77f0c001-hide&#x27;).toggle(); $(&#x27;#collapse-77f0c001&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-77f0c001-hide" onclick="$(&#x27;#collapse-77f0c001-show, #collapse-77f0c001-hide&#x27;).toggle(); $(&#x27;#collapse-77f0c001&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-77f0c001" style="display:none;"><pre>
# ./printCompanyDefaultVlanNwAddress.sh 13
107
10.16.211.0/24
</pre></div></li>
		<li>printAllDomains.sh:各KVMサーバで登録されている全てのインスタンスを表示する。
<a class="collapsible collapsed" href="#" id="collapse-c56574ee-show" onclick="$(&#x27;#collapse-c56574ee-show, #collapse-c56574ee-hide&#x27;).toggle(); $(&#x27;#collapse-c56574ee&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-c56574ee-hide" onclick="$(&#x27;#collapse-c56574ee-show, #collapse-c56574ee-hide&#x27;).toggle(); $(&#x27;#collapse-c56574ee&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-c56574ee" style="display:none;"><pre>
# ./printAllDomains.sh
[idc-ohyp01]
   1    13-test    起動中
</pre></div></li>
		<li>printDomainWorkingIn.sh:各KVMサーバで指定したドメインが登録されているかを表示する。
<a class="collapsible collapsed" href="#" id="collapse-f3fec80e-show" onclick="$(&#x27;#collapse-f3fec80e-show, #collapse-f3fec80e-hide&#x27;).toggle(); $(&#x27;#collapse-f3fec80e&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-f3fec80e-hide" onclick="$(&#x27;#collapse-f3fec80e-show, #collapse-f3fec80e-hide&#x27;).toggle(); $(&#x27;#collapse-f3fec80e&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-f3fec80e" style="display:none;"><pre>
# ./printDomainWorkingIn.sh 13-test
[idc-ohyp01]
   1    13-test    起動中
</pre></div></li>
	</ul>


	<p>次に仮想インスタンス作成の際に必要なdisk、bridgeインターフェイル等の操作を行うスクリプトを説明する。</p>


	<ul>
	<li>config.sh:ctl*.shで利用するグローバルオプションが記述された設定ファイル。KVMサーバに最適化されているため、設定を変更しないこと。</li>
		<li>ctlVlan.sh:引数なしで実行することでコマンドの簡単な使用方法が表示できる。引数に指定したvlan idのbridgeインターフェイスの作成/削除を行う。
<a class="collapsible collapsed" href="#" id="collapse-35335eb6-show" onclick="$(&#x27;#collapse-35335eb6-show, #collapse-35335eb6-hide&#x27;).toggle(); $(&#x27;#collapse-35335eb6&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-35335eb6-hide" onclick="$(&#x27;#collapse-35335eb6-show, #collapse-35335eb6-hide&#x27;).toggle(); $(&#x27;#collapse-35335eb6&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-35335eb6" style="display:none;"><pre>
# bridgeインターフェイス作成
# ./ctlVlan.sh 100
br2.100
# bridgeインターフェイス削除
# ./ctlVlan.sh 100 del
br2.100
# bridgeインターフェイス表示
# ./ctlVlan.sh 100 dryrun
br2.100
</pre></div></li>
		<li>ctlVlan4ManagementNw.sh:引数なしで実行することでコマンドの簡単な使用方法が表示できる。引数に指定したvlan idのbridgeインターフェイスの作成/削除を行う。管理系のインターフェイスにブリッジを作成する。
<a class="collapsible collapsed" href="#" id="collapse-0cea70de-show" onclick="$(&#x27;#collapse-0cea70de-show, #collapse-0cea70de-hide&#x27;).toggle(); $(&#x27;#collapse-0cea70de&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-0cea70de-hide" onclick="$(&#x27;#collapse-0cea70de-show, #collapse-0cea70de-hide&#x27;).toggle(); $(&#x27;#collapse-0cea70de&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-0cea70de" style="display:none;"><pre>
# bridgeインターフェイス作成
# ./ctlVlan4ManagementNw.sh 100
br1.100
# bridgeインターフェイス削除
# ./ctlVlan4ManagementNw.sh 100 del
br1.100
# bridgeインターフェイス表示
# ./ctlVlan4ManagementNw.sh 100 dryrun
br1.100
</pre></div></li>
		<li>ctlLvm.sh:引数なしで実行することでコマンドの簡単な使用方法が表示できる。引数に指定した名前とサイズを持つLVM領域の作成/削除を行う。
<a class="collapsible collapsed" href="#" id="collapse-be489de3-show" onclick="$(&#x27;#collapse-be489de3-show, #collapse-be489de3-hide&#x27;).toggle(); $(&#x27;#collapse-be489de3&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-be489de3-hide" onclick="$(&#x27;#collapse-be489de3-show, #collapse-be489de3-hide&#x27;).toggle(); $(&#x27;#collapse-be489de3&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-be489de3" style="display:none;"><pre>
# LVM作成
# ./ctlLvm.sh lv_test 20GB
/dev/vg_idc-01/lv_test
# LVM削除
# ./ctlLvm.sh lv_test 20GB del
/dev/vg_idc-01/lv_test
# LVM表示
# ./ctlLvm.sh lv_test 20GB dryrun
/dev/vg_idc-01/lv_test
</pre></div></li>
		<li>ctlCompanyNw.sh:引数なしで実行することでコマンドの簡単な使用方法が表示できる。事業者IDで利用するvlan idのbridgeインターフェイスの作成/削除を行う。
<a class="collapsible collapsed" href="#" id="collapse-cd62e10a-show" onclick="$(&#x27;#collapse-cd62e10a-show, #collapse-cd62e10a-hide&#x27;).toggle(); $(&#x27;#collapse-cd62e10a&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-cd62e10a-hide" onclick="$(&#x27;#collapse-cd62e10a-show, #collapse-cd62e10a-hide&#x27;).toggle(); $(&#x27;#collapse-cd62e10a&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-cd62e10a" style="display:none;"><pre>
# 事業者IDのbridgeインターフェイス作成
# ./ctlCompanyNw.sh 13
br2.104 br2.105 br2.106 br2.107 br2.108 br2.109 br2.110 br2.111 br2.2152 br2.2153 br2.2154 br2.2155 br2.2156 br2.2157 br2.2158 br2.2159
# 事業者IDのbridgeインターフェイス削除
# ./ctlCompanyNw.sh 13 del
br2.104 br2.105 br2.106 br2.107 br2.108 br2.109 br2.110 br2.111 br2.2152 br2.2153 br2.2154 br2.2155 br2.2156 br2.2157 br2.2158 br2.2159
# 事業者IDのbridgeインターフェイス表示
# ./ctlCompanyNw.sh 13 dryrun
br2.104 br2.105 br2.106 br2.107 br2.108 br2.109 br2.110 br2.111 br2.2152 br2.2153 br2.2154 br2.2155 br2.2156 br2.2157 br2.2158 br2.2159
</pre></div></li>
		<li>ctlManagementNw.sh:事業者IDで利用するvlan idのbridgeインターフェイスの作成/削除を行う。
<a class="collapsible collapsed" href="#" id="collapse-4b18f0a0-show" onclick="$(&#x27;#collapse-4b18f0a0-show, #collapse-4b18f0a0-hide&#x27;).toggle(); $(&#x27;#collapse-4b18f0a0&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-4b18f0a0-hide" onclick="$(&#x27;#collapse-4b18f0a0-show, #collapse-4b18f0a0-hide&#x27;).toggle(); $(&#x27;#collapse-4b18f0a0&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-4b18f0a0" style="display:none;"><pre>
# bridgeインターフェイス作成
# ./ctlManagementNw.sh
br1.21 br1.22 br1.23
# bridgeインターフェイス削除
# ./ctlManagementNw.sh del
br1.21 br1.22 br1.23
# bridgeインターフェイス表示
# ./ctlManagementNw.sh dryrun
br1.21 br1.22 br1.23
</pre></div></li>
		<li>createXml.sh:引数なしで実行することでコマンドの簡単な使用方法が表示できる。仮想インスタンスを作成する際に利用するXMLファイルを生成する。
<a class="collapsible collapsed" href="#" id="collapse-5544b821-show" onclick="$(&#x27;#collapse-5544b821-show, #collapse-5544b821-hide&#x27;).toggle(); $(&#x27;#collapse-5544b821&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-5544b821-hide" onclick="$(&#x27;#collapse-5544b821-show, #collapse-5544b821-hide&#x27;).toggle(); $(&#x27;#collapse-5544b821&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-5544b821" style="display:none;"><pre>
# ./createXml.sh 13_webserver 1C1G
/export/template/13_webserver.xml
</pre></div></li>
		<li>createXml4Mpx.sh:引数なしで実行することでコマンドの簡単な使用方法が表示できる。zabbix proxyを作成する際に利用するXMLファイルを生成する。
<a class="collapsible collapsed" href="#" id="collapse-7a43cee3-show" onclick="$(&#x27;#collapse-7a43cee3-show, #collapse-7a43cee3-hide&#x27;).toggle(); $(&#x27;#collapse-7a43cee3&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-7a43cee3-hide" onclick="$(&#x27;#collapse-7a43cee3-show, #collapse-7a43cee3-hide&#x27;).toggle(); $(&#x27;#collapse-7a43cee3&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-7a43cee3" style="display:none;"><pre>
# ./createXml4Mpx.sh 13
/export/template/idc-ompx013.xml
</pre></div></li>
		<li>createXml4Vrt.sh:引数なしで実行することでコマンドの簡単な使用方法が表示できる。仮想ルータを作成する際に利用するXMLファイルを生成する。
<a class="collapsible collapsed" href="#" id="collapse-e917556b-show" onclick="$(&#x27;#collapse-e917556b-show, #collapse-e917556b-hide&#x27;).toggle(); $(&#x27;#collapse-e917556b&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-e917556b-hide" onclick="$(&#x27;#collapse-e917556b-show, #collapse-e917556b-hide&#x27;).toggle(); $(&#x27;#collapse-e917556b&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-e917556b" style="display:none;"><pre>
# ./createXml4Vrt.sh 13
/export/template/idc-ovrt013.xml
</pre></div></li>
		<li>createXml4VrtConfig.sh:引数なしで実行することでコマンドの簡単な使用方法が表示できる。仮想ルータ作成後初期化設定ファイルを生成する。このコマンドはidc-onas01で実行すること。
<a class="collapsible collapsed" href="#" id="collapse-56009b8e-show" onclick="$(&#x27;#collapse-56009b8e-show, #collapse-56009b8e-hide&#x27;).toggle(); $(&#x27;#collapse-56009b8e&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-56009b8e-hide" onclick="$(&#x27;#collapse-56009b8e-show, #collapse-56009b8e-hide&#x27;).toggle(); $(&#x27;#collapse-56009b8e&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-56009b8e" style="display:none;"><pre>
# ./createXml4VrtConfig.sh 13 1.2.3.4/29
/export/template/idc-ovrt013_default_config.boot
/var/lib/tftpboot/idc-ovrt013_default_config.boot
</pre></div></li>
		<li>copyInDisk.sh:引数なしで実行することでコマンドの簡単な使用方法が表示できる。仮想インスタンスが使用するdisk内部の情報を書き換える。
<a class="collapsible collapsed" href="#" id="collapse-20535940-show" onclick="$(&#x27;#collapse-20535940-show, #collapse-20535940-hide&#x27;).toggle(); $(&#x27;#collapse-20535940&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-20535940-hide" onclick="$(&#x27;#collapse-20535940-show, #collapse-20535940-hide&#x27;).toggle(); $(&#x27;#collapse-20535940&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-20535940" style="display:none;"><pre>
# ./copyInDisk.sh 13_test
</pre></div></li>
		<li>copyInDisk4Mpx.sh:引数なしで実行することでコマンドの簡単な使用方法が表示できる。zabbix proxyが使用するdisk内部の情報を書き換える。
<a class="collapsible collapsed" href="#" id="collapse-636e0957-show" onclick="$(&#x27;#collapse-636e0957-show, #collapse-636e0957-hide&#x27;).toggle(); $(&#x27;#collapse-636e0957&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-636e0957-hide" onclick="$(&#x27;#collapse-636e0957-show, #collapse-636e0957-hide&#x27;).toggle(); $(&#x27;#collapse-636e0957&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-636e0957" style="display:none;"><pre>
# ./copyInDisk4Mpx.sh 13
</pre></div></li>
		<li>copyInDisk4ZabbixConf.sh:ドメイン名を引数に実行し、zabbix-agentの設定ファイルにデフォルトNWを設定して書き換える。
<a class="collapsible collapsed" href="#" id="collapse-fd4152c2-show" onclick="$(&#x27;#collapse-fd4152c2-show, #collapse-fd4152c2-hide&#x27;).toggle(); $(&#x27;#collapse-fd4152c2&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-fd4152c2-hide" onclick="$(&#x27;#collapse-fd4152c2-show, #collapse-fd4152c2-hide&#x27;).toggle(); $(&#x27;#collapse-fd4152c2&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-fd4152c2" style="display:none;"><pre>
# ./copyInDisk4ZabbixConf.sh 13-vm001
</pre></div></li>
		<li>createVm.sh:ドメイン名、スペックを引数に実行し、仮想インスタンス作成手順を簡略化したラッパースクリプトである。
<a class="collapsible collapsed" href="#" id="collapse-c1990e6b-show" onclick="$(&#x27;#collapse-c1990e6b-show, #collapse-c1990e6b-hide&#x27;).toggle(); $(&#x27;#collapse-c1990e6b&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-c1990e6b-hide" onclick="$(&#x27;#collapse-c1990e6b-show, #collapse-c1990e6b-hide&#x27;).toggle(); $(&#x27;#collapse-c1990e6b&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-c1990e6b" style="display:none;"><pre>
# ./createVm.sh 13-vm001 1C1G
</pre></div></li>
		<li>loadConfig2Vyos4IpMapping.s:事業者ID, IPアドレス, MACアドレスを引数に実行し、仮想ルータの「仮想インスタンスへのDHCPによる固定IPアドレス登録手順」を簡略化したラッパースクリプトである。
<a class="collapsible collapsed" href="#" id="collapse-a970e284-show" onclick="$(&#x27;#collapse-a970e284-show, #collapse-a970e284-hide&#x27;).toggle(); $(&#x27;#collapse-a970e284&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-a970e284-hide" onclick="$(&#x27;#collapse-a970e284-show, #collapse-a970e284-hide&#x27;).toggle(); $(&#x27;#collapse-a970e284&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-a970e284" style="display:none;"><pre>
# ./loadConfig2Vyos4IpMapping.sh 1 10.0.0.1 AA:BB:CC:DD:EE:FF
</pre></div></li>
	</ul>


	<p>また、NFS上の/export/templateフォルダは仮想インスタンスの設定バックアップ等に利用するフォルダである。<br />上記フォルダの利用は以降の手順の中で説明していく。</p>


	<p>仮想インスタンスの名称および関係のあるものの名称は以下とする。</p>


	<ul>
	<li>[事業者ID]-[任意のホスト名]: kvmで管理される仮想インスタンス名。以降libvirtの名称に合わせドメイン名と呼ぶ</li>
		<li>/dev/vg_idc-01/lv_[ドメイン名]: 仮想インスタンスが使用するシステム領域のLVM領域名</li>
		<li>/dev/vg_idc-01/lv_[ドメイン名]_XX: 仮想インスタンスが使用するデータ領域のLVM領域名。XXには0パディングされた2桁の連番が付く</li>
		<li>/dev/vg_idc-01/lv_[ドメイン名]_backup_YYYY-MM-DD: 仮想インスタンスのシステム領域バックアップのLVM領域名。日付を付与し、バックアップ日が特定できるようにする</li>
		<li>/dev/vg_idc-01/lv_[ドメイン名]_XX_backup_YYYY-MM-DD: 仮想インスタンスのデータ領域バックアップのLVM領域名。日付を付与し、バックアップ日が特定できるようにする</li>
		<li>/dev/vg_idc-01/lv_template_CentOS-6.6-x86_64: 仮想インスタンスのシステム領域テンプレート。仮想インスタンスを作成する際にここからコピーを行う</li>
		<li>/dev/vg_idc-01/lv_template_idc-ompx: zabbix proxyのテンプレート</li>
		<li>/dev/vg_idc-01/lv_template_idc-ovrt: 仮想ルータのテンプレート</li>
		<li>/export/template/[ドメイン名].xml: 仮想インスタンス構成が記載されたXMLファイル</li>
		<li>/export/template/template_[スペック].xml: 仮想インスタンスのサンプルXMLファイル。このファイルを元に仮想インスタンスXMLファイルを作成する</li>
		<li>/export/template/template_idc-ompx.xml: zabbix proxyのサンプルXMLファイル。このファイルを元にzabbix proxyXMLファイルを作成する</li>
		<li>/export/template/template_idc-ovrt.xml: 仮想ルータのサンプルXMLファイル。このファイルを元に仮想ルータXMLファイルを作成する</li>
		<li>/export/template/template_idc-ovrt_config.boot: 仮想ルータの設定ファイルテンプレート。このファイルを元に仮想ルータの設定ファイルを作成する</li>
		<li>/export/template/template_70-persistent-net.rules: 仮想インスタンスのインターフェイス固定化設定ファイルテンプレート</li>
		<li>/export/template/template_ifcfg-eth0: 仮想インスタンスのインターフェイス設定ファイルテンプレート</li>
		<li>/export/template/template_network: 仮想インスタンスのホスト名設定ファイルテンプレート</li>
		<li>/export/template/template_idc-ompx_70-persistent-net.rules: zabbix proxyのインターフェイス固定化設定ファイルテンプレート</li>
		<li>/export/template/template_idc-ompx_ifcfg-ethX: zabbix proxyのインターフェイス設定ファイルテンプレート</li>
		<li>/export/template/template_idc-ompx_network: zabbix proxyのホスト名設定ファイルテンプレート</li>
		<li>/export/template/idc-ovrtXXX_default_config.boot: /var/lib/tftpboot/idc-ovrtXXX_default_config.bootへのシンボリックリンク仮想ルータのデフォルト設定ファイル</li>
		<li>/var/lib/tftpboot/idc-ovrtXXX_default_config.boot: 仮想ルータのデフォルト設定ファイル</li>
		<li>/export/template/template_CentOS-6.6-x86_64.img.gz: 仮想インスタンスのイメージバックアップ</li>
		<li>/export/template/template_idc-ompx.img.gz: zabbix proxyのイメージバックアップ</li>
		<li>/export/template/template_idc-ovrt-dhcp.img.gz: 仮想ルータ用dhcpのイメージバックアップ</li>
		<li>/export/template/template_idc-ovrt.img.gz: 仮想ルータのイメージバックアップ</li>
		<li>/export/template/[ドメイン名]_backup_YYYY-MM-DD.img.gz: テナントサーバのイメージバックアップ</li>
	</ul>


	<a name="仮想インスタンス運用手順_作成"></a>
<h2 >作成<a href="#仮想インスタンス運用手順_作成" class="wiki-anchor">&para;</a></h2>


	<a name="仮想インスタンス運用手順_事業者の仮想インスタンス"></a>
<h3 >事業者の仮想インスタンス<a href="#仮想インスタンス運用手順_事業者の仮想インスタンス" class="wiki-anchor">&para;</a></h3>


	<p>まずはドメイン名を決定し、LVM領域を作成する。<br />利用するbridgeインターフェイスを作成し、<br />その後、xmlファイルを作成して仮想インスタンスの登録を行う。<br />ここで作成するxmlファイルはユーザ用デフォルトネットワークのみを割り当てるため、<br />追加が必要な場合はxmlファイルの編集が必要となる。</p>


<pre>
### LVM領域作成
# cd /export/clocon
# ./ctlLvm.sh lv_[ドメイン名] 20GB
# dd bs=1024k if=/dev/vg_idc-01/lv_template_CentOS-6.6-x86_64 of=/dev/vg_idc-01/lv_[ドメイン名]
### xmlファイルの作成
# ./createXml.sh [ドメイン名] [SPEC]
### 利用するbridgeインターフェイスの存在確認
# brctl show
### 存在しなければ作成する
# ./ctlCompanyNw.sh [事業者ID]
### 仮想インスタンスの登録
# virsh define /export/template/[ドメイン名].xml
### disk内部の変更すべき情報を書き換える
# ./copyInDisk.sh [ドメイン名]
### 登録の確認
# virsh list --all
</pre>

	<p>上記手順を簡略化したスクリプトを以下に示す。</p>


<pre>
# ./createVm.sh [ドメイン名] [SPEC]
</pre>

	<p>ここで、作成する仮想インスタンスのmacアドレスが/export/template/[ドメイン名].xmlに記述されているため、<br />このmacアドレスを仮想ルータに登録する必要がある。</p>


	<p>DHCPでIPアドレスを固定するため、idc-onas01からスクリプト実行し設定ファイルを読み込ませる。<br />以下の手順で仮想ルータの設定変更を実施する。</p>


<pre>
./loadConfig2Vyos4IpMapping.sh [事業者ID] [仮想マシンのIPアドレス] [仮想マシンのMACアドレス]
</pre>

	<p>また、監視が必要な場合にはzabbixへの登録を行う。<br />その場合、該当の事業者を管理するzabbix proxy配下に登録すること。</p>


	<a name="仮想インスタンス運用手順_事業者のzabbix-proxy"></a>
<h3 >事業者のzabbix proxy<a href="#仮想インスタンス運用手順_事業者のzabbix-proxy" class="wiki-anchor">&para;</a></h3>


	<p>基本的には仮想インスタンスと同一の手順を踏む。<br />ここでは仮に事業者ID13として手順を示す。</p>


<pre>
### LVM領域作成
# cd /export/clocon
# ./ctlLvm.sh lv_idc-ompx013 9GB
# dd bs=1024k if=/dev/vg_idc-01/lv_template_idc-ompx of=/dev/vg_idc-01/lv_idc-ompx013
### 利用するbridgeインターフェイスの存在確認
# brctl show
### 存在しなければ作成する
# ./ctlManagementNw.sh
# ./ctlCompanyNw.sh 13
### 仮想インスタンスの登録
# virsh define /export/template/idc-ompx013.xml
### disk内部の変更すべき情報を書き換える
# ./copyInDisk4Mpx.sh 13
### 登録の確認
# virsh list --all
</pre>

	<p>zabbix proxy作成後はzabbixへ<br />zabbix proxy自体の監視設定とzabbix proxyとして登録を行う。</p>


	<a name="仮想インスタンス運用手順_事業者の仮想ルータ"></a>
<h3 >事業者の仮想ルータ<a href="#仮想インスタンス運用手順_事業者の仮想ルータ" class="wiki-anchor">&para;</a></h3>


	<p>基本的には仮想インスタンスと同一の手順を踏む。<br />仮想ルータには初期化用の設定ファイルを作成する必要がある。<br />初期化用設定ファイルはtftpで参照できる必要があるので、idc-onas01にて実行する。<br />ここでは仮に事業者ID13として手順を示す。</p>


<pre>
### LVM領域作成
# cd /export/clocon
# ./ctlLvm.sh lv_idc-ovrt013 2GB
# dd bs=1024k if=/dev/vg_idc-01/lv_template_idc-ovrt of=/dev/vg_idc-01/lv_idc-ovrt013
### 利用するbridgeインターフェイスの存在確認
# brctl show
### 存在しなければ作成する
# ./ctlManagementNw.sh
# ./ctlCompanyNw.sh 13
### 仮想インスタンスの登録
# virsh define /export/template/idc-ovrt013.xml
### 登録の確認
# virsh list --all
</pre>

<pre>
### 初期化用設定ファイルが存在しない(/export/template/idc-ovrtXXX_default_config.boot)場合は以下を実行する
### 初期化用設定ファイルを作成する。
# hostname
idc-onas01
# cd /export/clocon
# ./createXml4VrfConfig.sh 13 1.2.3.4/29
</pre>

	<p>仮想ルータ作成後はzabbixへ仮想ルータの監視設定登録を行う。</p>


	<a name="仮想インスタンス運用手順_仮想ルータの初期化"></a>
<h4 >仮想ルータの初期化<a href="#仮想インスタンス運用手順_仮想ルータの初期化" class="wiki-anchor">&para;</a></h4>


	<p>仮想ルータは初期化をコンソール画面(VNC)にて行う必要がある。<br />その際には以下の項目に注意して初期化を行うために必要な設定を入れる。</p>


	<p>初期化手順にはidc-ovrt-dhcp（仮想ルータ用DHCP）が必要なので、予め起動しておく。</p>


	<p>まずは管理インターフェイスを特定するためにmacアドレスを表示する。<br />今回は仮にidc-ovrt013で行うものとする。<br /><pre>
### kvmホスト上で以下を実行する。
# cd /export/clocon
# grep idc-ovrt013 vrtManagementIf.list
idc-ovrt013-52:54:00:ac:a4:0e
# virsh start idc-ovrt013
# virsh vncdisplay idc-ovrt013
</pre></p>


	<p>以降、vrtで以下を実行する。</p>


<pre>
$ configure
# show interfaces
### macアドレスが一致するインターフェイスを特定する
### 特定したinterfaceにIPを設定 第四オクテットはホスト名と同じ数値にする。
# set interfaces ethernet ethX address dhcp
### idc-ovrt-dhcpが稼働していない場合はdhcpではなく、静的に設定する必要がある
### その場合はゲートウェイの設定も必要となる
# set interfaces ethernet ethX address 10.16.38.XXX/24
# set system gateway-address 10.16.38.254
### sshサービスの起動
# set service ssh port 22
# commit
### 疎通確認
# ping 10.16.38.254
</pre>

	<p>idc-onas01のscpで設定ファイルを入れ込む。<br />また、設定ファイルをリモートからロードするスクリプトも入れ込んでおく。</p>


<pre>
### 仮想ルータのホスト名と同じ設定ファイルを読み込む
# scp /var/lib/tftpboot/idc-ovrt013_default_config.boot vyos@idc-ovrt013:/config/config.boot
# scp /export/clocon/loadConfigWithSaveIfCommitIsOk.sh vyos@idc-ovrt013:/opt/vyatta/etc/config/
</pre>

	<p>仮想ルータを再起動する。</p>


<pre>
### 設定モードを抜けて再起動を行う
# exit
$ reboot
</pre>

	<a name="仮想インスタンス運用手順_起動"></a>
<h2 >起動<a href="#仮想インスタンス運用手順_起動" class="wiki-anchor">&para;</a></h2>


<pre>
# virsh start [ドメイン名]
</pre>

	<p>起動に失敗する場合は以下の設定を見直すこと。<br />特にこれらの作業はkvmのホストOS再起動時は改めて確認すること。</p>


	<ul>
	<li>virsh list --allにて該当ドメインの登録確認</li>
		<li>仮想インスタンス等にアタッチするデバイスの確認
	<ul>
	<li>vgchange -a y vg_idc-01によるボリュームグループの有効化</li>
		<li>ctlCompanyNw.shによるブリッジインターフェイスの作成</li>
		<li>仮想ルータ、zabbix proxyの場合ctlManagementNw.shによるブリッジインターフェイスの作成</li>
	</ul>
	</li>
		<li>vncポート指定時のポート番号利用重複</li>
	</ul>


	<a name="仮想インスタンス運用手順_停止"></a>
<h2 >停止<a href="#仮想インスタンス運用手順_停止" class="wiki-anchor">&para;</a></h2>


<pre>
# virsh stop [ドメイン名]
</pre>

	<a name="仮想インスタンス運用手順_再起動"></a>
<h2 >再起動<a href="#仮想インスタンス運用手順_再起動" class="wiki-anchor">&para;</a></h2>


<pre>
# virsh reboot [ドメイン名]
</pre>

	<a name="仮想インスタンス運用手順_強制停止"></a>
<h2 >強制停止<a href="#仮想インスタンス運用手順_強制停止" class="wiki-anchor">&para;</a></h2>


<pre>
# virsh destroy [ドメイン名]
</pre>

	<a name="仮想インスタンス運用手順_ライブマイグレーション"></a>
<h2 >ライブマイグレーション<a href="#仮想インスタンス運用手順_ライブマイグレーション" class="wiki-anchor">&para;</a></h2>


	<p>マイグレーション先にbridgeインターフェイスが存在することを確認する。<br />存在しない場合は先にctlManagementNw.shやctlCompanyNw.shでbridgeインターフェイスを作成しておくこと。<br />LVM領域は元々共有されているが、確認を行っておく。</p>


<pre>
# brctl show
# lvscan
### inactiveになっている場合はactive化する
# vgchange -a y vg_idc-01
</pre>

	<p>他のサーバに接続するためにはkvmmanagerユーザとなって行うこと。</p>


<pre>
# su - kvmmanager
# virsh -c qemu:///system migrate --live [ドメイン名] qemu+ssh://idc-ohyp0[1～7]/system
</pre>

	<a name="仮想インスタンス運用手順_バックアップの作成"></a>
<h2 >バックアップの作成<a href="#仮想インスタンス運用手順_バックアップの作成" class="wiki-anchor">&para;</a></h2>


	<p>まずは対象の仮想インスタンスが停止していることを確認する。<br /><del>次に対象のLVM領域名およびサイズを元にバックアップ用のLVM領域を作成する。</del><br /><del>その後コピーを行う。</del><br />バックアップはNFS領域に保持する。LVM領域から直接NFS領域にコピーすると時間がかかるため、<br />ローカルにコピー後圧縮しNFS領域に置く。</p>


	<p>最後に必要であれば仮想インスタンスを起動しておく。</p>


<pre>
### 仮想インスタンスの停止確認
# virsh list --all
### バックアップ対象のLVM領域名、サイズの確認
# lvscan

### ローカルにコピー
# dd bs=1024k if=/dev/vg_idc-01/lv_[ドメイン名] of=[ドメイン名]_backup_YYYY-MM-DD.img
### 圧縮
# gzip [ドメイン名]_backup_YYYY-MM-DD.img
# ls
[ドメイン名]_backup_YYYY-MM-DD.img.gz
# cp [ドメイン名]_backup_YYYY-MM-DD.img.gz /export/template
# 必要であれば仮想インスタンスを起動する
# virsh start [ドメイン名]

****** 以下、LVM領域にバックアップを行う旧手順 *****
### バックアップ用のLVM領域作成
# cd /export/clocon
# ./ctlLvm.sh lv_[ドメイン名]_backup_`date +%F` 20GB
# dd bs=1024k if=/dev/vg_idc-01/lv_[ドメイン名] of=/dev/vg_idc-01/lv_[ドメイン名]_backup_YYYY-MM-DD
### データ領域の利用があれば同様にコピーする
# ./ctlLvm.sh lv_[ドメイン名]_XX_backup_`date +%F` 20GB
# dd bs=1024k if=/dev/vg_idc-01/lv_[ドメイン名]_XX of=/dev/vg_idc-01/lv_[ドメイン名]_XX_backup_YYYY-MM-DD
</pre>

	<a name="仮想インスタンス運用手順_復旧"></a>
<h2 >復旧<a href="#仮想インスタンス運用手順_復旧" class="wiki-anchor">&para;</a></h2>


	<p>仮想インスタンスが故障した場合はバックアップから復旧を行う。<br />データ領域があれば同様の手順を行う。</p>


<pre>
### 領域が十分にある場所で行うこと。
# gunzip /export/template/[ドメイン名]_backup_YYYY-MM-DD.img.gz
# dd bs=1024k if=[ドメイン名]_backup_YYYY-MM-DD.img of=/dev/vg_idc-01/lv_[ドメイン名]

****** 以下、LVM領域にバックアップを行う旧手順 *****
# dd bs=1024k if=/dev/vg_idc-01/lv_[ドメイン名]_template_YYYY-MM-DD of=/dev/vg_idc-01/lv_[ドメイン名]
# dd bs=1024k if=/dev/vg_idc-01/lv_[ドメイン名]_XX_template_YYYY-MM-DD of=/dev/vg_idc-01/lv_[ドメイン名]_XX
</pre>

	<p>その後起動手順を試す。</p>


	<p>バックアップが存在しない場合は仮想インスタンスの再作成となる。</p>
<hr />
<a name="仮想ルータ設計" />
<a name="仮想ルータ設計_仮想ルータ設計"></a>
<h1 >仮想ルータ設計<a href="#仮想ルータ設計_仮想ルータ設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#仮想ルータ設計_仮想ルータ設計">仮想ルータ設計</a></li><li><a href="#仮想ルータ設計_Introduction">●Introduction</a><ul><li><a href="#仮想ルータ設計_目的">目的</a></li><li><a href="#仮想ルータ設計_定義範囲">定義範囲</a></li></ul>
</li><li><a href="#仮想ルータ設計_構成">●構成</a><ul><li><a href="#仮想ルータ設計_サーバ構成">サーバ構成</a></li><li><a href="#仮想ルータ設計_パーティション構成">パーティション構成</a></li><li><a href="#仮想ルータ設計_ポートアサイン">ポートアサイン</a></li><li><a href="#仮想ルータ設計_ログ設定">ログ設定</a><ul><li><a href="#仮想ルータ設計_ログローテ">ログローテ</a></li></ul>
</li></ul>
</li><li><a href="#仮想ルータ設計_システム設計">●システム設計</a><ul><li><a href="#仮想ルータ設計_設定ファイル">設定ファイル</a></li><li><a href="#仮想ルータ設計_ユーザアカウント設定">ユーザ・アカウント設定</a></li><li><a href="#仮想ルータ設計_sshサービス">sshサービス</a></li><li><a href="#仮想ルータ設計_DNSサービス">DNSサービス</a></li><li><a href="#仮想ルータ設計_DHCPサービス">DHCPサービス</a></li><li><a href="#仮想ルータ設計_NTPサービス">NTPサービス</a></li><li><a href="#仮想ルータ設計_SNMPサービス">SNMPサービス</a></li></ul>
</li><li><a href="#仮想ルータ設計_冗長化">●冗長化</a></li><li><a href="#仮想ルータ設計_バックアップ">●バックアップ</a><ul><li><a href="#仮想ルータ設計_バックアップ概要">バックアップ概要</a></li><li><a href="#仮想ルータ設計_バックアップ-2">バックアップ</a></li><li><a href="#仮想ルータ設計_リストア">リストア</a></li></ul>
</li><li><a href="#仮想ルータ設計_運用手順">●運用手順</a><ul><li><a href="#仮想ルータ設計_初期化">初期化</a></li><li><a href="#仮想ルータ設計_仮想インスタンスへのDHCPによる固定IPアドレス登録手順">仮想インスタンスへのDHCPによる固定IPアドレス登録手順</a></li><li><a href="#仮想ルータ設計_ディスティネーションNAT設定手順">ディスティネーションNAT設定手順</a></li><li><a href="#仮想ルータ設計_ファイアウォールルール適用変更手順">ファイアウォールルール適用変更手順</a></li><li><a href="#仮想ルータ設計_グローバルIPアドレス追加に伴うeth1のIPアドレス変更について">グローバルIPアドレス追加に伴うeth1のIPアドレス変更について</a></li></ul></li></ul>


<hr />


	<a name="仮想ルータ設計_Introduction"></a>
<h1 >●Introduction<a href="#仮想ルータ設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="仮想ルータ設計_目的"></a>
<h2 >目的<a href="#仮想ルータ設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>IaaSクラウドにおいて動的なインフラ環境の提供（=事業者の追加にあたる）は必須要件であるが、<br />その都度、物理NW機器の設定を変更・管理していくことは困難である。<br />これの解決策として仮想ルータの利用が挙げられる。<br />仮想ルータを事業者に提供し、各々の要件にあったFWやルーティングの設定を適用できる。</p>


	<p>本システムでは仮想ルータを事業者とインターネットとの境界に設置する。<br />仮想ルータはインターネットにさらされているため、<br />予め不要な通信を遮断する設定をする。<br />ただし事業者は仮想ルータの設定を自由に変更できるため、<br />変更する場合は適切に設定を行う必要がある。<br />本書はクラウドシステムで利用される仮想ルータについて定義する。<br />本書は仕様書の既読者を対象とする。</p>


	<a name="仮想ルータ設計_定義範囲"></a>
<h2 >定義範囲<a href="#仮想ルータ設計_定義範囲" class="wiki-anchor">&para;</a></h2>


	<p>定義範囲を以下に示す。</p>


	<p><img src="/attachments/download/1025/virtual_router_define_range.png" alt="" /></p>


	<ul>
	<li>クライアント
	<ul>
	<li>クライアントはインターネット経由で仮想サーバと通信を行う人を指し、事業者だけでなく一般ユーザも含める。</li>
	</ul>
	</li>
		<li>物理FW
	<ul>
	<li>仮想ルータにとって上位NW機器(インターネットやクラウド設備が提供するDNS/NTP等へ接続するゲートウェイ)となる。仮想ルータ宛の通信に関して基本的には制限を設けない。</li>
	</ul>
	</li>
		<li>仮想ルータ
	<ul>
	<li>各事業者のNWにおいてゲートウェイとなるルータ。ossのvyosを利用する。各事業者へはdhcp,dns,ntp等のサービスも提供する必要がある。また、仮想サーバにとってNATポイントになる。</li>
	</ul>
	</li>
		<li>仮想サーバ
	<ul>
	<li>各事業者が管理するサーバ。どのようなサービスでも提供可能であるが、サービスを公開するためには仮想ルータに設定を行う必要がある。</li>
	</ul></li>
	</ul>


<hr />


	<a name="仮想ルータ設計_構成"></a>
<h1 >●構成<a href="#仮想ルータ設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="仮想ルータ設計_サーバ構成"></a>
<h2 >サーバ構成<a href="#仮想ルータ設計_サーバ構成" class="wiki-anchor">&para;</a></h2>


	<p>仮想ルータに利用するOSを以下に記載する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>OS名</th>
			<th>バージョン</th>
		</tr>
		<tr>
			<td>vyos</td>
			<td>1.1.5(helium)</td>
		</tr>
	</table>




	<p>vyosはdebian linuxのディストリビューションであるが、<br />NW機器を操作している感覚(juniper製品と類似している。以降、vyosコマンドと呼ぶ)で設定を行うことができる。<br />通常のlinuxで利用可能なコマンドも使用できるが、<br />非推奨であるため本書ではvyosコマンドでの方法を示す。<br />vyosには操作モードと設定モードが存在し、<br />vyosコマンド例には操作モード時に「$」、設定モード時に「#」を先頭に付与して区別する。<br />通常時は操作モードであり、以下に設定モードへの変更方法を示す。</p>


	<p><a class="collapsible collapsed" href="#" id="collapse-d213733f-show" onclick="$(&#x27;#collapse-d213733f-show, #collapse-d213733f-hide&#x27;).toggle(); $(&#x27;#collapse-d213733f&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-d213733f-hide" onclick="$(&#x27;#collapse-d213733f-show, #collapse-d213733f-hide&#x27;).toggle(); $(&#x27;#collapse-d213733f&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-d213733f" style="display:none;"><pre>
$ configure 
[edit]
# 
</pre></div></p>


	<p>osの確認方法は以下に示す。</p>


	<p><a class="collapsible collapsed" href="#" id="collapse-f056875d-show" onclick="$(&#x27;#collapse-f056875d-show, #collapse-f056875d-hide&#x27;).toggle(); $(&#x27;#collapse-f056875d&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-f056875d-hide" onclick="$(&#x27;#collapse-f056875d-show, #collapse-f056875d-hide&#x27;).toggle(); $(&#x27;#collapse-f056875d&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-f056875d" style="display:none;"><pre>
$ show host os   
Linux vyos 3.13.11-1-amd64-vyos #1 SMP Wed Jan 7 22:24:09 UTC 2015 x86_64 GNU/Linux
$ show version
Version:      VyOS 1.1.3
Description:  VyOS 1.1.3 (helium)
Copyright:    2015 VyOS maintainers and contributors
Built by:     maintainers@vyos.net
Built on:     Wed Jan 28 14:11:52 UTC 2015
Build ID:     1501281411-ed1724a
System type:  x86 64-bit
Boot via:     image
Hypervisor:   KVM
HW model:     KVM
HW S/N:       Not Specified
HW UUID:      6BEF3185-DF5D-A9CC-EC4B-18BD8BD0CB47
Uptime:       02:48:20 up 30 days,  1:29,  1 user,  load average: 0.00, 0.01, 0.05
</pre></div></p>


	<table style="background:#f7f7f7;float:none;">
		<tr>
			<th style="background:#d3eaf3;text-align:left;">CPU</th>
			<td>x86_64 2C</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">メモリ</th>
			<td>2GB</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">ディスク</th>
			<td>2GB</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">NIC</th>
			<td>18ポート※</td>
		</tr>
	</table>




	<p>※ ユーザ用NW*16 + ユーザ用インターネット + クラウド管理</p>


	<p><a class="collapsible collapsed" href="#" id="collapse-e0704776-show" onclick="$(&#x27;#collapse-e0704776-show, #collapse-e0704776-hide&#x27;).toggle(); $(&#x27;#collapse-e0704776&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-e0704776-hide" onclick="$(&#x27;#collapse-e0704776-show, #collapse-e0704776-hide&#x27;).toggle(); $(&#x27;#collapse-e0704776&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-e0704776" style="display:none;"><pre>
$ show hardware cpu 
Architecture:          x86_64
CPU op-mode(s):        64-bit
CPU(s):                1
Thread(s) per core:    1
Core(s) per socket:    1
CPU socket(s):         1
NUMA node(s):          1
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 13
Stepping:              3
CPU MHz:               2593.748
Hypervisor vendor:     KVM
Virtualization type:   full
L1d cache:             32K
L1i cache:             32K
L2 cache:              4096K
$ show hardware mem 
MemTotal:        1024276 kB
MemFree:          821492 kB
Buffers:           41536 kB
Cached:            99852 kB
SwapCached:            0 kB
Active:            80700 kB
Inactive:          82944 kB
Active(anon):      22472 kB
Inactive(anon):      208 kB
Active(file):      58228 kB
Inactive(file):    82736 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Dirty:                 4 kB
Writeback:             0 kB
AnonPages:         22252 kB
Mapped:             6336 kB
Shmem:               428 kB
Slab:              27272 kB
SReclaimable:      18644 kB
SUnreclaim:         8628 kB
KernelStack:         688 kB
PageTables:         2120 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:      512136 kB
Committed_AS:      64872 kB
VmallocTotal:   34359738367 kB
VmallocUsed:        4140 kB
VmallocChunk:   34359730080 kB
HardwareCorrupted:     0 kB
AnonHugePages:         0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       28660 kB
DirectMap2M:     1019904 kB
$ show disk sda format 

Disk /dev/sda: 2147 MB, 2147483648 bytes
22 heads, 16 sectors/track, 11915 cylinders
Units = cylinders of 352 * 512 = 180224 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0000f37d

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *           6       11916     2096128   83  Linux
$ show interfaces 
Codes: S - State, L - Link, u - Up, D - Down, A - Admin Down
Interface        IP Address                        S/L  Description
---------        ----------                        ---  -----------
eth0             192.168.100.214/24                u/u  OUTSIDE 
eth1             192.168.76.254/24                 u/u  INSIDE 
lo               127.0.0.1/8                       u/u  
                 ::1/128
</pre></div></p>


	<a name="仮想ルータ設計_パーティション構成"></a>
<h2 >パーティション構成<a href="#仮想ルータ設計_パーティション構成" class="wiki-anchor">&para;</a></h2>


	<p>vyosではデフォルト構成を利用する。<br />そのためパーティション構成を省略する。</p>


	<p>KVMサーバのLVM領域として以下の名称で割り当てる。</p>


	<ul>
	<li>/dev/vg_idc-01/lv_idc-ovrfXXX</li>
	</ul>


	<p><a class="collapsible collapsed" href="#" id="collapse-649f7b02-show" onclick="$(&#x27;#collapse-649f7b02-show, #collapse-649f7b02-hide&#x27;).toggle(); $(&#x27;#collapse-649f7b02&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-649f7b02-hide" onclick="$(&#x27;#collapse-649f7b02-show, #collapse-649f7b02-hide&#x27;).toggle(); $(&#x27;#collapse-649f7b02&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-649f7b02" style="display:none;"><pre>
$ show system storage
Filesystem            Size  Used Avail Use% Mounted on
overlayfs             2.0G  242M  1.6G  13% /
tmpfs                 501M     0  501M   0% /lib/init/rw
udev                  493M  172K  493M   1% /dev
tmpfs                 501M  4.0K  501M   1% /dev/shm
/dev/sda1             2.0G  242M  1.6G  13% /live/image
/dev/sda1             2.0G  242M  1.6G  13% /live/cow
tmpfs                 501M     0  501M   0% /live
tmpfs                 501M     0  501M   0% /tmp
/dev/sda1             2.0G  242M  1.6G  13% /opt/vyatta/etc/config
tmpfs                 501M   72K  501M   1% /var/run
none                  501M  176K  500M   1% /opt/vyatta/config
</pre></div></p>


	<a name="仮想ルータ設計_ポートアサイン"></a>
<h2 >ポートアサイン<a href="#仮想ルータ設計_ポートアサイン" class="wiki-anchor">&para;</a></h2>


	<p>仮想ルータはユーザ用NW全てにアクセスできるようにするため、<br />ユーザに割り当てられた16VLAN全てのNICを追加しておく。<br />それに加え、仮想ルータ用のクラウド管理NW用のNICおよびユーザ用インターネットへのNICの追加も必要となる。<br />各VLANのサブネットは24bitで分割されているため、<br />仮想ルータのユーザ用NWおよびユーザ用インターネットにおける第四オクテットは254を利用する。<br />クラウド管理NWの第四オクテットは事業者IDと一致させる。</p>


	<p>また、デフォルトゲートウェイとして10.16.37.254を指定する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>インターフェイス</th>
			<th>ホストOS側のインターフェイス</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>eth0</td>
			<td>br1.22</td>
			<td>クラウド管理NW用。10.16.38.[事業者ID]のIPアドレスを持つ</td>
		</tr>
		<tr>
			<td>eth1</td>
			<td>br1.21</td>
			<td>ユーザ用インターネット用。10.16.37.[事業者ID]のIPアドレスを持つ<br />また、FWからグローバルIPアドレスが降りてくるためこのインターフェイスに設定する必要がある</td>
		</tr>
		<tr>
			<td>eth2 〜 17</td>
			<td>br2.XXXX (事業者に割り当てられるVLAN番号)</td>
			<td>その他のユーザ用NW。特にeth5はユーザ用デフォルトNW用でありsource natはこのNWに対して行う。<br />また、eth4はipmi用でありL3SWによって管理されるため無効設定とする。</td>
		</tr>
	</table>




<pre>
# set interfaces ethernet eth4 disable
</pre>

	<a name="仮想ルータ設計_ログ設定"></a>
<h2 >ログ設定<a href="#仮想ルータ設計_ログ設定" class="wiki-anchor">&para;</a></h2>


	<p>以下のコマンドでログを確認できる。</p>


	<p><a class="collapsible collapsed" href="#" id="collapse-ed5d9a23-show" onclick="$(&#x27;#collapse-ed5d9a23-show, #collapse-ed5d9a23-hide&#x27;).toggle(); $(&#x27;#collapse-ed5d9a23&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-ed5d9a23-hide" onclick="$(&#x27;#collapse-ed5d9a23-show, #collapse-ed5d9a23-hide&#x27;).toggle(); $(&#x27;#collapse-ed5d9a23&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-ed5d9a23" style="display:none;"><pre>
$ show log [表示させたい内容を選択:tabキーで候補がでる]
</pre></div></p>


	<p>また、ログはログサーバへ送信する。</p>


	<p><a class="collapsible collapsed" href="#" id="collapse-a3da7f3e-show" onclick="$(&#x27;#collapse-a3da7f3e-show, #collapse-a3da7f3e-hide&#x27;).toggle(); $(&#x27;#collapse-a3da7f3e&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-a3da7f3e-hide" onclick="$(&#x27;#collapse-a3da7f3e-show, #collapse-a3da7f3e-hide&#x27;).toggle(); $(&#x27;#collapse-a3da7f3e&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-a3da7f3e" style="display:none;"><pre>
# set system syslog host 10.16.33.194 facility all level info
# set system syslog host 10.16.33.194 facility all protocol udp
</pre></div></p>


	<a name="仮想ルータ設計_ログローテ"></a>
<h3 >ログローテ<a href="#仮想ルータ設計_ログローテ" class="wiki-anchor">&para;</a></h3>


	<p>デフォルトの設定のまま利用する。</p>


<hr />


	<a name="仮想ルータ設計_システム設計"></a>
<h1 >●システム設計<a href="#仮想ルータ設計_システム設計" class="wiki-anchor">&para;</a></h1>


	<p>仮想ルータのNW的な要件を以下に示す。</p>


	<ol>
	<li>インターネットへ通すのはユーザ用デフォルトNWのみとする</li>
		<li>ユーザ用の異なるNW間の通信は行わない(vlan間ルーティングはしない)</li>
		<li>ユーザ用デフォルトNW以外のユーザ用NWはDNS,DHCP,NTPのみに利用する</li>
	</ol>


	<p>今回、5つのゾーンを作成する。<br />概要を以下に示す。</p>


	<p><img src="/attachments/download/1026/virtual_router_network_detail.png" alt="" /></p>


	<p>上記4つのゾーンに加え、ルータ自身の通信を制御するzone_localを定義する。<br />以下にゾーン間の通信とFWルールの対応を示す。<br />ゾーンを定義するとデフォルトでパケットをdropするため、<br />FWルールの対応がないものは通信できないことを指す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>送信元</th>
			<th>送信先</th>
			<th>設定するFWルール名</th>
		</tr>
		<tr>
			<td>zone_internet</td>
			<td>zone_default</td>
			<td>internet_default</td>
		</tr>
		<tr>
			<td>zone_internet</td>
			<td>zone_custom</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zone_internet</td>
			<td>zone_management</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zone_internet</td>
			<td>zone_local</td>
			<td>internet_local</td>
		</tr>
		<tr>
			<td>zone_default</td>
			<td>zone_internet</td>
			<td>default_internet</td>
		</tr>
		<tr>
			<td>zone_default</td>
			<td>zone_custom</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zone_default</td>
			<td>zone_management</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zone_default</td>
			<td>zone_local</td>
			<td>default_local</td>
		</tr>
		<tr>
			<td>zone_custom</td>
			<td>zone_internet</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zone_custom</td>
			<td>zone_default</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zone_custom</td>
			<td>zone_management</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zone_custom</td>
			<td>zone_local</td>
			<td>custom_local</td>
		</tr>
		<tr>
			<td>zone_management</td>
			<td>zone_internet</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zone_management</td>
			<td>zone_default</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zone_management</td>
			<td>zone_custom</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zone_management</td>
			<td>zone_local</td>
			<td>management_local</td>
		</tr>
		<tr>
			<td>zone_local</td>
			<td>zone_internet</td>
			<td>local_internet</td>
		</tr>
		<tr>
			<td>zone_local</td>
			<td>zone_default</td>
			<td>local_default</td>
		</tr>
		<tr>
			<td>zone_local</td>
			<td>zone_custom</td>
			<td>local_custom</td>
		</tr>
		<tr>
			<td>zone_local</td>
			<td>zone_management</td>
			<td>local_management</td>
		</tr>
	</table>




	<p>各FWの通信設定を以下に示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>設定するFWルール名</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>internet_default</td>
			<td>全通信を許可（ユーザ設定により変更可能部分）</td>
		</tr>
		<tr>
			<td>internet_local</td>
			<td>関連パケットの許可</td>
		</tr>
		<tr>
			<td>default_internet</td>
			<td>全通信を許可（ユーザ設定により変更可能部分）</td>
		</tr>
		<tr>
			<td>default_local</td>
			<td>DNS, DHCP, NTP,ICMP,関連パケットの許可</td>
		</tr>
		<tr>
			<td>custom_local</td>
			<td>DNS, DHCP, NTP,ICMP,関連パケットの許可</td>
		</tr>
		<tr>
			<td>management_local</td>
			<td>全通信を許可</td>
		</tr>
		<tr>
			<td>local_internet</td>
			<td>全通信を許可</td>
		</tr>
		<tr>
			<td>local_default</td>
			<td>全通信を許可</td>
		</tr>
		<tr>
			<td>local_custom</td>
			<td>全通信を許可</td>
		</tr>
		<tr>
			<td>local_management</td>
			<td>全通信を許可</td>
		</tr>
	</table>




	<p>以上を踏まえ、デフォルト設定を以下に示す。</p>


	<p><a class="collapsible collapsed" href="#" id="collapse-07633ad0-show" onclick="$(&#x27;#collapse-07633ad0-show, #collapse-07633ad0-hide&#x27;).toggle(); $(&#x27;#collapse-07633ad0&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-07633ad0-hide" onclick="$(&#x27;#collapse-07633ad0-show, #collapse-07633ad0-hide&#x27;).toggle(); $(&#x27;#collapse-07633ad0&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-07633ad0" style="display:none;"><pre>
# WAN側インターフェイスとなるユーザ用インターネットをデフォルトゲートウェイとする
# set system gateway-address 10.16.37.254
# VLANの異なる別の管理NWへの通信をeth0へ排出する必要がある
# set protocols static route 10.16.33.0/24 next-hop '10.16.38.254'
# set protocols static route 10.16.39.0/24 next-hop '10.16.38.254'
# ホスト名を設定する
# set system host-name idc-ovrtXXX
### 要件1:ユーザ用デフォルトNWからのインターネット通信はNAPTする
# ユーザ用インターネットのインターフェイスを指定する
# set nat source rule 100 outbound-interface 'eth1'
# ユーザ用デフォルトNWのネットワークアドレスを指定する
# set nat source rule 100 source address '10.XXX.XXX.0/24'
### eth1はグローバルIPとローカル通信用に2つ持つはずなので、グローバルIPにnatする用に指定する
# set nat source rule 100 translation address 'XXX.XXX.XXX.XXX'
### 要件2:zoneを定義することで基本的にルーティングできない設定となる
###       別途許可する必要のある設定を行う
### 要件3:要件2で定義したFW設定にDNS,DHCP,NTPについては通信を許可する
# set firewall name internet_default default-action drop
# set firewall name internet_default enable-default-log
# set firewall name internet_default rule 9999 action accept # 別途定義を入れ込む場合rule9999をdisableに設定する
# set firewall name internet_default rule 9999 protocol all
# set firewall name default_internet default-action drop
# set firewall name default_internet enable-default-log
# set firewall name default_internet rule 9999 action accept # 別途定義を入れ込む場合rule9999をdisableに設定する
# set firewall name default_internet rule 9999 protocol all
### DNS, DHCP, NTPは仮想ルータがサービスを提供しているため、source/destinationどちらも設定を行う必要がある
# set firewall name internet_local default-action drop
# set firewall name internet_local enable-default-log
# set firewall name internet_local rule 10 action accept
# set firewall name internet_local rule 10 state established enable
# set firewall name internet_local rule 10 state related enable
# set firewall name default_local default-action drop
# set firewall name default_local enable-default-log
# set firewall name default_local rule 10 action accept
# set firewall name default_local rule 10 protocol udp
# set firewall name default_local rule 10 source port 53,67,68,123
# set firewall name default_local rule 20 action accept
# set firewall name default_local rule 20 protocol udp
# set firewall name default_local rule 20 destination port 53,67,68,123
# set firewall name default_local rule 30 action accept
# set firewall name default_local rule 30 protocol icmp
# set firewall name default_local rule 40 action accept
# set firewall name default_local rule 40 state established enable
# set firewall name default_local rule 40 state related enable
# set firewall name custom_local default-action drop
# set firewall name custom_local enable-default-log
# set firewall name custom_local rule 10 action accept
# set firewall name custom_local rule 10 protocol udp
# set firewall name custom_local rule 10 source port 53,67,68,123
# set firewall name custom_local rule 20 action accept
# set firewall name custom_local rule 20 protocol udp
# set firewall name custom_local rule 20 destination port 53,67,68,123
# set firewall name custom_local rule 30 action accept
# set firewall name custom_local rule 30 protocol icmp
# set firewall name custom_local rule 40 action accept
# set firewall name custom_local rule 40 state established enable
# set firewall name custom_local rule 40 state related enable
### 以下はローカルの通信であるため完全に信用する
# set firewall name management_local default-action accept
# set firewall name local_internet default-action accept
# set firewall name local_default default-action accept
# set firewall name local_custom default-action accept
# set firewall name local_management default-action accept
### 上記FWルールをゾーンに設定していく
# set zone-policy zone zone_management from zone_local firewall name local_management
# set zone-policy zone zone_management interface eth0
# set zone-policy zone zone_internet from zone_default firewall name default_internet
# set zone-policy zone zone_internet from zone_local firewall name local_internet
# set zone-policy zone zone_internet interface eth1
# set zone-policy zone zone_default from zone_internet firewall name internet_default
# set zone-policy zone zone_default from zone_local firewall name local_default
# set zone-policy zone zone_default interface eth5
# set zone-policy zone zone_custom from zone_local firewall name local_custom
# set zone-policy zone zone_custom interface eth2
# set zone-policy zone zone_custom interface eth3
# set zone-policy zone zone_custom interface eth4
# set zone-policy zone zone_custom interface eth6
# set zone-policy zone zone_custom interface eth7
# set zone-policy zone zone_custom interface eth8
# set zone-policy zone zone_custom interface eth9
# set zone-policy zone zone_custom interface eth10
# set zone-policy zone zone_custom interface eth11
# set zone-policy zone zone_custom interface eth12
# set zone-policy zone zone_custom interface eth13
# set zone-policy zone zone_custom interface eth14
# set zone-policy zone zone_custom interface eth15
# set zone-policy zone zone_custom interface eth16
# set zone-policy zone zone_custom interface eth17
# set zone-policy zone zone_local from zone_custom firewall name custom_local
# set zone-policy zone zone_local from zone_default firewall name default_local
# set zone-policy zone zone_local from zone_management firewall name management_local
# set zone-policy zone zone_local local-zone
</pre></div></p>


	<a name="仮想ルータ設計_設定ファイル"></a>
<h2 >設定ファイル<a href="#仮想ルータ設計_設定ファイル" class="wiki-anchor">&para;</a></h2>


	<p>設定モードで設定した全ての内容は以下の設定ファイルに格納されており、<br />起動時に読み込まれる設定も以下となる。</p>


	<ul>
	<li>/config/config.boot</li>
	</ul>


	<p>バックアップとしてリモートにtftpで保存する設定ファイル名称は以下とする。</p>


	<ul>
	<li>idc-ovrfXXX_YYYY-MM-DD_config.boot</li>
	</ul>


	<p>また、初期状態の設定ファイルをリモートに格納してあり、以下の名称とする。</p>


	<ul>
	<li>idc-ovrfXXX_default_config.boot</li>
	</ul>


	<p>現在の設定を設定ファイルに反映させるにはsaveコマンドを実行する。<br />save時はscp,ftp,tftpを利用できる。<br />設定ファイルのload時はscp,ftp,tftp,httpから選択可能である。</p>


<pre>
# save
Saving configuration to '/config/config.boot'...
Done
[edit]
</pre>

	<a name="仮想ルータ設計_ユーザアカウント設定"></a>
<h2 >ユーザ・アカウント設定<a href="#仮想ルータ設計_ユーザアカウント設定" class="wiki-anchor">&para;</a></h2>


	<p>デフォルトでvyosというユーザが作成される。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>ユーザ</th>
			<th>パスワード</th>
		</tr>
		<tr>
			<td>vyos</td>
			<td>isbadmin</td>
		</tr>
	</table>




	<a name="仮想ルータ設計_sshサービス"></a>
<h2 >sshサービス<a href="#仮想ルータ設計_sshサービス" class="wiki-anchor">&para;</a></h2>


	<p>以下のコマンドでsshサービスを起動する。<br />ただし、クラウド管理NW(10.16.38.0/24)のインターフェイスからの通信のみリッスンする。<br />またDNSを用いたホスト検証も無効にする。</p>


<pre>
# set service ssh port '22'
# set service ssh listen-address 10.16.38.XXX
# set service ssh disable-host-validation
</pre>

	<a name="仮想ルータ設計_DNSサービス"></a>
<h2 >DNSサービス<a href="#仮想ルータ設計_DNSサービス" class="wiki-anchor">&para;</a></h2>


	<p>以下のコマンドでDNSサービスを起動する。<br />仮想ルータは内部のサービスとしてDNSを提供するが、<br />レコード情報等は保持せずDNSフォワード機能を用いて名前解決を行う。<br />また、インターネット以外からの名前解決を受け付けるように設定する。</p>


<pre>
# set service dns forwarding cache-size 0
# set service dns forwarding name-server 10.16.35.82
# set service dns forwarding name-server 10.16.35.83
# set service dns forwarding listen-on 'eth0'
# set service dns forwarding listen-on 'eth2'
# set service dns forwarding listen-on 'eth3'
# set service dns forwarding listen-on 'eth4'
# set service dns forwarding listen-on 'eth6'
# set service dns forwarding listen-on 'eth5'
# set service dns forwarding listen-on 'eth7'
# set service dns forwarding listen-on 'eth8'
# set service dns forwarding listen-on 'eth9'
# set service dns forwarding listen-on 'eth10'
# set service dns forwarding listen-on 'eth11'
# set service dns forwarding listen-on 'eth12'
# set service dns forwarding listen-on 'eth13'
# set service dns forwarding listen-on 'eth14'
# set service dns forwarding listen-on 'eth15'
# set service dns forwarding listen-on 'eth16'
# set service dns forwarding listen-on 'eth17'
</pre>

	<a name="仮想ルータ設計_DHCPサービス"></a>
<h2 >DHCPサービス<a href="#仮想ルータ設計_DHCPサービス" class="wiki-anchor">&para;</a></h2>


	<p>以下のコマンドでDHCPサービスを起動する。<br />ネットワーク名は「vlan + vlanid」とする。<br />静的マッピングの名前は第四オクテットの値を0パッディングした3桁の数値を付与した「vm + XXX」とする。<br />これを各ユーザ用デフォルトNWおよびユーザ用カスタムNW用に作成する。</p>


	<p><strong>※ ここでIPアドレスが決定するため、DNSへ登録を行う。DNSへの登録は設計書を参照すること。</strong></p>


<pre>
### 管理用とWAN側のインターフェイスは設定を行わない
# set service dhcp-server shared-network-name pool_eth0 authoritative disable
# set service dhcp-server shared-network-name pool_eth0 subnet 10.16.38.0/24 start 10.16.38.1 stop 10.16.38.253
# set service dhcp-server shared-network-name pool_eth1 authoritative disable
# set service dhcp-server shared-network-name pool_eth1 subnet 10.16.37.0/24 start 10.16.37.1 stop 10.16.37.253
### デフォルトNWとカスタムNWにDHCPを設定していく
### 以下をテンプレートとして16NWのネットワークアドレスに置き換えて設定する
### ただし、default-routerの設定はユーザ用デフォルトネットワークのpool_eth5のみに設定する
# set service dhcp-server shared-network-name pool_ethX authoritative enable
# set service dhcp-server shared-network-name pool_ethX subnet 10.XXX.XXX.0/24 default-router 10.XXX.XXX.254
# set service dhcp-server shared-network-name pool_ethX subnet 10.XXX.XXX.0/24 dns-server 10.XXX.XXX.254
# set service dhcp-server shared-network-name pool_ethX subnet 10.XXX.XXX.0/24 ntp-server 10.XXX.XXX.254
# set service dhcp-server shared-network-name pool_ethX subnet 10.XXX.XXX.0/24 domain-name tenant.local
# set service dhcp-server shared-network-name pool_ethX subnet 10.XXX.XXX.0/24 start 10.XXX.XXX.1 stop 10.XXX.XXX.250
# set service dhcp-server shared-network-name pool_ethX subnet 10.XXX.XXX.0/24 lease 86400
# set service dhcp-server shared-network-name pool_ethX subnet 10.XXX.XXX.0/24 static-mapping vm001 ip-address 10.XXX.XXX.1
# set service dhcp-server shared-network-name pool_ethX subnet 10.XXX.XXX.0/24 static-mapping vm001 mac-address 52:54:00:XX:XX:0a
# set service dhcp-server shared-network-name pool_ethX subnet 10.XXX.XXX.0/24 static-mapping vm002 ip-address 10.XXX.XXX.2
# set service dhcp-server shared-network-name pool_ethX subnet 10.XXX.XXX.0/24 static-mapping vm002 mac-address 52:54:00:XX:XX:0b
</pre>

	<a name="仮想ルータ設計_NTPサービス"></a>
<h2 >NTPサービス<a href="#仮想ルータ設計_NTPサービス" class="wiki-anchor">&para;</a></h2>


	<p>以下のコマンドでNTPサービスを起動し、<br />上位NTPサーバも合わせて指定する。<br />NTPやDNSはidc-occr01,02のWAN側ネットワークを指定する。</p>


<pre>
# set system ntp server 10.16.35.82
# set system ntp server 10.16.35.83
# set system time-zone Asia/Tokyo
</pre>

	<a name="仮想ルータ設計_SNMPサービス"></a>
<h2 >SNMPサービス<a href="#仮想ルータ設計_SNMPサービス" class="wiki-anchor">&para;</a></h2>


	<p>zabbixで仮想ルータの監視を行う際はsnmpを用いる。<br />以下のコマンドでsnmpサービスを有効にする。</p>


<pre>
# set service snmp community idc_monitor
### マネジメントのIPアドレスからのみリッスンする
# set service snmp listen-address 10.16.38.XXX
</pre>

<hr />


	<a name="仮想ルータ設計_冗長化"></a>
<h1 >●冗長化<a href="#仮想ルータ設計_冗長化" class="wiki-anchor">&para;</a></h1>


	<p>本システムでは仮想ルータの冗長化は行わない。</p>


<hr />


	<a name="仮想ルータ設計_バックアップ"></a>
<h1 >●バックアップ<a href="#仮想ルータ設計_バックアップ" class="wiki-anchor">&para;</a></h1>


	<a name="仮想ルータ設計_バックアップ概要"></a>
<h2 >バックアップ概要<a href="#仮想ルータ設計_バックアップ概要" class="wiki-anchor">&para;</a></h2>


	<p>ログはログサーバに送信するため、<br />バックアップ対象はvyosの設定ファイルのみである。</p>


	<a name="仮想ルータ設計_バックアップ-2"></a>
<h2 >バックアップ<a href="#仮想ルータ設計_バックアップ-2" class="wiki-anchor">&para;</a></h2>


	<p>バックアップは設定変更時にtftpで設定ファイルをNASに送信する。<br />設定読み込みも同様のパスを指定して行うことができる。<br />現在の状態が一度保存する後バックアップを行う。</p>


<pre>
# save
# save tftp://10.16.33.194/`hostname`_`date +%F`_config.boot
</pre>

	<a name="仮想ルータ設計_リストア"></a>
<h2 >リストア<a href="#仮想ルータ設計_リストア" class="wiki-anchor">&para;</a></h2>


	<p>バックアップサーバからloadコマンドを用いて設定ファイルを読み込む。</p>


<pre>
### IPアドレスで指定すること
# load tftp://10.16.33.194/`hostname`_YYYY-MM-DD_config.boot
</pre>

<hr />


	<a name="仮想ルータ設計_運用手順"></a>
<h1 >●運用手順<a href="#仮想ルータ設計_運用手順" class="wiki-anchor">&para;</a></h1>


	<a name="仮想ルータ設計_初期化"></a>
<h2 >初期化<a href="#仮想ルータ設計_初期化" class="wiki-anchor">&para;</a></h2>


	<p>デフォルトの設定に戻す場合、NASサーバの設定ファイルバックアップから情報を戻す事。<br />以下のコマンドを実行すること。</p>


<pre>
# load tftp://10.16.33.194/`hostname`_default_config.boot
</pre>

	<a name="仮想ルータ設計_仮想インスタンスへのDHCPによる固定IPアドレス登録手順"></a>
<h2 >仮想インスタンスへのDHCPによる固定IPアドレス登録手順<a href="#仮想ルータ設計_仮想インスタンスへのDHCPによる固定IPアドレス登録手順" class="wiki-anchor">&para;</a></h2>


	<p>固定IPアドレスに必要な情報は仮想インスタンスに割り当てるIPアドレス、<br />割当先のインターフェイスのMACアドレスの2つです。</p>


	<p>割り当てるIPアドレスのネットワークアドレスが設定されたdhcp情報があるので、<br />その設定配下に固定マッピング設定を追加する。</p>


	<p>マッピングで指定するホスト名は割り当てるIPアドレスの第四オクテットを<br />3桁で0パディングしたものを付ける。</p>


<pre>
# set service dhcp-server shared-network-name pool_ethX subnet [ネットワークアドレス] static-mapping vm[IPアドレスの第四オクテット] ip-address [IPアドレス]
# set service dhcp-server shared-network-name pool_ethX subnet [ネットワークアドレス] static-mapping vm[IPアドレスの第四オクテット] mac-address [MACアドレス]
</pre>

	<p><strong>設定反映後、設定ファイルのバックアップを行う。</strong></p>


	<a name="仮想ルータ設計_ディスティネーションNAT設定手順"></a>
<h2 >ディスティネーションNAT設定手順<a href="#仮想ルータ設計_ディスティネーションNAT設定手順" class="wiki-anchor">&para;</a></h2>


	<p>ディスティネーションNATで必要な情報を以下に挙げる。</p>


	<ul>
	<li>変換するグローバルIPアドレス（１つしかない場合はそれで決定）</li>
		<li>宛先ポート</li>
		<li>プロトコロ</li>
		<li>変換後のプライベートIPアドレス</li>
		<li>変換後の宛先ポート</li>
	</ul>


	<p>ここでは例としてグローバルIPアドレス(1.2.3.4)のhttp(ポート80番)宛の通信を<br />内部のwebサーバ(10.0.0.1)のポート8080にtcp接続する設定を以下に示す。</p>


<pre>
# set nat destination rule 10 destination address 1.2.3.4
# set nat destination rule 10 destination port 80
# set nat destination rule 10 inbound-interface eth1
# set nat destination rule 10 protocol tcp
# set nat destination rule 10 translation address 10.0.0.1
# set nat destination rule 10 translation port 8080
</pre>

	<p><strong>設定反映後、設定ファイルのバックアップを行う。</strong><br />必要であれば以降のファイアウォールルール変更を実施すること。</p>


	<a name="仮想ルータ設計_ファイアウォールルール適用変更手順"></a>
<h2 >ファイアウォールルール適用変更手順<a href="#仮想ルータ設計_ファイアウォールルール適用変更手順" class="wiki-anchor">&para;</a></h2>


	<p>ファイアウォールルールで変更できるのは以下の設定のみである。</p>


	<ul>
	<li>default_internet 内部からインターネットへのファイアウォールルール</li>
		<li>internet_default インターネットから内部へのファイアウォールルール</li>
	</ul>


	<p>デフォルト設定では全ての通信を許可しているため、<br />ディスティネーションNATを設定すると全て通信を通してしまう。<br />まずは全許可を無効として、ルールを設定していく。<br />ここでは例としてグローバルIPアドレス(1.2.3.4)のhttp(ポート80番)宛の通信を<br />内部のwebサーバ(10.0.0.1)のポート8080にtcp接続設定を以下に示す。<br />ここで注意を要するのはファイアウォールルールはディスティネーションNAT後に評価される点である。<br />つまり、ここにグローバルIPアドレスでの許可を設定しても通信できないことに注意する。</p>


<pre>
### 始めに全許可設定を解除する。デフォルトアクションが破棄となっているので、全ての通信が遮断される
# set firewall name internet_default rule 9999 disable
### 内部から通信を確立した際に関係のあるパケットを許可する設定
# set firewall name internet_default rule 10 action accept
# set firewall name internet_default rule 10 state established enable
# set firewall name internet_default rule 10 state related enable
### webサーバのポート8080宛の通信は許可する設定
# set firewall name internet_default rule 20 action accept
# set firewall name internet_default rule 20 protocol tcp
# set firewall name internet_default rule 20 destination address 10.0.0.1
# set firewall name internet_default rule 20 destination port 8080
</pre>

	<p><strong>設定反映後、設定ファイルのバックアップを行う。</strong></p>


	<a name="仮想ルータ設計_グローバルIPアドレス追加に伴うeth1のIPアドレス変更について"></a>
<h2 >グローバルIPアドレス追加に伴うeth1のIPアドレス変更について<a href="#仮想ルータ設計_グローバルIPアドレス追加に伴うeth1のIPアドレス変更について" class="wiki-anchor">&para;</a></h2>


	<p>グローバルIPアドレスが追加された場合、eth1のIPアドレスが10.16.37.0/24以外のものとなってしまう。</p>


<pre>
### 既存のIPアドレスを削除する
# delete interfaces ethernet eth1 address 10.16.37.XX/24
# delete interfaces ethernet eth1 address XXX.XXX.XXX.XXX/XX
### 新たなIPアドレスを変更する
# set interfaces ethernet eth1 address 10.16.XXX.XXX/24
# set interfaces ethernet eth1 address XXX.XXX.XXX.XXX/XX
### source/destination natの全てを新たなIPアドレスに変更する
# set nat destination XXXXXXXXXXXXXXX
# set nat source XXXXXXXXXXXXX
### ゲートウェイを変更する
# system gateway-address 10.16.XXX.254
</pre>

	<p>ただし上記の設定では10.16.37.0/24のインターフェイスがなくなり、DNSやNTPが引けなくなってしまう。<br />その場合は新たにbr1.21に接続するeth18を作成し、10.16.37.0/24のIPアドレスを付与する。<br />さらにスタティックルートを設定することで、今までと同様にDNS/NTPを参照することができる。<br />以下に、eth18追加後のコマンドを示す。</p>


<pre>
# set interfaces ethernet eth18 address 10.16.37.XXX/24
# set protocols static route 10.16.35.0/24 next-hop 10.16.37.254
# set zone-policy zone zone_internet interface eth18
</pre>

	<p><strong>設定反映後、設定ファイルのバックアップを行う。</strong></p>
<hr />
<a name="共通監視設計" />
<a name="共通監視設計_共通監視設計"></a>
<h1 >共通監視設計<a href="#共通監視設計_共通監視設計" class="wiki-anchor">&para;</a></h1>


	<a name="共通監視設計_始めに"></a>
<h2 >始めに<a href="#共通監視設計_始めに" class="wiki-anchor">&para;</a></h2>


	<p>今回監視で利用するミドルウェアはzabbixである。<br />zabbixの公式ドキュメントはとてもよくまとまっているので、<br />一読すると理解が早まる。<br />本設計にはzabbixで用いられる用語を多用するため<br />特に用語説明の記載がある以下のページは参考になる。<br /><a href="https://www.zabbix.com/documentation/2.2/jp/manual/concepts" class="external">コンセプトページ</a></p>


	<p>また、共通監視設計として規定する必要がある監視種別は以下である。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>監視種別</th>
			<th>共通設計として定義</th>
		</tr>
		<tr>
			<td>死活監視</td>
			<td>○</td>
		</tr>
		<tr>
			<td>リソース監視</td>
			<td>○</td>
		</tr>
		<tr>
			<td>アプリケーション監視</td>
			<td>△(ssh等の基本的なアプリケーションは定義する)</td>
		</tr>
		<tr>
			<td>プロセス監視</td>
			<td>○</td>
		</tr>
		<tr>
			<td>ログ監視</td>
			<td>×(ログ監視は単純だが、汎用性がなく閾値設定も職人芸となるため別の方法があればそちらを選択すること)</td>
		</tr>
		<tr>
			<td>Web監視</td>
			<td>×(各サービスで設計すべき項目)</td>
		</tr>
		<tr>
			<td>ファイル監視</td>
			<td>○(セキュリティリスクやヒューマンエラーの検知に利用可能である) ※現在は特に設定すべき項目なし</td>
		</tr>
	</table>




<hr />


	<a name="共通監視設計_観点"></a>
<h2 >観点<a href="#共通監視設計_観点" class="wiki-anchor">&para;</a></h2>


	<p>監視について以下の点を考慮した設計とする。</p>


	<ul>
	<li>サービスに影響のある監視</li>
		<li>サービスのパフォーマンス</li>
		<li>障害時のトレーサビリティ</li>
		<li>バックアップサービスの正常性</li>
	</ul>


	<p>監視において主な役割は、サービス提供への影響を迅速に通知することにある。<br />基本的にサービスは冗長化されているため片系の障害は許容できる。<br />そのため障害をなるべく早く知ることで、サービス断を発生させずに復旧することができる。</p>


	<p>サービス提供の品質を考慮した際、サービスのパフォーマンス低下は問題となる。<br />このことから、サービスのパフォーマンスを取得することも必須である。<br />パフォーマンスの取得はキャパシティプランニングの際の参考にもなる。</p>


	<p>復旧後の作業として、原因を特定し予防することが必要である。<br />その際に利用できる情報を保持することがトレーサビリティを向上させるため、<br />それらの情報も取得する。</p>


	<p>サーバの物理的な故障によりデータ損失が発生してもバックアップが存在すれば<br />ある程度の復旧は可能である。<br />バックアップは全てのサーバで行うため共通設計の段階で考慮し、<br />バックアップのための共通の仕組みを作成し監視に組み込む必要がある。</p>


	<p>また設計指針として運用後に足りない監視が出てくる可能性があるため、<br />それらを容易に追加できる構成にする。</p>


<hr />


	<a name="共通監視設計_アイテム"></a>
<h2 >アイテム<a href="#共通監視設計_アイテム" class="wiki-anchor">&para;</a></h2>


	<p>zabbixのデフォルトアイテム一覧を添付する。</p>


	<p>zabbixはデフォルトのアイテム以外はプロセスを作成し実行する。<br />この性質のため、ユーザパラメータ等を多用しない設計が望まれる。<br />保守に必要なHWベンダ固有のコマンド等もIPMIエージェント(iLoやiDRACに対応)で取得できる。<br />また、デフォルトのテンプレートは変更する可能性が低い値<br />(OSのバージョン等)や1分平均、5分平均といった同等の値も取得することから利用しない。<br />こうすることで書き込みが多い監視サーバのiowaitを低減させることに繋がる。</p>


	<p>以下に監視に必要なディスク容量の計算方法を示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>パラメータ</th>
			<th>算出式(バイト)</th>
		</tr>
		<tr>
			<td>Zabbix設定</td>
			<td>固定サイズ。通常は10MB以下</td>
		</tr>
		<tr>
			<td>ヒストリ</td>
			<td>days*(items/refresh rate)*24*3600*bytes<br />items :アイテム数<br />days : ヒストリを保存する日数<br />refresh rate : アイテムの平均更新速度<br />bytes : 1個の値を保存するために必要なバイト数。データベースエンジンにもよるが、通常50バイト</td>
		</tr>
		<tr>
			<td>トレンド</td>
			<td>days*(items/3600)*24*3600*bytes<br />items : アイテム数<br />days : トレンド履歴を保存する日数<br />bytes : 1個のトレンドを保存するために必要なバイト数。データベースエンジンにもよるが、通常128バイト</td>
		</tr>
		<tr>
			<td>イベント</td>
			<td>days*events*24*3600*bytes<br />events :1秒あたりのイベント数。最悪のケースでは1秒に1個のイベントが発生すると想定します。<br />days : イベント歴を保存する日数<br />bytes : 1個のイベントを保存するために必要なバイト数。データベースエンジンにもよるが、通常130バイト</td>
		</tr>
	</table>




	<p>必要ディスク容量 = 設定 + ヒストリ + トレンド + イベント [byte]</p>


	<p>基本的に保存期間ログの世代管理同様は400日とする。<br />共通設計としてアイテム数は42個程だが、計算を簡単にするために50とする。<br />監視するホスト数も50ホストとし、計2500アイテムである。<br />イベント数も秒間1とする。<br />更新速度は基本的に60秒である。</p>


	<p>上記の過程の元共通設計のみで必要なディスク容量を計算する。</p>


	<p>ヒストリ = days*(items/refresh rate)*24*3600*bytes = 400*(2500/60)*24*3600*50 = 72,000,000,000 byte = 72GB<br />トレンド = days*(items/3600)*24*3600*bytes = 400*(2500/3600)*24*3600*128 = 3,072,000,000 byte = 3GB<br />イベント = days*events*24*3600*bytes = 400*1*24*3600*130 = 4,492,800,000 = 4.5GB<br />必要ディスク容量 = 設定 + ヒストリ + トレンド + イベント = 10MB + 72GB + 3GB + 4.5GB = 80GB</p>
<hr />
<a name="共通設計" />
<a name="共通設計_共通設計"></a>
<h1 >共通設計<a href="#共通設計_共通設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#共通設計_共通設計">共通設計</a></li><li><a href="#共通設計_Introduction">●Introduction</a><ul><li><a href="#共通設計_目的">目的</a></li></ul>
</li><li><a href="#共通設計_構成">●構成</a><ul><li><a href="#共通設計_パーティション構成">パーティション構成</a></li><li><a href="#共通設計_物理サーバ構成">物理サーバ構成</a></li><li><a href="#共通設計_ホスト設定">ホスト設定</a></li><li><a href="#共通設計_リゾルバ設定">リゾルバ設定</a></li><li><a href="#共通設計_IPv6設定">IPv6設定</a></li><li><a href="#共通設計_システムパッケージ一覧">システムパッケージ一覧</a></li><li><a href="#共通設計_導入パッケージ">導入パッケージ</a></li><li><a href="#共通設計_システムサービス一覧">システムサービス一覧</a></li><li><a href="#共通設計_ポートアサイン">ポートアサイン</a></li><li><a href="#共通設計_zabbix-agent設定">zabbix agent設定</a></li><li><a href="#共通設計_ログローテート設定">ログローテート設定</a></li><li><a href="#共通設計_rsyslog設定">rsyslog設定</a></li><li><a href="#共通設計_ntp設定">ntp設定</a></li><li><a href="#共通設計_fstab設定">fstab設定</a></li><li><a href="#共通設計_sshd設定">sshd設定</a></li><li><a href="#共通設計_ユーザアカウント設定">ユーザ・アカウント設定</a></li><li><a href="#共通設計_sudo設定">sudo設定</a></li><li><a href="#共通設計_kdump設定">kdump設定</a></li><li><a href="#共通設計_sysstat設定">sysstat設定</a></li><li><a href="#共通設計_OS設定">OS設定</a></li><li><a href="#共通設計_ログ">ログ</a></li><li><a href="#共通設計_監視設計">監視設計</a></li><li><a href="#共通設計_ネットワーク設計">ネットワーク設計</a></li><li><a href="#共通設計_バックアップ設計">バックアップ設計</a></li><li><a href="#共通設計_リストア設計">リストア設計</a></li></ul></li></ul>


<hr />


	<a name="共通設計_Introduction"></a>
<h1 >●Introduction<a href="#共通設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="共通設計_目的"></a>
<h2 >目的<a href="#共通設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>本書はクラウドサービスのサーバ構成の内、共通で定義可能な部分を記載する。<br />これにより各設計書では共通以外の部分を記載するだけでよく、<br />重複した内容を除外できる。<br />また、この内容を元にkickstartによる自動インストールを行う。<br />kickstartで利用するks.cfgファイルを添付する。</p>


<hr />


	<a name="共通設計_構成"></a>
<h1 >●構成<a href="#共通設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="共通設計_パーティション構成"></a>
<h2 >パーティション構成<a href="#共通設計_パーティション構成" class="wiki-anchor">&para;</a></h2>


	<p>パーティション構成を以下に示す。<br />クラウド利用するサーバOS領域として300GBあるため、<br />共通設計で利用するパーティションはそこから割り当てることとする。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>デバイスファイル</th>
			<th>マウントポイント</th>
			<th>LVM</th>
			<th>FSタイプ</th>
			<th>サイズ</th>
			<th>備考</th>
		</tr>
		<tr>
			<td>/dev/sda1</td>
			<td>/boot</td>
			<td>-</td>
			<td>ext4</td>
			<td>500MB</td>
			<td>centosは500MBの割り当てを推奨している</td>
		</tr>
		<tr>
			<td>tmpfs</td>
			<td>/dev/shm</td>
			<td>-</td>
			<td>tmpfs</td>
			<td>メモリ容量に依存</td>
			<td><strong>※TODO 利用しないためfstabから除外しマウントしない方がよい</strong></td>
		</tr>
		<tr>
			<td>/dev/mapper/vg_system-lv_swap</td>
			<td>-</td>
			<td>○</td>
			<td>swap</td>
			<td>12GB</td>
			<td>メモリが多い程退避場所であるswapは必要となるがswap outする機会も減るため、無限に増加させる必要はない</td>
		</tr>
		<tr>
			<td>/dev/mapper/vg_system-lv_root</td>
			<td>/</td>
			<td>○</td>
			<td>ext4</td>
			<td>10GB</td>
			<td>ルートパーティションはある程度の容量を割り当てる</td>
		</tr>
		<tr>
			<td>/dev/mapper/vg_system-lv_home</td>
			<td>/home</td>
			<td>○</td>
			<td>ext4</td>
			<td>10GB</td>
			<td>homeは利用しないため、少ない容量を割り当てる</td>
		</tr>
		<tr>
			<td>/dev/mapper/vg_system-lv_tmp</td>
			<td>/tmp</td>
			<td>○</td>
			<td>ext4</td>
			<td>10GB</td>
			<td>作業スペースとして多くの容量を割り当てる</td>
		</tr>
		<tr>
			<td>/dev/mapper/vg_system-lv_usr</td>
			<td>/usr</td>
			<td>○</td>
			<td>ext4</td>
			<td>10GB</td>
			<td>-</td>
		</tr>
		<tr>
			<td>/dev/mapper/vg_system-lv_var</td>
			<td>/var</td>
			<td>○</td>
			<td>ext4</td>
			<td>10GB</td>
			<td>DB等のデータ領域として利用する場合は別途パーティション分割を行うこと</td>
		</tr>
		<tr>
			<td>/dev/mapper/vg_system-lv_var_log</td>
			<td>/var/log</td>
			<td>○</td>
			<td>ext4</td>
			<td>50GB</td>
			<td>運用ログは長期に渡り保存するため、多くの容量を確保する</td>
		</tr>
	</table>




	<p>上記のパーティション分割では300GBに満たない。<br />その理由として運用時のディスクが足りない場合を考慮し、ディスク容量増加の余地を残しておくためである。<br />LVMで作成しているため、容量増加はオンラインで可能である。<br />データ領域を持たないサーバ等は残りのディスク容量をデータ領域として利用可能である。</p>


	<p>以下に作成するボリューム領域を示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>ボリュームグループ名</th>
		</tr>
		<tr>
			<td>vg_system</td>
		</tr>
	</table>




	<p>パーティションの確認コマンドを以下に示す。</p>


<pre>
# df -HT 
# fdisk -l
</pre>

	<a name="共通設計_物理サーバ構成"></a>
<h2 >物理サーバ構成<a href="#共通設計_物理サーバ構成" class="wiki-anchor">&para;</a></h2>


	<p>個別に記載する。</p>


	<a name="共通設計_ホスト設定"></a>
<h2 >ホスト設定<a href="#共通設計_ホスト設定" class="wiki-anchor">&para;</a></h2>


	<p>/etc/hostsにクラウドで利用する全てのホスト名（idc-から始まるホスト名）を記載する。<br />これは既知のホスト名による名前解決コストを回避するためである。</p>


<pre>
127.0.0.1   localhost

### management
10.16.33.50  idc-opfw01
10.16.33.51  idc-opfw02
10.16.17.2   idc-oprt01
10.16.17.3   idc-oprt02
10.16.17.17  idc-ol3s0a
10.16.17.33  idc-ol2s0a
10.16.17.34  idc-ol2s0b
10.16.17.35  idc-ol2s05
10.16.33.36  idc-ol2s0c
10.16.17.37  idc-ol2s08
10.16.17.38  idc-ol2s09
10.16.17.69  idc-olog01 
10.16.17.73  idc-ontp01
10.16.17.129 idc-oist0a
10.16.17.130 idc-oist01
10.16.17.131 idc-oist02
10.16.33.81  idc-occr0a
10.16.33.82  idc-occr01
10.16.33.83  idc-occr02
10.16.33.98  idc-ohyp01
10.16.33.99  idc-ohyp02
10.16.33.100 idc-ohyp03
10.16.33.101 idc-ohyp04
10.16.33.102 idc-ohyp05
10.16.33.103 idc-ohyp06
10.16.33.104 idc-ohyp07
10.16.33.145 idc-omon0a
10.16.33.146 idc-omon01
10.16.33.147 idc-omon02
10.16.33.194 idc-onas01
### ipmi
10.16.34.82  idc-occr01i
10.16.34.83  idc-occr02i
10.16.34.98  idc-ohyp01i
10.16.34.99  idc-ohyp02i
10.16.34.100 idc-ohyp03i
10.16.34.101 idc-ohyp04i
10.16.34.102 idc-ohyp05i
10.16.34.103 idc-ohyp06i
10.16.34.104 idc-ohyp07i
10.16.34.146 idc-omon01i
10.16.34.147 idc-omon02i
10.16.34.194 idc-onas01i
### default network(for internet)
10.16.35.82  idc-occr01v
10.16.35.83  idc-occr02v
### strorage
10.16.24.129 idc-oist0a
10.16.24.130 idc-oist01
10.16.24.131 idc-oist02
10.16.25.129 idc-oist0a
10.16.25.130 idc-oist01
10.16.25.131 idc-oist02
10.16.41.98  idc-ohyp01s1
10.16.41.99  idc-ohyp02s1
10.16.41.100 idc-ohyp03s1
10.16.41.101 idc-ohyp04s1
10.16.41.102 idc-ohyp05s1
10.16.41.103 idc-ohyp06s1
10.16.41.104 idc-ohyp07s1
10.16.42.98  idc-ohyp01s1
10.16.42.99  idc-ohyp02s1
10.16.42.100 idc-ohyp03s1
10.16.42.101 idc-ohyp04s1
10.16.42.102 idc-ohyp05s1
10.16.42.103 idc-ohyp06s1
10.16.42.104 idc-ohyp07s1
### heartbeat
10.16.32.82  idc-occr01h
10.16.32.83  idc-occr02h
10.16.32.146 idc-omon01h
10.16.32.147 idc-omon02h
</pre>

	<a name="共通設計_リゾルバ設定"></a>
<h2 >リゾルバ設定<a href="#共通設計_リゾルバ設定" class="wiki-anchor">&para;</a></h2>


	<p>/etc/resolv.confに記載する内容を以下に示す。<br />DNSは冗長化されているため、<br />2つ指定する。</p>


<pre>
domain idc.local
nameserver 10.16.33.82
nameserver 10.16.33.83
</pre>

	<a name="共通設計_IPv6設定"></a>
<h2 >IPv6設定<a href="#共通設計_IPv6設定" class="wiki-anchor">&para;</a></h2>


	<p>IPv6はサポートしないため、設定変更を行う。<br />/etc/sysconfig/networkに記載する内容を以下に示す。</p>


<pre>
NETWORKING=yes
HOSTNAME=XXX
GATEWAY=[10.16.17.254|10.16.33.254] # 自身の管理IPによってGATEWAYの値を変更する
NETWORKING_IPV6=No
</pre>

	<p>/etc/modprobe.d/disable-ipv6.confに記載する内容を以下に示す。</p>


<pre>
options ipv6 disable=1
</pre>

	<a name="共通設計_システムパッケージ一覧"></a>
<h2 >システムパッケージ一覧<a href="#共通設計_システムパッケージ一覧" class="wiki-anchor">&para;</a></h2>


	<p>サーバの構成はMinimalを選択、以下のグループパッケージをインストールする。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>グループ</th>
			<th>グループパッケージ名</th>
		</tr>
		<tr>
			<td>ベースシステム</td>
			<td>ベース</td>
		</tr>
		<tr>
			<td>開発</td>
			<td>開発ツール</td>
		</tr>
		<tr>
			<td>言語</td>
			<td>日本語サポート</td>
		</tr>
	</table>




	<a name="共通設計_導入パッケージ"></a>
<h2 >導入パッケージ<a href="#共通設計_導入パッケージ" class="wiki-anchor">&para;</a></h2>


	<p>各サーバ基本的にインターネット環境に接続できない。<br />そのためpackage管理ソフトのyumリポジトリはローカル環境にある。<br />以下に設定ファイルを示す。</p>


<pre>
# cat /etc/yum.repos.d/CentOS-Base.repo
[base-local]
name=CentOS-$releasever - Base
baseurl=http://10.16.33.82:8888/mrepo/centos$releasever-$basearch/RPMS.os/
gpgcheck=0

[updates-local]
name=CentOS-$releasever - Updates
baseurl=http://10.16.33.82:8888/mrepo/centos$releasever-$basearch/RPMS.updates/
gpgcheck=0

[extras-local]
name=CentOS-$releasever - Extras
baseurl=http://10.16.33.82:8888/mrepo/centos$releasever-$basearch/RPMS.extras/
gpgcheck=0

[centosplus-local]
name=CentOS-$releasever - Plus
baseurl=http://10.16.33.82:8888/mrepo/centos$releasever-$basearch/RPMS.centosplus/
gpgcheck=0
enabled=0

[contrib-local]
name=CentOS-$releasever - Contrib
baseurl=http://10.16.33.82:8888/mrepo/centos$releasever-$basearch/RPMS.contrib/
gpgcheck=0
enabled=0

[fasttrack-local]
name=CentOS-$releasever - Fasttrack
baseurl=http://10.16.33.82:8888/mrepo/centos$releasever-$basearch/RPMS.fasttrack/
gpgcheck=0
enabled=0
</pre>

	<p>上記設定ファイルの更新後、以下のコマンドの実行でアップデートを行う。</p>


<pre>
yum -y update
</pre>

	<p>基本的なパッケージに加え、以下のパッケージをインストールする。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>パッケージ名</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>zabbix-agent-2.4.3-1.el6.x86_64</td>
			<td>監視サーバの指示に従い実際に監視を行うエージェント</td>
		</tr>
		<tr>
			<td>telnet-0.17-48.el6.x86_64</td>
			<td>各種サービスの確認用</td>
		</tr>
	</table>




	<a name="共通設計_システムサービス一覧"></a>
<h2 >システムサービス一覧<a href="#共通設計_システムサービス一覧" class="wiki-anchor">&para;</a></h2>


	<p>自動起動設定を以下の状態になるように設定する。</p>


<pre>
# chkconfig 
abrt-ccpp          0:off    1:off    2:off    3:on    4:off    5:on    6:off
abrtd              0:off    1:off    2:off    3:on    4:off    5:on    6:off
acpid              0:off    1:off    2:on    3:on    4:on    5:on    6:off
atd                0:off    1:off    2:off    3:on    4:on    5:on    6:off
auditd             0:off    1:off    2:off    3:off    4:off    5:off    6:off # SELinuxは利用しないのでoffに変更
blk-availability    0:off    1:on    2:on    3:on    4:on    5:on    6:off
cpuspeed           0:off    1:on    2:on    3:on    4:on    5:on    6:off
crond              0:off    1:off    2:on    3:on    4:on    5:on    6:off
haldaemon          0:off    1:off    2:off    3:off    4:off    5:off    6:off # モニター関連のデーモン。不要なのでoffに変更
ip6tables          0:off    1:off    2:off    3:off    4:off    5:off    6:off # offに変更
iptables           0:off    1:off    2:off    3:off    4:off    5:off    6:off # offに変更
irqbalance         0:off    1:off    2:off    3:on    4:on    5:on    6:off
kdump              0:off    1:off    2:off    3:on    4:on    5:on    6:off
lvm2-monitor       0:off    1:on    2:on    3:on    4:on    5:on    6:off
mdmonitor          0:off    1:off    2:off    3:off    4:off    5:off    6:off # ソフトウェアRAIDは利用しないのでoffに変更
messagebus         0:off    1:off    2:off    3:off    4:off    5:off    6:off # haldaemonと同様に不要なのでoffに変更
netconsole         0:off    1:off    2:off    3:off    4:off    5:off    6:off
netfs              0:off    1:off    2:off    3:on    4:on    5:on    6:off
network            0:off    1:off    2:on    3:on    4:on    5:on    6:off
ntpd               0:off    1:off    2:on    3:on    4:on    5:on    6:off # 時刻同期を利用するのでonに変更
ntpdate            0:off    1:off    2:on    3:on    4:on    5:on    6:off # 時刻同期を利用するのでonに変更
postfix            0:off    1:off    2:off    3:off    4:off    5:off    6:off # メールは利用しないのでoffに変更
psacct             0:off    1:off    2:off    3:off    4:off    5:off    6:off
quota_nld          0:off    1:off    2:off    3:off    4:off    5:off    6:off
rdisc              0:off    1:off    2:off    3:off    4:off    5:off    6:off
restorecond        0:off    1:off    2:off    3:off    4:off    5:off    6:off
rngd               0:off    1:off    2:off    3:off    4:off    5:off    6:off
rsyslog            0:off    1:off    2:on    3:on    4:on    5:on    6:off
saslauthd          0:off    1:off    2:off    3:off    4:off    5:off    6:off
smartd             0:off    1:off    2:off    3:off    4:off    5:off    6:off
sshd               0:off    1:off    2:on    3:on    4:on    5:on    6:off
svnserve           0:off    1:off    2:off    3:off    4:off    5:off    6:off
sysstat            0:off    1:on    2:on    3:on    4:on    5:on    6:off
udev-post          0:off    1:on    2:on    3:on    4:on    5:on    6:off
zabbix-agent    0:off    1:on    2:on    3:on    4:on    5:on    6:off # 監視エージェントは全てのサーバに共通でインストールする
</pre>

	<a name="共通設計_ポートアサイン"></a>
<h2 >ポートアサイン<a href="#共通設計_ポートアサイン" class="wiki-anchor">&para;</a></h2>


	<p>全ての物理サーバは1Gb EthernetクアッドポートのNIC2枚(計8ポート)を搭載している。<br />サーバによってデフォルトではemX,pXpXといった名称となっているが、<br />それらをeth0から命名し直すこととする。<br />こうすることで基本的な設定ファイルを共有することができる。</p>


	<p>NICを冗長化するために物理NICを跨いだbondingの設定を行う。<br />インターフェイスとbondingの関係を以下に示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>NIC番号</th>
			<th>インターフェイス名</th>
			<th>Bondingインターフェイス名</th>
		</tr>
		<tr>
			<td>1</td>
			<td>eth0</td>
			<td>bond0</td>
		</tr>
		<tr>
			<td>1</td>
			<td>eth1</td>
			<td>bond1</td>
		</tr>
		<tr>
			<td>1</td>
			<td>eth2</td>
			<td>bond2</td>
		</tr>
		<tr>
			<td>1</td>
			<td>eth3</td>
			<td>bond3</td>
		</tr>
		<tr>
			<td>2</td>
			<td>eth4</td>
			<td>bond0</td>
		</tr>
		<tr>
			<td>2</td>
			<td>eth5</td>
			<td>bond1</td>
		</tr>
		<tr>
			<td>2</td>
			<td>eth6</td>
			<td>bond2</td>
		</tr>
		<tr>
			<td>2</td>
			<td>eth7</td>
			<td>bond3</td>
		</tr>
	</table>




	<p>インターフェイスの設定を以下に示す。</p>


	<ul>
	<li>/etc/sysconfig/network-scripts/ifcfg-bond0</li>
	</ul>


	<p>このインターフェイスは管理用のIPを持つ。<br />kickstartでのPXEブート時にDHCPを利用する関係上、このインターフェイスはlacpモードではなく<br />active-backupモードを利用する。<br />そのため、対向のL2SWについてアグリゲーションの設定ではなく、通常のインターフェイス設定とする。</p>


<pre>
DEVICE=bond0
ONBOOT=yes
BOOTPROTO=none
IPADDR=XXX.XXX.XXX.XXX
NETMASK=255.255.255.0
BONDING_OPTS="mode=1 miimon=100 updelay=0" 
</pre>

	<ul>
	<li>/etc/sysconfig/network-scripts/ifcfg-bondX</li>
	</ul>


	<p>その他のbondingインターフェイスはlacpモードで稼働させる。<br />対抗のL2SWもアグリゲーションの設定をしているため、<br />lacpモードでないと通信できない。</p>


<pre>
DEVICE=bondX
ONBOOT=yes
BOOTPROTO=none
IPADDR=XXX.XXX.XXX.XXX
NETMASK=255.255.255.0
BONDING_OPTS="mode=4 miimon=100 updelay=0" 
</pre>

	<ul>
	<li>/etc/sysconfig/network-scripts/ifcfg-ethX</li>
	</ul>


<pre>
DEVICE=ethX
BOOTPROTO=none
NM_CONTROLLED=no
ONBOOT="yes" 
TYPE="Ethernet" 
MASTER=bondX
SLAVE=yes
</pre>

	<a name="共通設計_zabbix-agent設定"></a>
<h2 >zabbix agent設定<a href="#共通設計_zabbix-agent設定" class="wiki-anchor">&para;</a></h2>


	<p>/etc/zabbix_agentd.confは以下の状態になるよう変更を加える。<br />ホスト自動登録プロセスを行うため、<br />HostMetadataItemを定義する。</p>


<pre>
$ egrep -v "(^$|^#)" zabbix_agentd.conf
PidFile=/var/run/zabbix/zabbix_agentd.pid
LogFile=/var/log/zabbix/zabbix_agentd.log
LogFileSize=0
Server=idc-omon0a
ServerActive=idc-omon0a
HostnameItem=system.hostname
HostMetadataItem=system.hostname
AllowRoot=1
Include=/etc/zabbix/zabbix_agentd.d/
</pre>

	<a name="共通設計_ログローテート設定"></a>
<h2 >ログローテート設定<a href="#共通設計_ログローテート設定" class="wiki-anchor">&para;</a></h2>


	<p>ログは400日間(1年と1ヶ月程度)保存するものとする。<br />ログファイルの設置場所は/var/log配下にアプリケーション用のディレクトリを作成することとする。<br />ローテーション時は該当日付をファイル名に付与し、ファイル圧縮も行う。</p>


	<ul>
	<li>/etc/logrotate.d/zabbix_agentd</li>
	</ul>


<pre>
/var/log/zabbix/zabbix_agentd.log {
    daily
    create 600 zabbix zabbix
    rotate 400
    ifempty
    missingok
    dateext
    compress
}
</pre>

	<a name="共通設計_rsyslog設定"></a>
<h2 >rsyslog設定<a href="#共通設計_rsyslog設定" class="wiki-anchor">&para;</a></h2>


	<p>/etc/rsyslog.confは以下の状態になるように設定する。</p>


<pre>
# egrep -v "(^$|^#)" /etc/rsyslog.conf
$ModLoad imuxsock # provides support for local system logging (e.g. via logger command)
$ModLoad imklog   # provides kernel logging support (previously done by rklogd)
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat
$IncludeConfig /etc/rsyslog.d/*.conf
*.info;mail.none;authpriv.none;cron.none                /var/log/messages
authpriv.*                                              /var/log/secure
mail.*                                                  -/var/log/maillog
cron.*                                                  /var/log/cron
*.emerg                                                 *
uucp,news.crit                                          /var/log/spooler
local7.*                                                /var/log/boot.log
</pre>

	<a name="共通設計_ntp設定"></a>
<h2 >ntp設定<a href="#共通設計_ntp設定" class="wiki-anchor">&para;</a></h2>


	<p>/etc/ntp.confは以下の状態になるように設定する。</p>


<pre>
# egrep -v "(^$|^#)" /etc/ntp.conf
driftfile /var/lib/ntp/drift
### FWがいるので、基本的にすべての通信を許可する
restrict default
server idc-occr01 iburst
server idc-occr02 iburst
includefile /etc/ntp/crypto/pw
keys /etc/ntp/keys
</pre>

	<a name="共通設計_fstab設定"></a>
<h2 >fstab設定<a href="#共通設計_fstab設定" class="wiki-anchor">&para;</a></h2>


	<p>バックアップ様にNFSへのマウントを追加する。</p>


<pre>
# egrep -v "(^$|^#)" /etc/fstab 
/dev/mapper/VolGroup-lv_root /                       ext4    defaults        1 1
UUID=5ba086fb-2b97-479f-9806-fb2abd32eee1 /boot                   ext4    defaults        1 2
/dev/mapper/VolGroup-lv_swap swap                    swap    defaults        0 0
tmpfs                   /dev/shm                tmpfs   defaults        0 0
devpts                  /dev/pts                devpts  gid=5,mode=620  0 0
sysfs                   /sys                    sysfs   defaults        0 0
proc                    /proc                   proc    defaults        0 0
</pre>

	<a name="共通設計_sshd設定"></a>
<h2 >sshd設定<a href="#共通設計_sshd設定" class="wiki-anchor">&para;</a></h2>


	<p>ユーザアカウントにて定義されたユーザの公開鍵を用いて、<br />公開鍵認証のみを許可する。<br />rootログインについては許可しない。</p>


<pre>
# egrep -v "(^$|^#)" /etc/ssh/sshd_config 
Protocol 2
SyslogFacility AUTHPRIV
PermitRootLogin no
RSAAuthentication yes
PubkeyAuthentication yes
AuthorizedKeysFile    .ssh/authorized_keys
PasswordAuthentication no
ChallengeResponseAuthentication no
GSSAPIAuthentication yes
GSSAPICleanupCredentials yes
UsePAM yes
AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES
AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT
AcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGE
AcceptEnv XMODIFIERS
X11Forwarding yes
Subsystem    sftp    /usr/libexec/openssh/sftp-server
UseDNS no
</pre>

	<a name="共通設計_ユーザアカウント設定"></a>
<h2 >ユーザ・アカウント設定<a href="#共通設計_ユーザアカウント設定" class="wiki-anchor">&para;</a></h2>


	<p>各作業者のユーザアカウントを作成する。<br />作業者は必ずisbグループに所属する。<br />また、各ユーザはsshログイン時に公開鍵認証を行うため、<br />公開鍵を作成する必要がある。<br />コマンド例を以下に示す。</p>


<pre>
# useradd -g 1001 -u 10XX &lt;worker&gt;
# passwd &lt;worker&gt;
</pre>

	<p>グループIDは1001からの連番とする。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>グループ</th>
			<th>グループID</th>
			<th>パスワード</th>
		</tr>
		<tr>
			<td>isb</td>
			<td>1001</td>
			<td>isbadmin</td>
		</tr>
		<tr>
			<td>servermanager</td>
			<td>1002</td>
			<td>-</td>
		</tr>
		<tr>
			<td>kvmmanager</td>
			<td>1003</td>
			<td>-</td>
		</tr>
	</table>




	<p>ユーザIDは1001からの連番であるが、<br />1100までは作業者用の予約領域(100人分)とする。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>ユーザ</th>
			<th>ユーザID</th>
			<th>グループID</th>
			<th>パスワード</th>
		</tr>
		<tr>
			<td>ta1</td>
			<td>1001</td>
			<td>1001</td>
			<td>ta1</td>
		</tr>
		<tr>
			<td>suga</td>
			<td>1002</td>
			<td>1001</td>
			<td>suga</td>
		</tr>
		<tr>
			<td>kajiro</td>
			<td>1003</td>
			<td>1001</td>
			<td>kajiro</td>
		</tr>
		<tr>
			<td>akiba</td>
			<td>1004</td>
			<td>1001</td>
			<td>akiba</td>
		</tr>
		<tr>
			<td>servermanager</td>
			<td>1101</td>
			<td>1002</td>
			<td>servermanager</td>
		</tr>
		<tr>
			<td>kvmmanager</td>
			<td>1102</td>
			<td>1003</td>
			<td>kvmmanager</td>
		</tr>
	</table>




	<a name="共通設計_sudo設定"></a>
<h2 >sudo設定<a href="#共通設計_sudo設定" class="wiki-anchor">&para;</a></h2>


<pre>
# egrep -v "(^$|^#)" /etc/sudoers
Defaults    requiretty
Defaults   !visiblepw
Defaults    always_set_home
Defaults    env_reset
Defaults    env_keep =  "COLORS DISPLAY HOSTNAME HISTSIZE INPUTRC KDEDIR LS_COLORS" 
Defaults    env_keep += "MAIL PS1 PS2 QTDIR USERNAME LANG LC_ADDRESS LC_CTYPE" 
Defaults    env_keep += "LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES" 
Defaults    env_keep += "LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE" 
Defaults    env_keep += "LC_TIME LC_ALL LANGUAGE LINGUAS _XKB_CHARSET XAUTHORITY" 
Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin
root    ALL=(ALL)     ALL
%isb    ALL=(ALL)     ALL
servermanager   ALL=(ALL)       NOPASSWD:ALL
</pre>

	<a name="共通設計_kdump設定"></a>
<h2 >kdump設定<a href="#共通設計_kdump設定" class="wiki-anchor">&para;</a></h2>


	<a name="共通設計_sysstat設定"></a>
<h2 >sysstat設定<a href="#共通設計_sysstat設定" class="wiki-anchor">&para;</a></h2>


<pre>
# egrep -v "(^$|^#)" /etc/sysconfig/sysstat
HISTORY=400
COMPRESSAFTER=31
SADC_OPTIONS="-S DISK" 
ZIP="bzip2" 
</pre>

	<a name="共通設計_OS設定"></a>
<h2 >OS設定<a href="#共通設計_OS設定" class="wiki-anchor">&para;</a></h2>


	<p>Ctrl+Alt+Del押下による再起動を抑止するための設定を入れ込む。<br />/etc/init/control-alt-delete.overrideに記載する内容を以下に示す。<br />（control-alt-delete.confの設定変更はyum update等で上書きされる可能性がある）</p>


<pre>
exec logger "Control-Alt-Delete disabled" 
</pre>

	<p>/etc/inittabに記載する内容を以下に示す。</p>


<pre>
id:3:initdefault:
</pre>

	<p>/boot/grub/grub.confに記載する内容を以下に示す。</p>


<pre>

</pre>

	<p>selinuxは使用しないので、/etc/sysconfig/selinuxを編集する。</p>


<pre>
SELINUX=disabled
SELINUXTYPE=targeted
</pre>

	<a name="共通設計_ログ"></a>
<h2 >ログ<a href="#共通設計_ログ" class="wiki-anchor">&para;</a></h2>


	<p>出力ログの一覧を以下に示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>ファイル</th>
			<th>パーミッション</th>
			<th>所有者</th>
			<th>グループ</th>
		</tr>
		<tr>
			<td>/var/log/audit/audit.log</td>
			<td>600</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/boot.log</td>
			<td>600</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/btmp</td>
			<td>600</td>
			<td>root</td>
			<td>utmp</td>
		</tr>
		<tr>
			<td>/var/log/cron</td>
			<td>600</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/dmesg</td>
			<td>644</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/dracut.log</td>
			<td>644</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/lastlog</td>
			<td>644</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/maillog</td>
			<td>600</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/messages</td>
			<td>600</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/secure</td>
			<td>600</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/spooler</td>
			<td>600</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/tallylog</td>
			<td>600</td>
			<td>root</td>
			<td>root</td>
		</tr>
		<tr>
			<td>/var/log/wtmp</td>
			<td>664</td>
			<td>root</td>
			<td>root</td>
		</tr>
	</table>




	<a name="共通設計_監視設計"></a>
<h2 >監視設計<a href="#共通設計_監視設計" class="wiki-anchor">&para;</a></h2>


	<p>監視に関しては記載項目が多いので、別ページにまとめる。</p>


	<p><a href="#共通監視設計" class="wiki-page">共通監視設計</a></p>


	<a name="共通設計_ネットワーク設計"></a>
<h2 >ネットワーク設計<a href="#共通設計_ネットワーク設計" class="wiki-anchor">&para;</a></h2>


	<p>ネットワーク設計に関しては「【共通】ネットワークアドレス管理台帳.xlsx」を参照のこと。</p>


	<a name="共通設計_バックアップ設計"></a>
<h2 >バックアップ設計<a href="#共通設計_バックアップ設計" class="wiki-anchor">&para;</a></h2>


	<p>バックアップの目的として障害時のサービス復旧および原因のトレーサビリティが挙げられる。<br />RPOを24時間、RTOは限りなく0を目指す。<br />このことから、HA等で冗長化されており、サービスに直結する部分のバックアップとログのバックアップを毎日行う必要がある。<br />サービスに直結するバックアップとしてDBのダンプ取得、ログのバックアップは/var/log配下の圧縮が挙げられる。<br />これらのバックアップは3世代分を管理する（ログの世代管理とは異なる）。</p>


	<a name="共通設計_リストア設計"></a>
<h2 >リストア設計<a href="#共通設計_リストア設計" class="wiki-anchor">&para;</a></h2>


	<p>まず、ログのバックアップに関してリストアしない方針とする。<br />DBに関してはダンプファイルのリストアを行うが、詳細に関しては各サービスの設計を参照すること。</p>
<hr />
<a name="監視プロキシ設計" />
<a name="監視プロキシ設計_監視プロキシ設計"></a>
<h1 >監視プロキシ設計<a href="#監視プロキシ設計_監視プロキシ設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#監視プロキシ設計_監視プロキシ設計">監視プロキシ設計</a></li><li><a href="#監視プロキシ設計_Introduction">●Introduction</a><ul><li><a href="#監視プロキシ設計_目的">目的</a></li></ul>
</li><li><a href="#監視プロキシ設計_構成">●構成</a><ul><li><a href="#監視プロキシ設計_サーバ構成">サーバ構成</a></li><li><a href="#監視プロキシ設計_パーティション構成">パーティション構成</a></li><li><a href="#監視プロキシ設計_物理サーバ構成">物理サーバ構成</a></li><li><a href="#監視プロキシ設計_導入パッケージ">導入パッケージ</a></li><li><a href="#監視プロキシ設計_システムサービス一覧">システムサービス一覧</a></li><li><a href="#監視プロキシ設計_ポートアサイン">ポートアサイン</a></li><li><a href="#監視プロキシ設計_ログ設定">ログ設定</a><ul><li><a href="#監視プロキシ設計_ログローテ">ログローテ</a></li></ul>
</li><li><a href="#監視プロキシ設計_セキュリティ">セキュリティ</a><ul><li><a href="#監視プロキシ設計_sshの待ち受けIPアドレス変更">sshの待ち受けIPアドレス変更</a></li><li><a href="#監視プロキシ設計_zabbix-proxyの待ち受けIPアドレス変更">zabbix proxyの待ち受けIPアドレス変更</a></li></ul>
</li></ul>
</li><li><a href="#監視プロキシ設計_システム設計">●システム設計</a><ul><li><a href="#監視プロキシ設計_postgresql設定">postgresql設定</a><ul><li><a href="#監視プロキシ設計_postgresqlconf">postgresql.conf</a></li><li><a href="#監視プロキシ設計_pg_hbaconf">pg_hba.conf</a></li><li><a href="#監視プロキシ設計_データベースユーザ">データベース・ユーザ</a></li></ul>
</li><li><a href="#監視プロキシ設計_zabbix設定">zabbix設定</a></li></ul>
</li><li><a href="#監視プロキシ設計_冗長化">●冗長化</a></li><li><a href="#監視プロキシ設計_バックアップ">●バックアップ</a></li></ul>


<hr />


	<a name="監視プロキシ設計_Introduction"></a>
<h1 >●Introduction<a href="#監視プロキシ設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="監視プロキシ設計_目的"></a>
<h2 >目的<a href="#監視プロキシ設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>本書はクラウドシステムで利用される監視プロキシについて定義する。<br />本書は仕様書・監視設計書の既読者を対象とする。</p>


<hr />


	<a name="監視プロキシ設計_構成"></a>
<h1 >●構成<a href="#監視プロキシ設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="監視プロキシ設計_サーバ構成"></a>
<h2 >サーバ構成<a href="#監視プロキシ設計_サーバ構成" class="wiki-anchor">&para;</a></h2>


	<p>zabbix proxyについては仮想化基盤上に構築し、<br />各user毎にzabbix proxyインスタンスを作成する。<br />zabbix proxyはuserの全てのNWを持つ必要がある。<br />冗長化は行わないため、<br />zabbix proxy関連の障害発生時にzabbixサーバへ未転送の監視データは消失してしまう。<br />仮想インスタンスにおける障害発生時のフェールオーバ設定が複雑であることと、<br />監視データ自体には重要度が低いことを考慮すると監視データの消失を許容する。</p>


	<a name="監視プロキシ設計_パーティション構成"></a>
<h2 >パーティション構成<a href="#監視プロキシ設計_パーティション構成" class="wiki-anchor">&para;</a></h2>


	<p>zabbix proxyは通常仮想インスタンスに割り当てられる4GBのシステム領域に加え、zabbix proxy用のデータ領域をもつものとする。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>ボリュームグループ名</th>
			<th>ロジカルボリューム名</th>
		</tr>
		<tr>
			<td>vg_data</td>
			<td>lv_postgres</td>
		</tr>
	</table>




	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>デバイスファイル</th>
			<th>マウントポイント</th>
			<th>LVM</th>
			<th>FSタイプ</th>
			<th>サイズ</th>
			<th>備考</th>
		</tr>
		<tr>
			<td>/dev/vg_data/lv_postgres</td>
			<td>/var/lib/pgsql/9.4/data</td>
			<td>○</td>
			<td>ext4</td>
			<td>5GB</td>
			<td>postgresが利用するデータディレクトリ</td>
		</tr>
	</table>




	<p>KVMサーバのLVM領域として以下の名称で割り当てる。</p>


	<ul>
	<li>/dev/vg_idc-01/lv_idc-ompxXXX: システム領域 + データ領域</li>
	</ul>


	<a name="監視プロキシ設計_物理サーバ構成"></a>
<h2 >物理サーバ構成<a href="#監視プロキシ設計_物理サーバ構成" class="wiki-anchor">&para;</a></h2>


	<p>仮想インスタンの設定情報を以下に示す。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr>
			<th style="background:#d3eaf3;text-align:left;">CPU</th>
			<td>x86_64 2C</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">メモリ</th>
			<td>4GB</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">ディスク</th>
			<td>9GB</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">NIC</th>
			<td>17ポート</td>
		</tr>
	</table>




	<a name="監視プロキシ設計_導入パッケージ"></a>
<h2 >導入パッケージ<a href="#監視プロキシ設計_導入パッケージ" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>パッケージ名</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>acpid</td>
			<td>KVMから停止をさせるために必要</td>
		</tr>
		<tr>
			<td>postgresql94-server.x86_64</td>
			<td>RDBMSの代表的なOSSであるpostgresql server</td>
		</tr>
		<tr>
			<td>postgresql94-libs-9.4.1</td>
			<td>postgresqlの開発関連ライブラリ</td>
		</tr>
		<tr>
			<td>postgresql94-contrib-9.4.1</td>
			<td>postgresqlの拡張機能</td>
		</tr>
		<tr>
			<td>postgresql94-9.4.1</td>
			<td>psqlコマンド</td>
		</tr>
		<tr>
			<td>zabbix-proxy</td>
			<td>zabbix proxyサービス</td>
		</tr>
	</table>




	<a name="監視プロキシ設計_システムサービス一覧"></a>
<h2 >システムサービス一覧<a href="#監視プロキシ設計_システムサービス一覧" class="wiki-anchor">&para;</a></h2>


<pre>
acpid           0:off   1:off   2:on    3:on    4:on    5:on    6:off
postgresql-9.4  0:off   1:off   2:on   3:on   4:on   5:on   6:off
zabbix-proxy    0:off    1:on    2:on    3:on    4:on    5:on    6:off
</pre>

	<a name="監視プロキシ設計_ポートアサイン"></a>
<h2 >ポートアサイン<a href="#監視プロキシ設計_ポートアサイン" class="wiki-anchor">&para;</a></h2>


	<p>zabbix proxyはユーザ用NW全てにアクセスできるようにするため、<br />ユーザに割り当てられた16VLAN全てのNICを追加しておく。<br />それに加え、zabbix proxy用のクラウド管理NW用のNICの追加も必要となる。<br />各VLANのサブネットは24bitで分割されているため、<br />zabbix proxyのユーザ用NWにおける第四オクテットは253を利用する。<br />クラウド管理NWの第四オクテットは事業者IDと一致させる。</p>


	<p>また、デフォルトゲートウェイとして10.16.39.254を指定する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>インターフェイス</th>
			<th>ホストOS側のインターフェイス</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>eth0</td>
			<td>br1.23</td>
			<td>クラウド管理NW用。10.16.39.[事業者ID]のIPアドレスを持つ</td>
		</tr>
		<tr>
			<td>eth1 ～ 16</td>
			<td>br2.XXXX (事業者に割り当てられるVLAN番号)</td>
			<td>ユーザ用NW。特にeth4はユーザ用デフォルトNWである。</td>
		</tr>
	</table>




	<a name="監視プロキシ設計_ログ設定"></a>
<h2 >ログ設定<a href="#監視プロキシ設計_ログ設定" class="wiki-anchor">&para;</a></h2>


	<p>ログの出力先を以下に示す。<br />zabbix proxyではログをsyslogに出力するため、<br />ローカルでは通常のsyslogの出力先にのみ保存する。</p>


<pre>
/var/log/zabbix/zabbix_proxy.log
/var/log/pgsql/postgresql.log
</pre>

	<p>また、syslogを利用するため、<br />messagesファイルへの出力を停止して上記ログファイルへ送る設定、<br />および、リモートのsyslogへ転送する設定を記述する。</p>


	<ul>
	<li>/etc/rsyslog.conf</li>
	</ul>


<pre>
$ModLoad imuxsock # provides support for local system logging (e.g. via logger command)
$ModLoad imklog   # provides kernel logging support (previously done by rklogd)
$ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat
$IncludeConfig /etc/rsyslog.d/*.conf
*.info;mail.none;authpriv.none;cron.none;local6.none;daemon.none                /var/log/messages/messages
authpriv.*                                              /var/log/secure/secure
mail.*                                                  -/var/log/mail/maillog
cron.*                                                  /var/log/cron/cron
*.emerg                                                 *
uucp,news.crit                                          /var/log/spooler/spooler
local7.*                                                /var/log/boot/boot.log
local6.* /var/log/pgsql/postgresql.log
daemon.* /var/log/zabbix/zabbix_proxy.log
local6.*;daemon.* @10.16.33.194
</pre>

	<a name="監視プロキシ設計_ログローテ"></a>
<h3 >ログローテ<a href="#監視プロキシ設計_ログローテ" class="wiki-anchor">&para;</a></h3>


	<p>30世代、約1ヶ月分のログを保持することとする。<br />zabbix proxyは余分なディスクを持っていないため、<br />ログローテートでsizeを指定し最大で各150MB、計300MBとなるように設定する。</p>


	<ul>
	<li>/etc/logrotate.d/zabbix-proxy</li>
	</ul>


<pre>
/var/log/zabbix/zabbix_proxy.log {
    daily
    create 600 root root
    rotate 30
    size 5M
    ifempty
    missingok
    dateext
    compress
}
</pre>

	<ul>
	<li>/etc/logrotate.d/zabbix-agent</li>
	</ul>


<pre>
/var/log/zabbix/zabbix_agentd.log {
    daily
    create 600 root root
    rotate 30
    size 5M
    ifempty
    missingok
    dateext
    compress
}
</pre>

	<ul>
	<li>/etc/logrotate.d/postgresql</li>
	</ul>


<pre>
/var/log/pgsql/postgresql.log {
    daily
    create 600 root root
    rotate 30
    size 5M
    ifempty
    missingok
    dateext
    compress
}
</pre>

	<a name="監視プロキシ設計_セキュリティ"></a>
<h2 >セキュリティ<a href="#監視プロキシ設計_セキュリティ" class="wiki-anchor">&para;</a></h2>


	<p>zabbix proxyは事業者NWの中にいるので、<br />事業者からの接続は受け付けないように設定する必要がある。</p>


	<a name="監視プロキシ設計_sshの待ち受けIPアドレス変更"></a>
<h3 >sshの待ち受けIPアドレス変更<a href="#監視プロキシ設計_sshの待ち受けIPアドレス変更" class="wiki-anchor">&para;</a></h3>


	<p>クラウド管理NWからの通信のみ許可する。</p>


	<a name="監視プロキシ設計_zabbix-proxyの待ち受けIPアドレス変更"></a>
<h3 >zabbix proxyの待ち受けIPアドレス変更<a href="#監視プロキシ設計_zabbix-proxyの待ち受けIPアドレス変更" class="wiki-anchor">&para;</a></h3>


	<p>zabbix proxyは事業者のVMを監視する必要があるので、<br />listen ipは制限できない。</p>


<hr />


	<a name="監視プロキシ設計_システム設計"></a>
<h1 >●システム設計<a href="#監視プロキシ設計_システム設計" class="wiki-anchor">&para;</a></h1>


	<a name="監視プロキシ設計_postgresql設定"></a>
<h2 >postgresql設定<a href="#監視プロキシ設計_postgresql設定" class="wiki-anchor">&para;</a></h2>


	<a name="監視プロキシ設計_postgresqlconf"></a>
<h3 >postgresql.conf<a href="#監視プロキシ設計_postgresqlconf" class="wiki-anchor">&para;</a></h3>


	<p>zabbixのみがpostgresにアクセスするため基本的にはデフォルトの設定を利用する。<br />ログローテートの設定および、shared_buffesの設定を入れ込む。<br />また、zabbix proxyはディスク節約のため、ログをsyslogへ出力する。<br />出力の際のfacilityをLOCAL6に設定する。</p>


	<ul>
	<li>/var/lib/pgsql/9.4/data/postgresql.conf</li>
	</ul>


<pre>
max_connections = 100
shared_buffers = 1024MB # zabbix proxyは4GBのメモリを想定するため
dynamic_shared_memory_type = posix
log_destination = 'syslog'
logging_collector = on
syslog_facility = 'LOCAL6'
log_line_prefix = '[%m][%p][%u][%d] '
log_timezone = 'Japan'
datestyle = 'iso, ymd'
timezone = 'Japan'
lc_messages = 'ja_JP.UTF-8'
lc_monetary = 'ja_JP.UTF-8'
lc_numeric = 'ja_JP.UTF-8'
lc_time = 'ja_JP.UTF-8'
default_text_search_config = 'pg_catalog.simple'
</pre>

	<a name="監視プロキシ設計_pg_hbaconf"></a>
<h3 >pg_hba.conf<a href="#監視プロキシ設計_pg_hbaconf" class="wiki-anchor">&para;</a></h3>


	<p>zabbixで利用するDBはローカルからの接続のみである。<br />通常はセキュリティを考慮するとユーザ毎に設定する必要があるが、<br />上記を踏まえローカルからの接続をすべて許可する設定とする。</p>


	<ul>
	<li>/var/lib/pgsql/9.4/data/pg_hba.conf</li>
	</ul>


<pre>
local   all             all                                     trust
host    all             all             127.0.0.1/32            trust
host    all             all             ::1/128                 trust
</pre>

	<a name="監視プロキシ設計_データベースユーザ"></a>
<h3 >データベース・ユーザ<a href="#監視プロキシ設計_データベースユーザ" class="wiki-anchor">&para;</a></h3>


	<p>以下のデータベースおよび、ユーザを作成する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>データベース名</th>
			<th>ユーザ名</th>
			<th>パスワード</th>
		</tr>
		<tr>
			<td>zabbix</td>
			<td>zabbix</td>
			<td>zabbixzabbix</td>
		</tr>
	</table>




	<a name="監視プロキシ設計_zabbix設定"></a>
<h2 >zabbix設定<a href="#監視プロキシ設計_zabbix設定" class="wiki-anchor">&para;</a></h2>


	<p>zabbix proxyではzabbixサーバの負荷を考慮してzabbix proxyから動作を行うactiveモードに設定する。<br />下記の設定では「ConfigFrequency=3600」が有効となっているため、<br />zabbixサーバからの設定変更が反映されるまでに1時間かかる。<br />また、ログをsyslogへ出力する。<br />zabbix proxyではfacilityはdaemonで固定となっており、<br />変更の場合は再コンパイルが必要となる。<br />zabbixサーバと通信できない場合にローカルDBへ168時間(1 week, 7 days分)保持する。</p>


	<ul>
	<li>/etc/zabbix/zabbix_proxy.conf</li>
	</ul>


<pre>
Server=idc-omon0a
HostnameItem=system.hostname
LogFileSize=0
PidFile=/var/run/zabbix/zabbix_proxy.pid
DBHost=127.0.0.1
DBName=zabbix
DBUser=zabbix
DBPort=5432
ProxyOfflineBuffer=168
CacheSize=128M
HistoryCacheSize=128M
HistoryTextCacheSize=128M
ExternalScripts=/usr/lib/zabbix/externalscripts
</pre>

<hr />


	<a name="監視プロキシ設計_冗長化"></a>
<h1 >●冗長化<a href="#監視プロキシ設計_冗長化" class="wiki-anchor">&para;</a></h1>


	<p>zabbix proxyは冗長化を行わない。</p>


<hr />


	<a name="監視プロキシ設計_バックアップ"></a>
<h1 >●バックアップ<a href="#監視プロキシ設計_バックアップ" class="wiki-anchor">&para;</a></h1>


	<p>DB、ログのバックアップは行わない。</p>
<hr />
<a name="監視設計" />
<a name="監視設計_監視設計"></a>
<h1 >監視設計<a href="#監視設計_監視設計" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#監視設計_監視設計">監視設計</a></li><li><a href="#監視設計_Introduction">●Introduction</a><ul><li><a href="#監視設計_目的">目的</a></li><li><a href="#監視設計_定義範囲">定義範囲</a></li></ul>
</li><li><a href="#監視設計_構成">●構成</a><ul><li><a href="#監視設計_サーバ構成">サーバ構成</a></li><li><a href="#監視設計_パーティション構成">パーティション構成</a></li><li><a href="#監視設計_物理サーバ構成">物理サーバ構成</a></li><li><a href="#監視設計_導入パッケージ">導入パッケージ</a></li><li><a href="#監視設計_システムサービス一覧">システムサービス一覧</a></li><li><a href="#監視設計_ポートアサイン">ポートアサイン</a></li><li><a href="#監視設計_ログ設定">ログ設定</a><ul><li><a href="#監視設計_ログローテ">ログローテ</a></li></ul>
</li></ul>
</li><li><a href="#監視設計_システム設計">●システム設計</a><ul><li><a href="#監視設計_httpd設定">httpd設定</a></li><li><a href="#監視設計_drbd設定">drbd設定</a><ul><li><a href="#監視設計_drbdconf">drbd.conf</a></li><li><a href="#監視設計_global_commonconf">global_common.conf</a></li><li><a href="#監視設計_postgresres">postgres.res</a></li></ul>
</li><li><a href="#監視設計_pacemaker設定">pacemaker設定</a><ul><li><a href="#監視設計_corosyncconf">corosync.conf</a></li><li><a href="#監視設計_pacemaker">pacemaker</a></li></ul>
</li><li><a href="#監視設計_postgresql設定">postgresql設定</a><ul><li><a href="#監視設計_カーネルパラメータの設定">カーネルパラメータの設定</a></li><li><a href="#監視設計_postgresqlconf">postgresql.conf</a></li><li><a href="#監視設計_pg_hbaconf">pg_hba.conf</a></li><li><a href="#監視設計_データベースユーザ">データベース・ユーザ</a></li></ul>
</li><li><a href="#監視設計_zabbix設定">zabbix設定</a></li></ul>
</li><li><a href="#監視設計_冗長化">●冗長化</a></li><li><a href="#監視設計_バックアップ">●バックアップ</a><ul><li><a href="#監視設計_バックアップ概要">バックアップ概要</a></li><li><a href="#監視設計_バックアップ-2">バックアップ</a></li><li><a href="#監視設計_リストア">リストア</a></li></ul>
</li><li><a href="#監視設計_運用手順">●運用手順</a><ul><li><a href="#監視設計_起動">起動</a></li><li><a href="#監視設計_停止">停止</a></li><li><a href="#監視設計_復旧">復旧</a></li></ul></li></ul>


<hr />


	<a name="監視設計_Introduction"></a>
<h1 >●Introduction<a href="#監視設計_Introduction" class="wiki-anchor">&para;</a></h1>


	<a name="監視設計_目的"></a>
<h2 >目的<a href="#監視設計_目的" class="wiki-anchor">&para;</a></h2>


	<p>本書はクラウドシステムで利用される監視について定義する。<br />本書は仕様書の既読者を対象とする。</p>


	<a name="監視設計_定義範囲"></a>
<h2 >定義範囲<a href="#監視設計_定義範囲" class="wiki-anchor">&para;</a></h2>


	<p>定義範囲を以下に示す。</p>


	<p><img src="/attachments/download/870/monitor_system_define_range.png" alt="" /></p>


	<ul>
	<li>user          … クラウドシステムの利用者(各userに関係性はない)</li>
		<li>zabbix proxy  … user nwを直接監視するサービス</li>
		<li>zabbix        … zabbix proxyからの情報収集、web gui提供を行う</li>
		<li>heartbeat nw  … ハートビート通信を行うネットワーク。</li>
		<li>monitoring nw … 監視関係の通信を行うネットワーク。監視サービスネットワークを指す</li>
		<li>user1 nw      … userが利用するネットワーク。他のuser nwとはvlan等により完全に分割されているものとする。ユーザ用のネットワークを指す</li>
		<li>user2 nw      … userが利用するネットワーク。他のuser nwとはvlan等により完全に分割されているものとする。ユーザ用のネットワークを指す</li>
	</ul>


<hr />


	<a name="監視設計_構成"></a>
<h1 >●構成<a href="#監視設計_構成" class="wiki-anchor">&para;</a></h1>


	<a name="監視設計_サーバ構成"></a>
<h2 >サーバ構成<a href="#監視設計_サーバ構成" class="wiki-anchor">&para;</a></h2>


	<p>冗長構成を考慮したDB構成を以下に示す。</p>


	<p><img src="/attachments/download/871/monitor_system_structure.png" alt="" /></p>


	<p>zabbixサーバは物理サーバ2台をHAクラスタとして稼働させる。<br />サービス提供用にVIPを用意し、zabbix proxyはVIPへの接続のみを意識する。<br />また、ローカルストレージをレプリケーションすることで、ディスクアクセスの排他制御等を考慮する必要がないシェアードナッシング構成となる。<br />これによりzabbixにおいて利用するDBシステムのフェールオーバが可能となり、障害時のダウンタイムを数秒単位に抑えられる。</p>


	<p>ハートビート通信を行うネットワークは物理サーバ同士の直接接続を行う。<br />ハートビート通信の障害はスプリットブレインを発生し重度のサーバ障害になるため、<br />これについても冗長化を行う。</p>


	<p>以下にzabbixサーバの構成を記載する。</p>


	<a name="監視設計_パーティション構成"></a>
<h2 >パーティション構成<a href="#監視設計_パーティション構成" class="wiki-anchor">&para;</a></h2>


	<p>監視データを保存するzabbixのデータ領域についてパーティション分割を行う。<br />このディスク領域はdrbdデバイスとなる。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>ボリュームグループ名</th>
			<th>ロジカルボリューム名</th>
		</tr>
		<tr>
			<td>vg_data</td>
			<td>lv_postgres</td>
		</tr>
	</table>




	<p>上記で/dev/vg_data/lv_postgresというデバイスファイルが作成されるが、<br />それをdrbdでレプリケーション対象とする。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>デバイスファイル</th>
			<th>マウントポイント</th>
			<th>LVM</th>
			<th>FSタイプ</th>
			<th>サイズ</th>
			<th>備考</th>
		</tr>
		<tr>
			<td>/dev/drbd0</td>
			<td>/var/lib/pgsql/9.4/data</td>
			<td>○</td>
			<td>ext4</td>
			<td>100GB</td>
			<td>postgresが利用するデータディレクトリ。データ領域のディスクから100GBを割り当てる。<br />disk容量のあまりは十分にあるため、オンラインで拡張していく※</td>
		</tr>
	</table>




	<p>※drbdの同期時間がかなり長くなるため、拡張は最小限にする。<br />参考までに1TBのdrbd同期時間はzabbixを完全停止した状態で3時間、<br />稼働中に同期すると15日間程度かかることに注意する。<br />100GBでは単純に上記時間の10分の1となる。</p>


	<a name="監視設計_物理サーバ構成"></a>
<h2 >物理サーバ構成<a href="#監視設計_物理サーバ構成" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr>
			<th style="background:#d3eaf3;text-align:left;">物理機器</th>
			<td>dell poweredge r620</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">CPU</th>
			<td>Intel(R) Xeon(R) CPU E5-2630L v2 @ 2.4GHz x86_64 6C</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">メモリ</th>
			<td>32GB</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">ディスク</th>
			<td>300GB(RAID1) 1.8TB(RAID5) 400GB(RAID5)</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">NIC</th>
			<td>8ポート</td>
		</tr>
	</table>




	<a name="監視設計_導入パッケージ"></a>
<h2 >導入パッケージ<a href="#監視設計_導入パッケージ" class="wiki-anchor">&para;</a></h2>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>パッケージ名</th>
			<th>説明</th>
		</tr>
		<tr>
			<td>drbd84-utils.x86_64</td>
			<td>drbdadm等の管理コマンド</td>
		</tr>
		<tr>
			<td>kmod-drbd84.x86_64</td>
			<td>ストレージレプリケーションを行い、シェアードナッシングを実現する。drbd本体</td>
		</tr>
		<tr>
			<td>pacemaker-1.0.13-2.el6.x86_64</td>
			<td>リソース制御機能</td>
		</tr>
		<tr>
			<td>heartbeat-3.0.5-1.1.el6.x86_64</td>
			<td>クラスタ制御機能</td>
		</tr>
		<tr>
			<td>postgresql94-server.x86_64</td>
			<td>RDBMSの代表的なOSSであるpostgresql server</td>
		</tr>
		<tr>
			<td>postgresql94-libs-9.4.1</td>
			<td>postgresqlの開発関連ライブラリ</td>
		</tr>
		<tr>
			<td>postgresql94-contrib-9.4.1</td>
			<td>postgresqlの拡張機能</td>
		</tr>
		<tr>
			<td>postgresql94-9.4.1</td>
			<td>psqlコマンド</td>
		</tr>
		<tr>
			<td>zabbix-server</td>
			<td>zabbix server本体</td>
		</tr>
	</table>




	<a name="監視設計_システムサービス一覧"></a>
<h2 >システムサービス一覧<a href="#監視設計_システムサービス一覧" class="wiki-anchor">&para;</a></h2>


	<p>zabbixは全てpacemakerにて管理するため、<br />関連するサービスの自動起動設定をoffにする。</p>


<pre>
corosync        0:off   1:off   2:off   3:off   4:off   5:off   6:off
drbd            0:off   1:off   2:off   3:off   4:off   5:off   6:off
htcacheclean    0:off   1:off   2:off   3:off   4:off   5:off   6:off
httpd           0:off   1:off   2:off   3:off   4:off   5:off   6:off
ipmi            0:off   1:off   2:off   3:off   4:off   5:off   6:off
ipmievd         0:off   1:off   2:off   3:off   4:off   5:off   6:off
pacemaker       0:off   1:off   2:off   3:off   4:off   5:off   6:off
postgresql-9.4  0:off   1:off   2:off   3:off   4:off   5:off   6:off
snmpd           0:off   1:off   2:off   3:off   4:off   5:off   6:off
snmptrapd       0:off   1:off   2:off   3:off   4:off   5:off   6:off
zabbix-server   0:off   1:off   2:off   3:off   4:off   5:off   6:off
</pre>

	<a name="監視設計_ポートアサイン"></a>
<h2 >ポートアサイン<a href="#監視設計_ポートアサイン" class="wiki-anchor">&para;</a></h2>


	<p>zabbixサーバはHA構成であり、active-standby間を直接接続したbondingインターフェイスを作成する。</p>


	<ul>
	<li>/etc/sysconfig/network-scripts/ifcfg-bond3</li>
	</ul>


	<p>L2SWを経由しないためNICの冗長化モードをactive-backupとする。</p>


<pre>
DEVICE=bond3
ONBOOT=yes
BOOTPROTO=none
IPADDR=10.16.32.[146|147]
NETMASK=255.255.255.0
BONDING_OPTS="mode=1 miimon=100 updelay=0" 
</pre>

	<a name="監視設計_ログ設定"></a>
<h2 >ログ設定<a href="#監視設計_ログ設定" class="wiki-anchor">&para;</a></h2>


	<p>ログの出力先を以下に示す。</p>


<pre>
/var/log/zabbix/zabbix_server.log
/var/log/httpd/access_log
/var/log/httpd/error_log
/var/log/cluster/pacemaker.log
/var/log/cluster/corosync.log
/var/log/pgsql/postgresql-%Y%m%d.log # postgresql本体の機能で日付の付与、ローテーションを行う
</pre>

	<a name="監視設計_ログローテ"></a>
<h3 >ログローテ<a href="#監視設計_ログローテ" class="wiki-anchor">&para;</a></h3>


	<p>postgresのログに関してはpostgres.confの設定で行う。</p>


	<ul>
	<li>/etc/logrotate.d/zabbix-server</li>
	</ul>


<pre>
/var/log/zabbix/zabbix_server.log {
    daily
    create 600 zabbix zabbix
    rotate 400
    ifempty
    missingok
    dateext
    compress
}
</pre>

	<ul>
	<li>/etc/logrotate.d/httpd</li>
	</ul>


<pre>
/var/log/httpd/*log {
    daily
    create 600 root root
    rotate 400
    ifempty
    missingok
    dateext
    compress
    sharedscripts
    postrotate
        /sbin/service httpd reload &gt; /dev/null 2&gt;/dev/null || true
    endscript
}
</pre>

	<ul>
	<li>/etc/logrotate.d/pacemaker</li>
	</ul>


<pre>
/var/log/cluster/pacemaker.log
/var/log/cluster/corosync.log {
    daily
    create 600 hacluster haclient
    rotate 400
    ifempty
    missingok
    dateext
    compress
    copytruncate
}
</pre>

<hr />


	<a name="監視設計_システム設計"></a>
<h1 >●システム設計<a href="#監視設計_システム設計" class="wiki-anchor">&para;</a></h1>


	<a name="監視設計_httpd設定"></a>
<h2 >httpd設定<a href="#監視設計_httpd設定" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>/etc/httpd/conf.d/zabbix.conf</li>
	</ul>


<pre>
Alias /zabbix /usr/share/zabbix
&lt;Directory "/usr/share/zabbix"&gt;
    Options FollowSymLinks
    AllowOverride None
    Order allow,deny
    Allow from all
    php_value max_execution_time 300
    php_value memory_limit 128M
    php_value post_max_size 16M
    php_value upload_max_filesize 2M
    php_value max_input_time 300
    php_value date.timezone Asia/Tokyo
&lt;/Directory&gt;
&lt;Directory "/usr/share/zabbix/conf"&gt;
    Order deny,allow
    Deny from all
    &lt;files *.php&gt;
        Order deny,allow
        Deny from all
    &lt;/files&gt;
&lt;/Directory&gt;
&lt;Directory "/usr/share/zabbix/api"&gt;
    Order deny,allow
    Deny from all
    &lt;files *.php&gt;
        Order deny,allow
        Deny from all
    &lt;/files&gt;
&lt;/Directory&gt;
&lt;Directory "/usr/share/zabbix/include"&gt;
    Order deny,allow
    Deny from all
    &lt;files *.php&gt;
        Order deny,allow
        Deny from all
    &lt;/files&gt;
&lt;/Directory&gt;
&lt;Directory "/usr/share/zabbix/include/classes"&gt;
    Order deny,allow
    Deny from all
    &lt;files *.php&gt;
        Order deny,allow
        Deny from all
    &lt;/files&gt;
&lt;/Directory&gt;
</pre>

	<a name="監視設計_drbd設定"></a>
<h2 >drbd設定<a href="#監視設計_drbd設定" class="wiki-anchor">&para;</a></h2>


	<a name="監視設計_drbdconf"></a>
<h3 >drbd.conf<a href="#監視設計_drbdconf" class="wiki-anchor">&para;</a></h3>


	<p>デフォルトのまま利用する。</p>


	<ul>
	<li>/etc/drbd.conf</li>
	</ul>


<pre>
include "drbd.d/global_common.conf";
include "drbd.d/*.res";
</pre>

	<a name="監視設計_global_commonconf"></a>
<h3 >global_common.conf<a href="#監視設計_global_commonconf" class="wiki-anchor">&para;</a></h3>


	<p>drbdのグローバル設定を行う設定ファイル。</p>


	<ul>
	<li>/etc/drbd.d/global_common.conf</li>
	</ul>


<pre>
global {
  usage-count no;
}
common {
  # 完全同期のCプロトコルを設定
  protocol C;
  # 基本的にpacemakerにて管理させるためハンドラの設定は行わない
  handlers {
  }
  startup {
    # 以下の設定がないと片系のノードの起動を待ち続ける
    wfc-timeout 10;
    degr-wfc-timeout 10;
    outdated-wfc-timeout 10;
  }
  options {
  }
  disk {
  }
  net {
  }
}
</pre>

	<a name="監視設計_postgresres"></a>
<h3 >postgres.res<a href="#監視設計_postgresres" class="wiki-anchor">&para;</a></h3>


	<p>今回アプリケーション毎にストレージレプリケーションを行うため、<br />アプリケーション名に.resというファイル名を用意する。</p>


	<ul>
	<li>/etc/drbd.d/postgres.res</li>
	</ul>


<pre>
resource postgres {
  device /dev/drbd0;
  disk   /dev/vg_data/lv_postgres;
  meta-disk internal;
  on idc-omon01 {
    address 10.16.32.146:7788;
  }
  on idc-omon02 {
    address 10.16.32.147:7788;
  }
}
</pre>

	<a name="監視設計_pacemaker設定"></a>
<h2 >pacemaker設定<a href="#監視設計_pacemaker設定" class="wiki-anchor">&para;</a></h2>


	<a name="監視設計_corosyncconf"></a>
<h3 >corosync.conf<a href="#監視設計_corosyncconf" class="wiki-anchor">&para;</a></h3>


	<p>pacemakerのノード管理にはcorosyncを利用する。</p>


	<ul>
	<li>/etc/corosync/corosync.conf</li>
	</ul>


<pre>
compatibility: whitetank
service {
  name: pacemaker
  ver: 0
  use_mgmud: yes
}
totem {
  version: 2
  crypto_cipher: none
  crypto_hash: none
  secauth: off
  rrp_mode: none
  interface {
    ringnumber: 0
    bindnetaddr: 10.16.32.0
    mcastport: 5405
    ttl: 1
  }
  transport: udpu
}
logging {
  fileline: off
  to_stderr: no
  to_logfile: yes
  logfile: /var/log/cluster/corosync.log
  logfile_priority: info
  to_syslog: no
  debug: off
  timestamp: on
  logger_subsys {
    subsys: QUORUM
    debug: off
  }
}
nodelist {
  node {
    ring0_addr: 10.16.32.146
    nodeid: 1
  }
  node {
    ring0_addr: 10.16.32.147
    nodeid: 2
  }
}
quorum {
  provider: corosync_votequorum
  expected_votes: 2
}
</pre>

	<a name="監視設計_pacemaker"></a>
<h3 >pacemaker<a href="#監視設計_pacemaker" class="wiki-anchor">&para;</a></h3>


	<p>pacemakerでは管理するサービス等をリソースと呼ぶ。</p>


	<ul>
	<li>pacemakerリソース内容</li>
	</ul>


<pre>
property stonith-enabled="false" \
  no-quorum-policy="ignore" \
  crmd-transition-delay="2s" 
rsc_defaults resource-stickiness="INFINITY" \
  migration-threshold="1" 
primitive pr_vip ocf:heartbeat:IPaddr2 \
  params ip=10.16.33.145 nic="bond0" cidr_netmask="24" \
  op monitor interval="10s" 
primitive pr_ping2gw ocf:pacemaker:ping \
  params name="ping_to_gateway" host_list="10.16.33.254" multiplier="100" dampen="1" \
  op monitor interval="10s" timeout="60" \
  op start timeout="60" 
primitive pr_postgres_drbd ocf:linbit:drbd \
  params drbd_resource="postgres" \
  op start interval="0s" timeout="240s" \
  op stop interval="0s" timeout="100s" \
  op monitor interval="15s" timeout="60s" role="Master" \
  op monitor interval="30s" timeout="60s" role="Slave" 
primitive pr_fs_postgres_drbd ocf:heartbeat:Filesystem \
  params device="/dev/drbd0" directory="/var/lib/pgsql/9.4/data" fstype="ext4" \
  op start interval="0s" timeout="60s" \
  op stop interval="0s" timeout="60s" \
  op monitor interval="30s" timeout="40s" 
primitive pr_postgres ocf:heartbeat:pgsql \
  params pgctl="/usr/pgsql-9.4/bin/pg_ctl" \
  psql="/usr/pgsql-9.4/bin/psql" \
  pgdata="/var/lib/pgsql/9.4/data" \
  op start interval="0s" timeout="120s" \
  op monitor interval="10s" timeout="60s" \
  op stop interval="0s" timeout="120s" 
primitive pr_zabbix lsb:zabbix-server \
  op start interval="0s" timeout="30s" \
  op stop interval="0s" timeout="30s" \
  op monitor interval="15s" timeout="30s" 
primitive pr_httpd ocf:heartbeat:apache \
  params configfile="/etc/httpd/conf/httpd.conf" \
  op start interval="0s" timeout="40s" \
  op stop interval="0s" timeout="60s" \
  op monitor interval="15s" timeout="30s" 
ms ms_postgres_drbd pr_postgres_drbd \
  meta master-max="1" master-node-max="1" \
  clone-max="2" clone-node-max="1" notify="true" \
  is-managed="true" target-role="Started" 
group grp_postgres pr_vip pr_fs_postgres_drbd pr_postgres pr_zabbix pr_httpd
clone clo_ping2gw pr_ping2gw
location loc_ping grp_postgres \
  rule -inf: not_defined ping_to_gateway or ping_to_gateway lt 100
colocation col_postgres_on_drbd inf: grp_postgres ms_postgres_drbd:Master
order ord_postgres_after_drbd inf: ms_postgres_drbd:promote grp_postgres:start
</pre>

	<p>今回watchdogを利用するので、/etc/init/pacemaker.combined.confに添付のパッチファイルを適用する。</p>


<pre>
# cd /etc/init/
# patch &lt; pacemaker.combined.conf.patch
</pre>

	<a name="監視設計_postgresql設定"></a>
<h2 >postgresql設定<a href="#監視設計_postgresql設定" class="wiki-anchor">&para;</a></h2>


	<a name="監視設計_カーネルパラメータの設定"></a>
<h3 >カーネルパラメータの設定<a href="#監視設計_カーネルパラメータの設定" class="wiki-anchor">&para;</a></h3>


	<p>postgresでは共有メモリを利用しており、<br />メモリを多く利用するオペレーションが発生した場合の性能に影響がある。<br />/etc/sysctl.confに以下の内容を追記する。<br /><a href="http://www.postgresql.jp/document/9.3/html/kernel-resources.html#SYSVIPC" class="external">参考</a></p>


<pre>
kernel.shmmax=17179869184 # 16GB以上であれば変更の必要なし
kernel.shmall=4194304
</pre>

	<a name="監視設計_postgresqlconf"></a>
<h3 >postgresql.conf<a href="#監視設計_postgresqlconf" class="wiki-anchor">&para;</a></h3>


	<p>zabbixのみがpostgresにアクセスするため基本的にはデフォルトの設定を利用する。<br />ログローテートの設定および、shared_buffesの設定を入れ込む。</p>


	<ul>
	<li>/var/lib/pgsql/9.4/data/postgresql.conf</li>
	</ul>


<pre>
max_connections = 100
shared_buffers = 8192MB # zabbixサーバは32GBのメモリを持つため、その25%を割り当てる
dynamic_shared_memory_type = posix
log_destination = 'stderr'
logging_collector = on
log_directory = '/var/log/pgsql'
log_filename = 'postgresql-%Y%m%d.log'
log_truncate_on_rotation = on
log_rotation_age = 1d
log_rotation_size = 0
log_line_prefix = '[%m][%p][%u][%d] '
log_timezone = 'Japan'
datestyle = 'iso, ymd'
timezone = 'Japan'
lc_messages = 'ja_JP.UTF-8'
lc_monetary = 'ja_JP.UTF-8'
lc_numeric = 'ja_JP.UTF-8'
lc_time = 'ja_JP.UTF-8'
default_text_search_config = 'pg_catalog.simple'
</pre>

	<a name="監視設計_pg_hbaconf"></a>
<h3 >pg_hba.conf<a href="#監視設計_pg_hbaconf" class="wiki-anchor">&para;</a></h3>


	<p>zabbixで利用するサーバローカルからの接続のみである。<br />通常はセキュリティを考慮するとユーザ毎に設定する必要があるが、<br />上記を踏まえローカルからの接続をすべて許可する設定とする。</p>


	<ul>
	<li>/var/lib/pgsql/9.4/data/pg_hba.conf</li>
	</ul>


<pre>
local   all             all                                     trust
host    all             all             127.0.0.1/32            trust
host    all             all             ::1/128                 trust
</pre>

	<a name="監視設計_データベースユーザ"></a>
<h3 >データベース・ユーザ<a href="#監視設計_データベースユーザ" class="wiki-anchor">&para;</a></h3>


	<p>以下のデータベースおよび、ユーザを作成する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>データベース名</th>
			<th>ユーザ名</th>
			<th>パスワード</th>
		</tr>
		<tr>
			<td>zabbix</td>
			<td>zabbix</td>
			<td>zabbixzabbix</td>
		</tr>
	</table>




	<a name="監視設計_zabbix設定"></a>
<h2 >zabbix設定<a href="#監視設計_zabbix設定" class="wiki-anchor">&para;</a></h2>


	<p>zabbixのweb guiは以下のURL、ユーザ:パスワードを使用する。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr>
			<th style="background:#d3eaf3;text-align:left;">URL</th>
			<td><a class="external" href="http://10.16.33.145/zabbix">http://10.16.33.145/zabbix</a></td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">USER</th>
			<td>Admin</td>
		</tr>
		<tr>
			<th style="background:#d3eaf3;text-align:left;">PASSWORD</th>
			<td>zabbix</td>
		</tr>
	</table>




	<ul>
	<li>/etc/zabbix/zabbix_server.conf</li>
	</ul>


<pre>
SourceIP=10.16.33.145
LogFile=/var/log/zabbix/zabbix_server.log
LogFileSize=0
PidFile=/var/run/zabbix/zabbix_server.pid
DBHost=127.0.0.1
DBName=zabbix
DBUser=zabbix
DBPort=5432
SNMPTrapperFile=/var/log/snmptt/snmptt.log
CacheSize=128M
HistoryCacheSize=128M
HistoryTextCacheSize=128M
TrendCacheSize=128M
ValueCacheSize=128M
AlertScriptsPath=/usr/lib/zabbix/alertscripts
ExternalScripts=/usr/lib/zabbix/externalscripts
</pre>

<hr />


	<a name="監視設計_冗長化"></a>
<h1 >●冗長化<a href="#監視設計_冗長化" class="wiki-anchor">&para;</a></h1>


	<p>zabbixサーバは物理サーバ2台でのHA構成となる。<br />片系がactiveの場合、standbyはウォームスタンバイとなる。<br />ただしpacemakerによりstandbyがprimaryとなった際のサービス起動は自動で管理される。<br />zabbixサーバ#1がactiveとした場合の上記リソース関係を以下の表にまとめる。</p>


	<table style="background:#f7f7f7;float:none;">
		<tr style="background:#d3eaf3;">
			<th>idc-omon01(active)</th>
			<th>idc-omon02(standby)</th>
		</tr>
		<tr>
			<td>vip</td>
			<td>-</td>
		</tr>
		<tr>
			<td>ping to gateway</td>
			<td>ping to gateway</td>
		</tr>
		<tr>
			<td>drbd primary</td>
			<td>drbd secondary</td>
		</tr>
		<tr>
			<td>fs mount</td>
			<td>-</td>
		</tr>
		<tr>
			<td>postgres</td>
			<td>-</td>
		</tr>
		<tr>
			<td>zabbix-server</td>
			<td>-</td>
		</tr>
		<tr>
			<td>httpd</td>
			<td>-</td>
		</tr>
	</table>




<hr />


	<a name="監視設計_バックアップ"></a>
<h1 >●バックアップ<a href="#監視設計_バックアップ" class="wiki-anchor">&para;</a></h1>


	<a name="監視設計_バックアップ概要"></a>
<h2 >バックアップ概要<a href="#監視設計_バックアップ概要" class="wiki-anchor">&para;</a></h2>


	<p>DBサーバは冗長化を行っているものの、ヒューマンエラーによる対策は十分とはいえない。<br />そのため定期的なバックアップを取得し、いつでも状態を戻せる必要がある。<br />DBサーバにおいてバックアップ対象となるのはpostgresqlのデータバックアップである。<br />postgresqlにおいてバックアップ方法は以下の2通り存在する。</p>


	<ul>
	<li>ダンプコマンドによるダンプバックアップ</li>
		<li>ファイルシステムバックアップ(通称、ベースバックアップ)</li>
	</ul>


	<p>ダンプコマンドよるバックアップは単一のsqlファイル(もしくはpostgresql独自フォーマットファイル。リストアには専用コマンドが必要)を出力するため運用が容易である。<br />ベースバックアップ方式ではデータディレクトリをそのまま圧縮して利用するだけであるが、上記の方法に比べ運用の手順が増える。<br />運用コストを低下を優先しダンプコマンドによるsqlファイルバックアップを採用する。</p>


	<a name="監視設計_バックアップ-2"></a>
<h2 >バックアップ<a href="#監視設計_バックアップ-2" class="wiki-anchor">&para;</a></h2>


	<p>cronによるスケジューリングによりダンプとファイル圧縮を行い、nasサーバへ保管する。<br />毎日バックアップを取得するものとし、その際にファイル名に日時を付与することでリカバリポイントがわかるようにする。</p>


	<p>手動で実行する場合はプライマリノードで以下を実行する。</p>


<pre>
# /usr/local/bin/backup_idc.sh
</pre>

	<a name="監視設計_リストア"></a>
<h2 >リストア<a href="#監視設計_リストア" class="wiki-anchor">&para;</a></h2>


	<p>DB領域がデータ不整合等で復旧できない場合は再構築が必要となる。<br />バックアップはダンプファイルであるため、DB領域を初期化後ダンプファイルを流し込む。<br />まずはpacemakerを停止する。<br />その後、drbdだけを起動しマウントを行った上で作業を行う。</p>


<pre>
### drbdを起動する。
# sudo /etc/init.d/drbd start
# sudo drbdadm primary postgres
# sudo mount /dev/drbd0 /var/lib/pgsql/9.4/data
### 現状のDB領域をできれば別の場所に保管しておく。
# sudo tar czf data.tar.gz /var/lib/pgsql/9.4/data
### 削除前に設定ファイルだけは保持しておく
# sudo cp -p /var/lib/pgsql/9.4/data/{pg_hba,postgresql}.conf /tmp
# sudo rm -rf /var/lib/pgsql/9.4/data/*
# sudo /etc/init.d/postgresql-9.4 initdb
# sudo mv /tmp/{pg_hba,postgresql}.conf /var/lib/pgsql/9.4/data
# sudo /etc/init.d/postgresql-9.4 start
</pre>

	<p>圧縮されたダンプファイルを解凍しsqlファイルの流し込みを行う。</p>


<pre>
# zcat backup_db_YYYY-MM-DD.gz | psql postgres postgres
</pre>

	<p>完了後、起動したサービス等を停止する。</p>


<pre>
# sudo /etc/init.d/postgresql-9.4 stop
# sudo umount /var/lib/pgsql/9.4/data
# sudo /etc/init.d/drbd stop
</pre>

	<p>起動手順に従いサービスを起動する。</p>


<hr />


	<a name="監視設計_運用手順"></a>
<h1 >●運用手順<a href="#監視設計_運用手順" class="wiki-anchor">&para;</a></h1>


	<a name="監視設計_起動"></a>
<h2 >起動<a href="#監視設計_起動" class="wiki-anchor">&para;</a></h2>


	<p>zabbixは全てpacemaker管理のため、<br />pacemakerを起動する。<br />以下のコマンドを監視サーバの両系にて実行する。</p>


<pre>
# sudo initctl start pacemaker.combined
### 起動確認 各リソースがStarted idc-omon0[1 or 2]となればOK
# sudo crm_mon
Last updated: Fri May  8 14:47:42 2015
Last change: Thu Apr 30 10:17:14 2015
Stack: corosync
Current DC: idc-omon01 (1) - partition with quorum
Version: 1.1.12-561c4cf
2 Nodes configured
9 Resources configured

Online: [ idc-omon01 idc-omon02 ]

 Resource Group: grp_postgres
     pr_vip     (ocf::heartbeat:IPaddr2):    Started idc-omon01
     pr_fs_postgres_drbd        (ocf::heartbeat:Filesystem):    Started idc-omon01
     pr_postgres        (lsb:postgresql-9.4):   Started idc-omon01
     pr_zabbix  (lsb:zabbix-server):    Started idc-omon01
     pr_httpd   (ocf::heartbeat:apache):        Started idc-omon01
 Master/Slave Set: ms_postgres_drbd [pr_postgres_drbd]
     Masters: [ idc-omon01 ]
     Slaves: [ idc-omon02 ]
 Clone Set: clo_ping2gw [pr_ping2gw]
     Started: [ idc-omon01 idc-omon02 ]
</pre>

	<p>以下のコマンドを実行し、zabbixのweb guiが閲覧できることを確認する。</p>


<pre>
# curl idc-omon0a/zabbix/
</pre>

	<a name="監視設計_停止"></a>
<h2 >停止<a href="#監視設計_停止" class="wiki-anchor">&para;</a></h2>


	<p>crm_monコマンドで現在のセカンダリノードを特定し、セカンダリノードからpacemakerの停止を行う。<br />プライマリでもpacemakerを停止後、crm_monコマンドでリソース状態が取得できないことを確認する。</p>


<pre>
# sudo initctl start pacemaker.combined
# sudo crm_mon
</pre>

	<a name="監視設計_復旧"></a>
<h2 >復旧<a href="#監視設計_復旧" class="wiki-anchor">&para;</a></h2>


	<p>障害発生時にまず状況の確認を行う。<br />両ノードにて以下のコマンドを実行する。</p>


<pre>
# sudo crm_mon
</pre>

	<p>上記のコマンド実行後、障害箇所を特定しログを確認する。<br />状況によりケースバイケースだが、<br />スプリットブレイン状態の場合は以下の手順を実行する。</p>


	<ol>
	<li>VIPに接続してどちらに飛んでいるか確認
<a class="collapsible collapsed" href="#" id="collapse-97f4c152-show" onclick="$(&#x27;#collapse-97f4c152-show, #collapse-97f4c152-hide&#x27;).toggle(); $(&#x27;#collapse-97f4c152&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-97f4c152-hide" onclick="$(&#x27;#collapse-97f4c152-show, #collapse-97f4c152-hide&#x27;).toggle(); $(&#x27;#collapse-97f4c152&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-97f4c152" style="display:none;"><pre>
###### 対象のサーバ以外から
# ping 10.16.33.145
</pre><br /><pre>
###### 対象のサーバ両方で待ち受けて来た方
# tcpdump -i bond0 host 10.16.33.145
</pre></div></li>
		<li>pingが飛んでこなかった方を停止
<a class="collapsible collapsed" href="#" id="collapse-ec03b102-show" onclick="$(&#x27;#collapse-ec03b102-show, #collapse-ec03b102-hide&#x27;).toggle(); $(&#x27;#collapse-ec03b102&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-ec03b102-hide" onclick="$(&#x27;#collapse-ec03b102-show, #collapse-ec03b102-hide&#x27;).toggle(); $(&#x27;#collapse-ec03b102&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-ec03b102" style="display:none;"><pre>
# initctl stop pacemaker.combined
</pre></div></li>
		<li>pacemakerを停止させてもサービスが落ちない場合があるので必ず各プロセスを確認すること。</li>
		<li>停止させた側のDRBDなど初期化するなりしてクラスタへ再参加。
<a class="collapsible collapsed" href="#" id="collapse-edf2b88c-show" onclick="$(&#x27;#collapse-edf2b88c-show, #collapse-edf2b88c-hide&#x27;).toggle(); $(&#x27;#collapse-edf2b88c&#x27;).fadeToggle(150);; return false;">表示</a><a class="collapsible" href="#" id="collapse-edf2b88c-hide" onclick="$(&#x27;#collapse-edf2b88c-show, #collapse-edf2b88c-hide&#x27;).toggle(); $(&#x27;#collapse-edf2b88c&#x27;).fadeToggle(150);; return false;" style="display:none;">隠す</a><div class="collapsed-text" id="collapse-edf2b88c" style="display:none;"><p>停止させた側(secondary)で<br /><pre>
# /etc/init.d/drbd stop
# drbdadm down all
# drbdadm attach all
# drbdadm invalidate all
# drbdadm up all
# drbdadm connect all
</pre><br />primaryで再接続<br /><pre>
# drbdadm connect all
</pre></p></div></li>
		<li>同期が完了した後drbdサービスを停止させpacemakerを起動。</li>
	</ol>


	<p>FSシステムの故障等が発生した場合はバックアップからリストアを行う。</p>
<hr />
<a name="笑い男ロゴ" />
<a name="笑い男ロゴ_笑い男ロゴ"></a>
<h1 >笑い男ロゴ<a href="#笑い男ロゴ_笑い男ロゴ" class="wiki-anchor">&para;</a></h1>


	<p><img src="/attachments/download/934/warai_flat.png" alt="" /></p>
<hr />
<a name="議事録" />
<a name="議事録_議事録"></a>
<h1 >議事録<a href="#議事録_議事録" class="wiki-anchor">&para;</a></h1>


	<ul class="toc"><li><a href="#議事録_議事録">議事録</a><ul><li><a href="#議事録_20150123-中村S須賀上代">20150123 中村S　須賀　上代</a></li></ul></li></ul>


<hr />


	<a name="議事録_20150123-中村S須賀上代"></a>
<h2 >20150123 中村S　須賀　上代<a href="#議事録_20150123-中村S須賀上代" class="wiki-anchor">&para;</a></h2>


	<ul>
	<li>KVMサーバ自体にクラウドインターネット</li>
		<li>zabbix proxyが存在しないパターンも作成する</li>
		<li>仮想ルータのDNS、NTP、DHCPに機能を持たせる（フォワーディング）</li>
		<li>デフォルトネットワーク以外のカスタムネットワークをもたせた仮想インスタンスの作成を行う</li>
		<li>zabbix proxyは予めすべてのインターフェイスを持たせておく</li>
		<li>クラウド領域はクラウド管理ネットワークで監視する（zabbix間通信とは別）</li>
		<li>お祭りネットワークが課題となる
	<ul>
	<li>hyper-v上で動いている監視系は移行後L2化して解体する</li>
		<li>コンダクト旧環境の管理ネットワークは旧環境の移行後クラウド管理に統合する</li>
		<li>IPMIは仮想ルータ化する</li>
		<li>クラウド管理は仮想ルータ化する</li>
	</ul>
	</li>
		<li>クラウド領域拡大部分にベアメタルの例を追加してIPMIネットワークを用意する（zabbix proxyにもインターフェイスを追加）</li>
		<li>仮想ルータのネットワーク的な立ち位置について</li>
		<li>試験サーバはKVMと同じでストレージだけ外す(名前はTKVM)</li>
	</ul>


<hr />

</body>
</html>
